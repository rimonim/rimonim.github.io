[
  {
    "objectID": "blog/troll_classification/index.html",
    "href": "blog/troll_classification/index.html",
    "title": "Advanced Machine Learning Approaches for Detecting Trolls on Twitter",
    "section": "",
    "text": "Content Warning\n\n\n\nThis report includes texts written by internet trolls, many of which are extremely offensive."
  },
  {
    "objectID": "blog/troll_classification/index.html#abstract",
    "href": "blog/troll_classification/index.html#abstract",
    "title": "Advanced Machine Learning Approaches for Detecting Trolls on Twitter",
    "section": "Abstract",
    "text": "Abstract\nSocial media platforms such as Twitter have revolutionized how people interact, share information, and express their opinions. However, this rapid expansion has also brought with it an alarming rise in malicious activities, with online trolls exploiting the platform to spread hate, misinformation, and toxicity. Detecting and mitigating such trolls have become critical in maintaining a healthy digital environment and safeguarding the well-being of users.\nIn this report, I present an exploratory investigation into the development of a cutting-edge machine learning model for the identification and classification of trolls on Twitter. In particular, I train and test three model architectures: Partial least squares (PLS) regression, boosting, and a fine-tuned transformer neural network."
  },
  {
    "objectID": "blog/troll_classification/index.html#exploratory-analysis-and-feature-selection",
    "href": "blog/troll_classification/index.html#exploratory-analysis-and-feature-selection",
    "title": "Advanced Machine Learning Approaches for Detecting Trolls on Twitter",
    "section": "Exploratory Analysis and Feature Selection",
    "text": "Exploratory Analysis and Feature Selection\nThe training data for this report consist of short texts from Twitter, each manually labeled with label indicating whether it is or is not a troll.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(tidytext)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(pls)\n\n\nAttaching package: 'pls'\n\nThe following object is masked from 'package:caret':\n\n    R2\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\nlibrary(varrank)\n\nload(\"troll_classification.RData\")\n\n# train &lt;- read_csv(\"train.csv\") %&gt;% mutate(label = factor(label))\n# test &lt;- read_csv(\"test.csv\")\n\nhead(train)\n\n# A tibble: 6 × 3\n  rowid content                                                            label\n  &lt;dbl&gt; &lt;chr&gt;                                                              &lt;fct&gt;\n1     1 'How to Talk to Girls'. I'm going to write a gay centred spinoff … 0    \n2     2 'Turns out not where but who you're with that really matters.' ;)… 0    \n3     3 'you can do it!' kick ass fdo!                                     0    \n4     4 --- each confit will make the fat saltier (it's already pretty sa… 0    \n5     5 --- that shit right there fool ... is FUCKING TIGHT!!!! --- http:… 1    \n6     6 --- you may be working from the same book as me; I'm confiting po… 0    \n\n\nThese data represent a more difficult classification task than many real-world applications, as no information is given about thread-level context or other texts produced by the same account. This report will focus purely on features of the individual text.\n\nWord Clouds\nAs an initial step in exploratory data analysis, I generated three word clouds, each on a different scope of analysis: individual words, shingles (that is, short sequences of characters), and n-grams (sequences of multiple words). It is important to perform initial analyses on different scopes, since the final tokenization method will constrain the type of features to which the model will have access. For example, it may be that trolls are more likely to use strings of punctuation like “!?!?”. A model using a word-based tokenizer may ignore punctuation altogether and miss such an informative feature. On the other hand, sequences of multiple words may reflect semantic structure in ways that 4-character shingles cannot.\n\ntroll_words &lt;- train %&gt;% \n  filter(label == 1) %&gt;%\n  mutate(clean_text = tm::removeNumbers(content),\n         clean_text = tm::removePunctuation(clean_text),\n         clean_text = tm::stripWhitespace(clean_text)) %&gt;%\n  unnest_tokens(word, clean_text, to_lower = FALSE) %&gt;%\n  count(word, sort=T) %&gt;%\n  mutate(troll_prop = n/sum(n))\n\nnotroll_words &lt;- train %&gt;% \n  filter(label == 0) %&gt;%\n  mutate(clean_text = tm::removeNumbers(content),\n         clean_text = tm::removePunctuation(clean_text),\n         clean_text = tm::stripWhitespace(clean_text)) %&gt;%\n  unnest_tokens(word, clean_text, to_lower = FALSE) %&gt;%\n  count(word, sort=T) %&gt;%\n  mutate(notroll_prop = n/sum(n))\n\nfull_words &lt;- full_join(troll_words, notroll_words, by = \"word\") %&gt;%\n  mutate(troll_prop = ifelse(is.na(troll_prop), 0, troll_prop),\n         notroll_prop = ifelse(is.na(notroll_prop), 0, notroll_prop)) %&gt;%\n  mutate(troll_notroll_diff = troll_prop - notroll_prop,\n         color = ifelse(troll_notroll_diff &gt; 0, \"red\", \"blue\"),\n         abs = abs(troll_notroll_diff)) %&gt;%\n  arrange(desc(abs))\nWords displaying the greatest difference in usage frequency between troll and non-troll texts.\n\n\nwordcloud(words = full_words$word, freq = full_words$abs, min.freq = 0,\n          max.words = 100, random.order = FALSE, rot.per = 0.3, \n          colors=full_words$color, ordered.colors=TRUE)\n\n\n\n\nThe above word cloud makes it clear that certain words are extremely indicative of troll text, and they are nearly all obscenities and/or insults. It also seems clear that trolls write in the third person more often.\nNotably, there do look to be a number of “stopwords” (e.g. “u”, “ur”, “a”, “he”, “hes” and “her”) with predictive properties on the troll side, and “i”, “to”, “the”, and “if” on the non-troll side. These short, high frequency words are often removed in pre-processing. Here though, they seem to have important predictive value.\nFinally, it looks like question words (e.g. “who”, “what”, “how”) might be negative indicator of trolls. This will be further investigated below.\n\ntroll_shingles &lt;- train %&gt;% \n  filter(label == 1) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(shingle, clean_text, token = \"character_shingles\", n = 4L,\n                strip_non_alphanum = FALSE, to_lower = FALSE) %&gt;%\n  count(shingle, sort=T) %&gt;%\n  mutate(troll_prop = n/sum(n))\n\nnotroll_shingles &lt;- train %&gt;% \n  filter(label == 0) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(shingle, clean_text, token = \"character_shingles\", n = 4L,\n                strip_non_alphanum = FALSE, to_lower = FALSE) %&gt;%\n  count(shingle, sort=T) %&gt;%\n  mutate(notroll_prop = n/sum(n))\n\nfull_shingles &lt;- full_join(troll_shingles, notroll_shingles, by = \"shingle\") %&gt;%\n  mutate(troll_prop = ifelse(is.na(troll_prop), 0, troll_prop),\n         notroll_prop = ifelse(is.na(notroll_prop), 0, notroll_prop)) %&gt;%\n  mutate(troll_notroll_diff = troll_prop - notroll_prop,\n         color = ifelse(troll_notroll_diff &gt; 0, \"red\", \"blue\"),\n         abs = abs(troll_notroll_diff)) %&gt;%\n  arrange(desc(abs))\n\n\nwordcloud(words = full_shingles$shingle, freq = full_shingles$abs, min.freq = 0,\n          max.words = 250, random.order = FALSE, rot.per = 0.3, \n          colors=full_shingles$color, ordered.colors=TRUE)\n\n\n\n\nWords displaying the greatest difference in usage frequency between troll and non-troll texts.\n\n\n\n\nThere are a number of capitalizations, long strings of repeating letters (which shingles are more likely to capture), and punctuation (e.g. ?!?!). The shingles scope of analysis seems like it is capturing some important details. This will be worthwhile if I can leverage these features in the dimensionality reduction process.\n\ntroll_ngrams &lt;- train %&gt;% \n  filter(label == 1) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(ngram, clean_text, token = \"skip_ngrams\", n = 3L, k = 1) %&gt;%\n  count(ngram, sort=T) %&gt;%\n  mutate(troll_prop = n/sum(n))\n\nnotroll_ngrams &lt;- train %&gt;% \n  filter(label == 0) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(ngram, clean_text, token = \"skip_ngrams\", n = 3L, k = 1) %&gt;%\n  count(ngram, sort=T) %&gt;%\n  mutate(notroll_prop = n/sum(n))\n\nfull_ngrams &lt;- full_join(troll_ngrams, notroll_ngrams, by = \"ngram\") %&gt;%\n  mutate(troll_prop = ifelse(is.na(troll_prop), 0, troll_prop),\n         notroll_prop = ifelse(is.na(notroll_prop), 0, notroll_prop)) %&gt;%\n  mutate(troll_notroll_diff = troll_prop - notroll_prop,\n         color = ifelse(troll_notroll_diff &gt; 0, \"red\", \"blue\"),\n         abs = abs(troll_notroll_diff)) %&gt;%\n  arrange(desc(abs))\n\n\nwordcloud(words = full_ngrams$ngram, freq = full_ngrams$abs, min.freq = 0,\n          max.words = 150, random.order = FALSE, rot.per = 0.3, \n          colors=full_ngrams$color, ordered.colors=TRUE)\n\n\n\n\nWords displaying the greatest difference in usage frequency between troll and non-troll texts.\n\n\n\n\nOn the level of n-grams, the most informative predictors look to be the same vulgar slurs found on the single-word level. Nevertheless, there are many multi-word sequences in this word cloud. Especially striking is the common appearance of the word “you” on both the troll and non-troll sides, in varying contexts. Whereas in single-word analysis “YOU” and “your” seemed to be indicative of trolls, n-gram level analysis makes it clear that certain phrases such as “would you”, “do you think”, “have you ever”, and “if you” are in fact much highly indicative of non-trolls. This suggests that allowing n-grams may increase the predictive abilities of the model, providing the dimentionality reduction works properly.\n\n\nOther Important Features\nWhile single words, shingles, and n-grams seem to cover a lot of differences between troll and non-troll texts, I can think of a few more features that may be relevant but will not be detected by any of the levels of tokenization analysis above. Here are some things that will not be captured in tokenization, but might be indicative of trolls:\n\nuse of all-caps text\nuse of punctuation in normal/unconventional ways (e.g. period at the end of sentence, three exclamation points, ***, …, quotes)\nemoticons (e.g. “:-)”, “&lt;3”, “:3”)\nuser tags\nsame character many times in a row\nreadability, as measured by various algorithms (e.g. “Scrabble”, “SMOG.simple”, “Traenkle.Bailer2”, “meanWordSyllables”)\n\n\n# Get list of emoticons and add escapes for use as regex\nemoticons &lt;- str_replace_all(str_replace_all(lexicon::hash_emoticons$x, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\"), \"([.|()^{}+$*?]|\\\\[|\\\\])\", \"\\\\\\\\\\\\1\")\ncount_emoticons &lt;- function(x){\n  count &lt;- rep_len(0L, length(x))\n  for (i in 1:length(emoticons)) {\n    count &lt;- count + str_count(x, emoticons[i])\n  }\n  count\n}\nquestion_words &lt;- c(\"who\", \"what\", \"when\", \"where\", \"how\", \"why\", \"whose\",\n                    \"Who\", \"What\", \"When\", \"Where\", \"How\", \"Why\", \"Whose\",\n                    \"Would\", \"Have\", \"Do\", \"Does\", \"Did\", \"Didn't\", \n                    \"Didnt\", \"Are\", \"Aren't\", \"Arent\")\ncount_question_words &lt;- function(x){\n  count &lt;- rep_len(0L, length(x))\n  for (i in 1:length(question_words)) {\n    count &lt;- count + str_count(x, question_words[i])\n  }\n  count\n}\nprofanity &lt;- str_replace_all(str_replace_all(lexicon::profanity_banned, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\"), \"([.|()^{}+$*?]|\\\\[|\\\\])\", \"\\\\\\\\\\\\1\")\ncount_profanity &lt;- function(x){\n  count &lt;- rep_len(0L, length(x))\n  for (i in 1:length(profanity)) {\n    count &lt;- count + str_count(str_to_lower(x), profanity[i])\n  }\n  count\n}\n\ntrain_features &lt;- train %&gt;% \n  mutate(ncaps = str_count(content, \"[A-Z]\"), # capital Letters\n         allcaps_words = str_count(content, \"\\\\b[A-Z]{2,}\\\\b\"), # words of ALLCAPS text\n         conventional_periods = str_count(content, \"[:alnum:]\\\\.[:space:]\"), # conventionally used periods\n         ellipses = str_count(content, \"\\\\.\\\\.\"), # ...\n         exclamation = str_count(content, \"\\\\!\\\\!\"), # !!\n         emoticons = count_emoticons(content),\n         question_words = count_question_words(content),\n         profanity = count_profanity(content),\n         noprofanity = as.integer(profanity == 0),\n         urls = str_count(content, \"http://\"),\n         words = str_count(content, '\\\\w+'),\n         quotations = str_count(content, '\".+\"'))\n\n# Readability measures\ntrain_features &lt;- train_features %&gt;% \n  bind_cols(quanteda.textstats::textstat_readability(train_features$content, \n                                                     measure = c(\"Scrabble\", \n                                                                 \"SMOG.simple\", \n                                                                 \"Traenkle.Bailer\",\n                                                                 \"meanWordSyllables\")) %&gt;% select(-document))\n\n\ntrain_features %&gt;% \n  mutate(label = if_else(label == 1, \"Trolls\", \"Non-trolls\")) %&gt;% \n  pivot_longer(ncaps:meanWordSyllables, names_to = \"feature\", values_to = \"value\") %&gt;% \n  group_by(label, feature) %&gt;% \n  mutate(mean_value = mean(value, na.rm = TRUE),\n         quantile_value_hi = quantile(value, probs = .95, na.rm = TRUE),\n         quantile_value_lo = quantile(value, probs = .05, na.rm = TRUE)) %&gt;% \n  slice_sample(n = 1000) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(label, value)) +\n    ggbeeswarm::geom_quasirandom(alpha = .1) +\n    geom_point(aes(y = quantile_value_hi), color = \"skyblue\", size = 3) + \n    geom_point(aes(y = quantile_value_lo), color = \"skyblue\", size = 3) + \n    geom_point(aes(y = mean_value), color = \"red\", size = 3) + \n    facet_wrap(~feature, scales = \"free\") +\n    labs(x = \"\", y = \"\") +\n    theme_bw()\n\n\n\n\nSome of these (e.g. conventional periods, ellipses, exclamation marks) will in fact be automatically captured by shingles. I’ll keep the ones that won’t (total emoticons, allcaps words, quotations, ncaps, question_words, profanity, lack of profanity, urls).\nLet’s take a closer look at the number of words in each text:\n\ntrain_features %&gt;% \n  mutate(label = if_else(label == 1, \"Trolls\", \"Non-trolls\")) %&gt;% \n  ggplot(aes(words, fill = label)) +\n    geom_density(alpha = .5) +\n    scale_x_continuous(limits = c(0, 50)) +\n    theme_bw()\n\nWarning: Removed 44 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nThese distributions do have notably different shapes: Non-troll texts are very commonly around 6 words long, and fall off sharply above that. Troll texts, on the other hand, are more evenly distributed between 5 and 25 words in length. This means that texts around 6 words long are disproportionately likely not to be trolls, whereas texts that are 14-26 words long are disproportionately likely to be trolls. I will therefore create two binary variables: short_text for texts under 13 words long, and med_text for texts 14-26 words long.\n\ntrain_features &lt;- train_features %&gt;% \n  mutate(short_text = as.integer(words &lt; 13),\n         med_text = as.integer((words &gt; 13) & (words &lt; 27))) %&gt;% \n  select(-words)"
  },
  {
    "objectID": "blog/troll_classification/index.html#retrain-best-model-on-full-training-set",
    "href": "blog/troll_classification/index.html#retrain-best-model-on-full-training-set",
    "title": "Advanced Machine Learning Approaches for Detecting Trolls on Twitter",
    "section": "Retrain best model on full training set",
    "text": "Retrain best model on full training set\nNow that the PLS model incorporating n-grams and shingles is established as the superior one, I will retrain it on the full training dataset before submitting it to the Kaggle competition.\n\n# Full-set PLS Model (10-fold cross-validation)\n# Using feature set with ngrams\nset.seed(123)\npls_mod_final &lt;- train(\n  label~., data = select(train_allfeatures, -c(rowid, content, clean_text)), method = \"pls\",\n  scale = TRUE,\n  trControl = trainControl(\"cv\", number = 10),\n  tuneLength = 4\n  )\n\nplot(pls_mod_final) # Still best with 2 components\n\n# Identify features found in train but not test set\ntest &lt;- read_csv(\"test.csv\")\n\ntest_shingles &lt;- test %&gt;% \n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(shingle, clean_text, token = \"character_shingles\", n = 4L,\n                strip_non_alphanum = FALSE, to_lower = FALSE) %&gt;%\n  count(shingle, sort=T)\nirrelevant_shingles &lt;- setdiff(top_shingles, test_shingles$shingle) # none missing!\n\ntest_ngrams &lt;- test %&gt;% \n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(ngram, clean_text, token = \"skip_ngrams\", n = 3L, k = 1) %&gt;%\n  count(ngram, sort=T)\nirrelevant_ngrams &lt;- setdiff(top_ngrams, test_ngrams$ngram) # list of 7\n\n# Add all features to test set\n\ntest_features &lt;- test %&gt;% \n  mutate(ncaps = str_count(content, \"[A-Z]\"), # capital Letters\n         allcaps_words = str_count(content, \"\\\\b[A-Z]{2,}\\\\b\"), # words of ALLCAPS text\n         conventional_periods = str_count(content, \"[:alnum:]\\\\.[:space:]\"), # conventionally used periods\n         ellipses = str_count(content, \"\\\\.\\\\.\"), # ...\n         exclamation = str_count(content, \"\\\\!\\\\!\"), # !!\n         emoticons = count_emoticons(content),\n         question_words = count_question_words(content),\n         profanity = count_profanity(content),\n         noprofanity = as.integer(profanity == 0),\n         urls = str_count(content, \"http://\"),\n         words = str_count(content, '\\\\w+'),\n         quotations = str_count(content, '\".+\"'),\n         short_text = as.integer(words &lt; 13),\n         med_text = as.integer((words &gt; 13) & (words &lt; 27)),\n         clean_text = tm::stripWhitespace(content)) %&gt;% \n  # Readability measures and quantized length\n  bind_cols(quanteda.textstats::textstat_readability(test_features$content, \n                                                     measure = c(\"Scrabble\", \n                                                                 \"SMOG.simple\", \n                                                                 \"Traenkle.Bailer\",\n                                                                 \"meanWordSyllables\"))) %&gt;% \n  select(c(rowid, content, allcaps_words, emoticons, question_words, profanity, noprofanity, quotations, SMOG.simple, Traenkle.Bailer, short_text, med_text, clean_text)) %&gt;%\n  ## Compute shingles\n  unnest_tokens(shingle, clean_text, token = \"character_shingles\", n = 4L,\n                strip_non_alphanum = FALSE, to_lower = FALSE, drop = FALSE) %&gt;% \n  # replace everything but top 1000 with placeholder\n  mutate(shingle = if_else(shingle %in% top_shingles, shingle, \"shingle\")) %&gt;% \n  group_by(across(everything())) %&gt;% summarise(n = n()) %&gt;% ungroup() %&gt;% \n  # pivot shingles to columns\n  pivot_wider(id_cols = rowid:clean_text, names_from = \"shingle\", values_from = \"n\", names_prefix = \"shingle_\", values_fill = 0L) %&gt;% \n  mutate(across(everything(), ~replace_na(.x, 0))) %&gt;% \n  ungroup()\n\n# add ngrams to other features as sparse features\ntest_features &lt;- test_features %&gt;% \n  select(c(rowid, content)) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  ## Compute ngrams\n  unnest_tokens(ngram, clean_text, token = \"skip_ngrams\", n = 3L, k = 1) %&gt;%\n  # replace everything but top 1000 with placeholder\n  mutate(ngram = if_else(ngram %in% top_ngrams, ngram, \"ngram\")) %&gt;% \n  group_by(across(everything())) %&gt;% summarise(n = n()) %&gt;% ungroup() %&gt;% \n  # pivot ngrams to columns\n  pivot_wider(id_cols = rowid:content, names_from = \"ngram\", values_from = \"n\", names_prefix = \"ngram_\", values_fill = 0L) %&gt;% \n  mutate(across(everything(), ~replace_na(.x, 0))) %&gt;% \n  full_join(test_features %&gt;% select(-c(clean_text)))\n\n# Add train-unique features in with all zeros\npaste0(\"ngram_\", irrelevant_ngrams)\ntest_features &lt;- test_features %&gt;% \n  mutate(`ngram_fake fake fake` = 0, `ngram_fake fake` = 0,\n         `ngram_whore whore` = 0, `ngram_it fuck` = 0,\n         `ngram_fuck u` = 0, `ngram_a a a` = 0,\n         `ngram_lick` = 0)\n\n# Predictions to csv\ndata.frame(Id = test_features$rowid,\n           Category = predict(pls_mod_final,\n                              ncomp = pls_mod_final$bestTune$ncomp,\n                              newdata = test_features)) %&gt;%  \n  write_csv(\"~/Downloads/pls_mod_predictions.csv\")"
  },
  {
    "objectID": "blog/medieval_philosophers/index.html",
    "href": "blog/medieval_philosophers/index.html",
    "title": "Designing a Poster to Visualize the Timeline of Philosophers in the Islamic World",
    "section": "",
    "text": "One day when I was in 5th grade, I walked into my classroom to find a new poster on the wall. It was a visualization of the entirety of world history - I was transfixed. This is that poster below. You can buy it here. It’s greatest innovation is squishing geography (which is generally two-dimensional) onto the y-axis. A lot of detail is lost, but the gained ability to visualize history all at once on a wall poster makes it worth it.\n\nI quickly asked my parents if I could get one for myself. When I finally did, I set it up next to my bed - as I lay there every night, I would look at all the little details. I even found a few mistakes.\nFast forward a decade and a half. I’ve been really enjoying this podcast, The History of Philosophy Without Any Gaps. I’m on episode 290 at the moment. Learning about all of these philosophers is great, but they can be hard to keep track of. I need a timeline that keeps track of geography too.\nThis is exactly the kind of problem for which I developed the wormsplot package in R, inspired by that wonderful poster from my childhood. In this post, I’ll walk through the process of designing a wall poster to visualize the major philosophers of the Islamic world in the Middle Ages.\nCode and data for this project can be found here.\n\nStep 1: The Data\nI gathered this data myself from wherever I could find it - mostly Wikipedia and The Stanford Encyclopedia of Philosophy. It includes one row for each new stop along the way of a biography (starting with the birth date - usually an educated guess), plus one extra for the death date. I’m no historian, so don’t rely too heavily on the accuracy of these data. Actually even if I were a historian this would take a lot of guesswork - that’s how medieval history goes. Anyhow, here’s what it looks like: one column for name, one for city, one for date, and one for the philosopher’s religion (Muslim, Jewish, or Christian). For cities that no longer exist or are called something different now, I wrote the closest modern equivalent and made a note of it on the side.\n\n\n# A tibble: 10 × 5\n   name                      city     date religion  notes   \n   &lt;chr&gt;                     &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   \n 1 al-Kindī                  Kufa      801 Muslim    &lt;NA&gt;    \n 2 al-Kindī                  Baghdad   820 Muslim    &lt;NA&gt;    \n 3 al-Kindī                  Baghdad   871 Muslim    &lt;NA&gt;    \n 4 Ḥunayn ibn Isḥāq          Kufa      809 Christian al-Ḥīrah\n 5 Ḥunayn ibn Isḥāq          Baghdad   828 Christian &lt;NA&gt;    \n 6 Ḥunayn ibn Isḥāq          Baghdad   873 Christian &lt;NA&gt;    \n 7 Isḥāq ibn Ḥunayn          Baghdad   830 Christian &lt;NA&gt;    \n 8 Isḥāq ibn Ḥunayn          Baghdad   910 Christian &lt;NA&gt;    \n 9 Abū Bakr al-Rāzī (Rhazes) Rey       864 Muslim    &lt;NA&gt;    \n10 Abū Bakr al-Rāzī (Rhazes) Baghdad   880 Muslim    &lt;NA&gt;    \n\n\n\n\nStep 2: Fitting Geography Onto One Axis\nThe biggest challenge here is that y-axis. The first step: Find the latitude and longitude of each city in the data by calling the Open Street Map API. This worked very smoothly for everything except the city of Alexandria, which it identified at Alexandria, Virginia. After fixing that problem, I could plot a map of all the cities in the data:\n\n\n\n\n\nThere are many ways to reduce two-dimensional data to one dimension, and the best choice depends on the task at hand. If my places were grouped into distinct regions, I might consider t-SNE or UMAP. If they were generally aligned along some diagonal axis, I might use Principle Component Analysis. As it stands though, I know exactly how I want my y-axis to be organized: It should go East to West along the coast of North Africa and then West to East within Western Europe. This makes sense both geographically and historically: Since Andalusia (Muslim Spain) was the main point of contact between the Islamic world and Christian Europe, Southern France should be ‘farther’ from Tunisia than Spain.\nSo I split the cities into Europe and Non-Europe and lined them up by longitude accordingly. After a few manual adjustments (going through all of Turkey before moving South along the Mediterranean coast, and moving Northern France and London to the far end of the axis), I ended up with this ordering:\n\n\n\n\n\nThe final step was to make up for the fact that certain cities that are very close to each other in longitude are actually quite far away on the North-South axis. I achieved this by scaling the distance between each city on the axis by the true Euclidean distance between them. This stretches out certain parts of the axis disproportionately, but it means that adjacent locations are the right relative distances away from each other. With that, here are all the cities arranged along the new y-axis:\n\n\n\n\n\n\n\nStep 3: Layout\nWith the y-axis defined, it’s time for some graphic design. I originally toyed with a gradient background along the y-axis, but settled on dividing it up into larger regions. Here’s the resulting blank plot:\n\n\n\n\n\n\n\nStep 4: Plot!\nNow that we have a suitable background, all that remains to represent the data. This is done using the wormsplot function stat_worm(). Labels are added with the aid of the label_worms() function. The worms are colored by their religion: Muslims in green, Jews in blue, and Christians in red.\n\n\n\n\n\nI think it looks quite nice! Even without squinting at the individual names, big trends in the history of philosophy are immediately evident, like the Baghdad school starting with al-Kindī and running for about 150 years, or the explosion of philosophical activity in Andalusia in the 12th century.\nAt the moment, this is just a proof of concept. The labeling of the worms especially needs some work. If I were to produce a finished poster, I would also include a timeline of important events along the bottom, and a legend to explain the format. I might also want to simplify the data somewhat - a few of these figures moved around a lot in their lifetimes (Ibn `Arabi, I’m looking at you) and are making it a bit difficult to follow the lines."
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html",
    "href": "blog/reddit_environmentalists/index.html",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "",
    "text": "Greta Thunberg may be the most well-known climate activist today. Along with her climate activism, she is known for her psychiatric diagnoses. In fact, these two aspects of her public persona often go together - autism and anxiety disorders are the superpowers that allow her to take a principled stand.\nTold by Thunberg herself, this is an inspiring and compelling claim. But is it true in general that autistic and/or anxious thought patterns cause people to take the sustainability crisis more seriously? Is this true even when it comes to smaller-scale activism than Thunberg’s - say, involvement with environmentalist groups on Reddit?"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#neutralized-ccr",
    "href": "blog/reddit_environmentalists/index.html#neutralized-ccr",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "Neutralized CCR",
    "text": "Neutralized CCR\nIn my last post, I explored various methods of quantifying psychological content in text, including contextualized construct representation (CCR)2, which leverages the power of large language models to turn established psychiatric questionnaires into embeddings - lists of activations of neurons in the model - that can be compared to the same model’s embeddings of other texts.\nToward the end of that post, I raised a potential difficulty with this method: How can I be sure that I am not measuring the extent to which people write like they’re writing a questionnaire? This is critical, since questionnaires tend to be written in formal, well organized, full sentences, and certain psychological constructs might lead to people writing less - or more - like that.\nThe solution I proposed was to create a “neutralized” version of the questionnaire average embedding, which reflects each original question and its grammatical negation. By subtracting out this neutralized embedding (which nevertheless maintains its questionnaire-ness all around) from the original embedding, I can isolate the psychological construct of interest without fear of confounds from the particular writing style of the questionnaire. This is the approach I will take here, using the Adult Social Behavior Questionnaire (ASBQ)3 for clinical assessment of autism spectrum disorder in adults. The questionnaire includes six subscales: reduced contact, reduced empathy, reduced interpersonal insight, violations of social conventions, insistence on sameness, and sensory stimulation and motor stereotypes. Excluding the last one (I have a hard time imagining how it would apply to Reddit activity), I will treat each subscale on its own.\nAs in my last post, I am using embeddings from the second to last layer of BERT (base uncased).\n\n# Adult Social Behavior Questionnaire (ASBQ); self-report, excluding 'sensory stimulation and motor stereotypes'\nreduced_contact &lt;- c(\n  \"i do not take the initiative in contacts with other people\",\n  \"i have little or no interest in socializing with others\",\n  \"i ignore invitations from others to do something with them\",\n  \"i avoid people who try to make contact with me\",\n  \"the only contact i have with others is when i have to buy something or arrange something, for example with people in a shop or in a government office\",\n  \"i am a loner, even in a group i hold myself apart\",\n  \"i do not enjoy doing things with other people, for example, doing a chore together or going somewhere together\"\n  )\nreduced_empathy &lt;- c(\n  \"i find it difficult to put myself in someone else’s shoes, for example, i can not see why someone is angry\",\n  \"i am unaware of other people’s emotional needs, for example, i do not encourage other people or reassure them\",\n  \"i find it hard to sense what someone else will like or think is nice\",\n  \"i am not really bothered by someone else in pain\",\n  \"i do not notice when someone is upset or has problems\",\n  \"the reason why i would contact others is to get things done rather than because i am interested in them\",\n  \"i do not show sympathy when others hurt themselves or are unhappy\"\n  )\nreduced_interpersonal_insight &lt;- c(\n  \"i do not get jokes\",\n  \"i take everything literally, for example, i do not understand certain expressions\",\n  \"i am very naïve; i believe everything i am told\",\n  \"it is easy to take advantage of me or get me to do other people’s dirty work\",\n  \"i do not notice when others make fun of me\",\n  \"i find it hard to follow the gist of a conversation—i miss the point\",\n  \"i need an explanation before i understand the meaning behind someone’s words\",\n  \"i give answers that are not relevant because i have not really understood the question\"\n  )\nviolations_of_social_conventions &lt;- c(\n  \"i do not differentiate between friends and strangers, for example, i do not care who i am with\",\n  \"i seek contact with anyone and everyone; i show no reserve\",\n  \"i touch people when it is not suitable, for example, i hug virtual strangers\",\n  \"the questions i ask are too personal, or i tell others things that are too personal\",\n  \"i behave the same wherever i am; it makes no difference to me whether i am at home or somewhere else (visiting others, at work, in the streets)\",\n  \"i ask strangers for things i need, for example for food or drink if i am hungry or thirsty\"\n  )\ninsistence_on_sameness &lt;- c(\n  \"i panic when things turn out differently than i am used to\",\n  \"i resist change; if it were left up to me, everything would stay the same\",\n  \"i want to do certain things in exactly the same way every time\",\n  \"i do not like surprises, for example, unexpected visitors\",\n  \"i do not like a lot of things happening at once\",\n  \"i really need fixed routines and things to be predictable\",\n  \"i hate it when plans are changed at the last moment\",\n  \"it takes me ages to get used to somewhere new\"\n  )\n#~~~~~~~~~~~~~~~~~~~~~~~~# Negated Versions\nreduced_contact_neg &lt;- c(\n  \"i take the initiative in contacts with other people\",\n  \"i have lots of interest in socializing with others\",\n  \"i do not ignore invitations from others to do something with them\",\n  \"i do not avoid people who try to make contact with me\",\n  \"i have contact with others all the time, not just when i have to buy something or arrange something, or with people in a shop or in a government office\",\n  \"i am not a loner, in a group i do not hold myself apart\",\n  \"i enjoy doing things with other people, for example, doing a chore together or going somewhere together\"\n  )\nreduced_empathy_neg &lt;- c(\n  \"i do not find it difficult to put myself in someone else’s shoes, for example, i can see why someone is angry\",\n  \"i am aware of other people’s emotional needs, for example, i encourage other people or reassure them\",\n  \"i do not find it hard to sense what someone else will like or think is nice\",\n  \"i really bothered by someone else in pain\",\n  \"i notice when someone is upset or has problems\",\n  \"the reason why i would contact others is because i am interested in them rather than to get things done\",\n  \"i show sympathy when others hurt themselves or are unhappy\"\n  )\nreduced_interpersonal_insight_neg &lt;- c(\n  \"i get jokes\",\n  \"i do not take everything literally, for example, i understand expressions\",\n  \"i am not very naïve; i do not believe everything i am told\",\n  \"it is not easy to take advantage of me or get me to do other people’s dirty work\",\n  \"i notice when others make fun of me\",\n  \"i do not find it hard to follow the gist of a conversation—i do not miss the point\",\n  \"i do not need an explanation before i understand the meaning behind someone’s words\",\n  \"i give answers that are relevant because i have really understood the question\"\n  )\nviolations_of_social_conventions_neg &lt;- c(\n  \"i differentiate between friends and strangers, for example, i care who i am with\",\n  \"i do not seek contact with anyone and everyone; i show reserve\",\n  \"i do not touch people when it is not suitable, for example, i do not hug virtual strangers\",\n  \"the questions i ask are not too personal; i do not tell others things that are too personal\",\n  \"i do not behave the same wherever i am; it makes a difference to me whether i am at home or somewhere else (visiting others, at work, in the streets)\",\n  \"i do not ask strangers for things i need, for example for food or drink if i am hungry or thirsty\"\n  )\ninsistence_on_sameness_neg &lt;- c(\n  \"i do not panic when things turn out differently than i am used to\",\n  \"i do not resist change; if it were left up to me, nothing would stay the same\",\n  \"i do not want to do things in exactly the same way every time\",\n  \"i like surprises, for example, unexpected visitors\",\n  \"i like a lot of things happening at once\",\n  \"i do not really need fixed routines or things to be predictable\",\n  \"i do not hate it when plans are changed at the last moment\",\n  \"it does not take me ages to get used to somewhere new\"\n  )\n\n#~~~~~~~~~~~~~~~~~~~~~~~~# Generate Embeddings\n\n# Mean embeddings of each subscale and its negation\nreduced_contact &lt;- textEmbed(reduced_contact)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nreduced_empathy &lt;- textEmbed(reduced_empathy)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nreduced_interpersonal_insight &lt;- textEmbed(reduced_interpersonal_insight)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nviolations_of_social_conventions &lt;- textEmbed(violations_of_social_conventions)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\ninsistence_on_sameness &lt;- textEmbed(insistence_on_sameness)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\n\nreduced_contact_neg &lt;- textEmbed(reduced_contact_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nreduced_empathy_neg &lt;- textEmbed(reduced_empathy_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nreduced_interpersonal_insight_neg &lt;- textEmbed(reduced_interpersonal_insight_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nviolations_of_social_conventions_neg &lt;- textEmbed(violations_of_social_conventions_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\ninsistence_on_sameness_neg &lt;- textEmbed(insistence_on_sameness_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\n\n# Neutralized Questionnaire - Initial - mean(Initial, Negated)\nreduced_contact_neutralized &lt;- reduced_contact - (reduced_contact + reduced_contact_neg)/2\nreduced_empathy_neutralized &lt;- reduced_empathy - (reduced_empathy + reduced_empathy_neg)/2\nreduced_interpersonal_insight_neutralized &lt;- reduced_interpersonal_insight - (reduced_interpersonal_insight + reduced_interpersonal_insight_neg)/2\nviolations_of_social_conventions_neutralized &lt;- violations_of_social_conventions - (violations_of_social_conventions + violations_of_social_conventions_neg)/2\ninsistence_on_sameness_neutralized &lt;- insistence_on_sameness - (insistence_on_sameness + insistence_on_sameness_neg)/2"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#raspergers",
    "href": "blog/reddit_environmentalists/index.html#raspergers",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "r/aspergers",
    "text": "r/aspergers\nMy second metric for autistic tendencies is less sensitive to the nuances of various ASD symptoms, but is more directly tailored to the task at hand. The r/aspergers subreddit describes itself as “the internet’s largest community of people affected by Autism Spectrum Disorder”. So here, I am simply going to compute the average embedding of posts in this group and use it as a paradigm of the construct of interest.\n\nASD_threads &lt;- find_thread_urls(subreddit = \"aspergers\")\n\nASD_threads$full_text &lt;- preprocess(paste(ASD_threads$title, ASD_threads$text))\n\n# Embedding\nASD_threads_embeddings &lt;- textEmbed(ASD_threads$full_text)$texts[[1]]\n\n# Average Embedding\nASD &lt;- ASD_threads_embeddings %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#kaggle-dataset",
    "href": "blog/reddit_environmentalists/index.html#kaggle-dataset",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "Kaggle Dataset",
    "text": "Kaggle Dataset\nI could easily take the same approach for anxiety as I did for autism. I could even use questionnaires designed specifically for OCD and selective mutism, the diagnoses mentioned by Greta Thunberg in the quote above. Nevertheless, I am going to use a different, more straightforward method: the Students Anxiety and Depression Dataset includes 733 handcoded examples of social media posts and comments that reflect anxiety. I will simply take the average BERT embedding of these as the quintessence of anxiety on social media.\n\nanxiety_texts &lt;- readxl::read_xlsx(\"dataset.xlsx\") %&gt;% \n  filter(label == 1) %&gt;% pull(text)\n\nanxiety_texts &lt;- preprocess(anxiety_texts)\n\nanxiety &lt;- textEmbed(anxiety_texts)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#rocd",
    "href": "blog/reddit_environmentalists/index.html#rocd",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "r/OCD",
    "text": "r/OCD\nAs with ASD above, my second metric will simply be an average embedding of posts on a disorder-specific subreddit. This time, r/OCD.\n\nOCD_threads &lt;- find_thread_urls(subreddit = \"OCD\")\n\nOCD_threads$full_text &lt;- preprocess(paste(OCD_threads$title, OCD_threads$text))\n\nOCD_threads_subset &lt;- slice_sample(OCD_threads, n = 100)\n\n# Embedding\nOCD_threads_embeddings &lt;- textEmbed(OCD_threads$full_text)$texts[[1]]\n\n# Average Embedding\nOCD &lt;- OCD_threads_embeddings %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#footnotes",
    "href": "blog/reddit_environmentalists/index.html#footnotes",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThunberg, Greta (24 November 2018). School strike for climate – save the world by changing the rules. TEDxStockholm. Event occurs at 1:46.↩︎\nAtari, M., Omrani, A., & Dehghani, M. (2023, February 24). Contextualized construct representation: Leveraging psychometric scales to advance theory-driven text analysis. https://doi.org/10.31234/osf.io/m93pd↩︎\nHorwitz, E., Schoevers, R.A., Ketelaars, C., Kan, C.C., Lammeren, A.V., Meesters, Y., Spek, A.A., Wouters, S., Teunisse, J.P., Cuppen, L., Bartels, A.A., Schuringa, E., Moorlag, H., Raven, D., Wiersma, D., Minderaa, R.B., & Hartman, C.A. (2016). Clinical assessment of ASD in adults using self- and other-report : Psychometric properties and validity of the Adult Social Behavior Questionnaire (ASBQ). Research in Autism Spectrum Disorders, 24, 17-28.↩︎\nTaylor, E. C., Livingston, L. A., Callan, M. J., Hanel, P. H. P., & Shah, P. (2021). Do autistic traits predict pro-environmental attitudes and behaviors, and climate change belief? Journal of Environmental Psychology, 76, Article 101648. https://doi.org/10.1016/j.jenvp.2021.101648↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "MS in Neurocognitive Experimental Psychology and Data Science, 2022-Present\nBen Gurion University of the Negev\nBA in Psychology and Biology, 2018-2021\nYeshiva University"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "MS in Neurocognitive Experimental Psychology and Data Science, 2022-Present\nBen Gurion University of the Negev\nBA in Psychology and Biology, 2018-2021\nYeshiva University"
  },
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "Louis Teitelbaum’s CV",
    "section": "",
    "text": "louist@post.bgu.ac.il\n Home Page |  Blog\n GitHub |  LinkedIn\n\n\n\nEnglish - native\nHebrew - fluent\n\n\n\nProgramming: R, Python, SQL, Git(Hub), SPSS, Excel\nDocumentation: (R)Markdown/Quarto, JupyterLab, Google Colab, Google Docs/Sheets\nDesign: ggplot2, Gimp, Photoshop, Inkscape\nDeployment: Shiny, Streamlit\nMachine Learning: Bayesian inference, multilevel modeling, logistic and binomial regression, PCA, random forest, elastic-net regression, k-means clustering, t-SNE, word/document embeddings, sentiment analysis"
  },
  {
    "objectID": "CV/index.html#contact",
    "href": "CV/index.html#contact",
    "title": "Louis Teitelbaum’s CV",
    "section": "",
    "text": "louist@post.bgu.ac.il\n Home Page |  Blog\n GitHub |  LinkedIn"
  },
  {
    "objectID": "CV/index.html#languages",
    "href": "CV/index.html#languages",
    "title": "Louis Teitelbaum’s CV",
    "section": "",
    "text": "English - native\nHebrew - fluent"
  },
  {
    "objectID": "CV/index.html#technical-skills",
    "href": "CV/index.html#technical-skills",
    "title": "Louis Teitelbaum’s CV",
    "section": "",
    "text": "Programming: R, Python, SQL, Git(Hub), SPSS, Excel\nDocumentation: (R)Markdown/Quarto, JupyterLab, Google Colab, Google Docs/Sheets\nDesign: ggplot2, Gimp, Photoshop, Inkscape\nDeployment: Shiny, Streamlit\nMachine Learning: Bayesian inference, multilevel modeling, logistic and binomial regression, PCA, random forest, elastic-net regression, k-means clustering, t-SNE, word/document embeddings, sentiment analysis"
  },
  {
    "objectID": "CV/index.html#title",
    "href": "CV/index.html#title",
    "title": "Louis Teitelbaum’s CV",
    "section": "Louis Teitelbaum",
    "text": "Louis Teitelbaum"
  },
  {
    "objectID": "CV/index.html#professional-experience",
    "href": "CV/index.html#professional-experience",
    "title": "Louis Teitelbaum’s CV",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nNeurocognitive Psychology Researcher\nN/A\nN/A\nSince 2020\nWorking Memory and Predictive Processing: - Thesis work under supervision of Prof. Yoav Kessler, Ben Gurion University\nLanguage and Communication: - Thesis work under supervision of Prof. Bruno Galantucci, Yeshiva University Psychology Department - Developed novel Natural Language Processing (NLP) based metrics for diadic conversations using Python and R. - Wrote peer review - Designed research agenda and experimental paradigm for the continuation of research post-COVID-19 - Read over 100 scholarly articles in the field of Language and Communication\nPower Dynamics in Long-Term Romantic Relationships: - Research Assistant for Dr. Jenny Isaacs, Yeshiva University Psychology Department - Researched and designed survey for data collection. - Analyzed data in SPSS and R; finalized analysis for publication. - Logged over 200 hours as a primary contributor to project.\n\n\nR Developer\nN/A\nN/A\nSince 2021\n\nDeveloper of wormsplot for visualizing the movements of historical figures; Read the blog post."
  },
  {
    "objectID": "CV/index.html#education",
    "href": "CV/index.html#education",
    "title": "Louis Teitelbaum’s CV",
    "section": "Education",
    "text": "Education\n\nM.A. in Experimental Cognitive Psychology\nBen-Gurion University\nBe’er Sheva, Israel\n2022 - Present\nSupervisor: Prof. Yoav Kessler\n\n\nB.A. in Psychology with Minor in Biology\nYeshiva University Honors\nNew York, NY\n2018 - 2021\nGraduated Magna Cum Laude\nActivities and societies: Neuroscience Club President"
  },
  {
    "objectID": "CV/index.html#awards",
    "href": "CV/index.html#awards",
    "title": "Louis Teitelbaum’s CV",
    "section": "Awards and Scholarships",
    "text": "Awards and Scholarships\n\nChaplain Joseph Hoenig Memorial Award for Excellence in the Study of Psychology\nYeshiva University Psychology Department\nNew York, NY\nSep 2021\n\n\nAmerican Psychology-Law Society Grant for Undergraduate Research\nAmerican Psychological Association Division 41: American Psychology-Law Society\nN/A\nJan 2021\nFor research in marital power dynamics in divorce mediation.\n\n\nThree funding awards from the Yeshiva University Honors Program\nYeshiva University Honors Program\nNew York, NY\n2021"
  },
  {
    "objectID": "CV/index.html#pubs",
    "href": "CV/index.html#pubs",
    "title": "Louis Teitelbaum’s CV",
    "section": "Scientific Publications and Presentations",
    "text": "Scientific Publications and Presentations\n\nTheses\nN/A\nN/A\nN/A\n\nTeitelbaum, L. (2021). When faithful informational exchange is just not worth it: Reformulation ability as a predictor of other-initiated repair [Honors thesis, Yeshiva University]. Yeshiva Academic Institutional Repository.\n\n\n\nPoster Presentations\nN/A\nN/A\nN/A\n\nTeitelbaum, L. & Galantucci, B. (2021). When faithful informational Exchanges are just too much work: Repair avoidance and reformulation cost. Poster presented at the 2021 Association for Psychological Science (APS) Virtual Convention.\nTeitelbaum, L., Isaacs, J., & Pittinsky, N. (2021). Is power all bad? Marital power imbalances and effective role division. Poster presented at the 2021 Association for Psychological Science (APS) Virtual Convention."
  },
  {
    "objectID": "CV/cover_letter.html",
    "href": "CV/cover_letter.html",
    "title": "Louis Teitelbaum’s Cover Letter",
    "section": "",
    "text": "louist@post.bgu.ac.il\n Home Page |  Blog\n GitHub |  LinkedIn"
  },
  {
    "objectID": "CV/cover_letter.html#contact",
    "href": "CV/cover_letter.html#contact",
    "title": "Louis Teitelbaum’s Cover Letter",
    "section": "",
    "text": "louist@post.bgu.ac.il\n Home Page |  Blog\n GitHub |  LinkedIn"
  },
  {
    "objectID": "CV/cover_letter.html#title",
    "href": "CV/cover_letter.html#title",
    "title": "Louis Teitelbaum’s Cover Letter",
    "section": "Louis Teitelbaum",
    "text": "Louis Teitelbaum\n\n29th June 2023\n Dear Autodesk Construction Solutions team,\nMy name is Louis Teitelbaum. I am a master’s student in data science and cognitive psychology at Ben Gurion University, now finishing my first year. I have three years of experience working with quantitative data in R, Python, SPSS, and Excel. In that time, I’ve worked on research in three different labs in two different countries, and built numerous side projects on my own time (take a look at my website!). Among other topics, I’ve researched cognitive dynamics in collaborative dialogue (using NLP), the efficiency of the police emergency response hotline in Israel (1-0-0), and in-group–out-group bias in corporate organizational structure. Given your advertised requirements, I believe my skills and experience in learning new coding tools on short notice (I learned R and Python on my own) will help me quickly become useful to the company.\nI have always been interested in accessible data visualization, and this year I am especially excited about interactive dashboards. I’ve learned Streamlit for Python and Shiny for R (examples of my work in both are on my website) and am now proficient in SQL. Working with these tools has made me curious to learn more and to integrate effective analysis with effective communication. Autodesk Construction Solutions seems like a perfect fit for these goals.\n Thank you for the opportunity,\n\nLouis"
  },
  {
    "objectID": "CV/cover_letter_hebrew.html",
    "href": "CV/cover_letter_hebrew.html",
    "title": "Louis Teitelbaum’s Cover Letter",
    "section": "",
    "text": "louist@post.bgu.ac.il\n Home Page |  Blog\n GitHub |  LinkedIn"
  },
  {
    "objectID": "CV/cover_letter_hebrew.html#contact",
    "href": "CV/cover_letter_hebrew.html#contact",
    "title": "Louis Teitelbaum’s Cover Letter",
    "section": "",
    "text": "louist@post.bgu.ac.il\n Home Page |  Blog\n GitHub |  LinkedIn"
  },
  {
    "objectID": "CV/cover_letter_hebrew.html#title",
    "href": "CV/cover_letter_hebrew.html#title",
    "title": "Louis Teitelbaum’s Cover Letter",
    "section": "לואיס טייטלבאום",
    "text": "לואיס טייטלבאום\n\n29 ביוני, 2023\n כללית חדשנות שלום,\nשמי לואיס טייטלבאום, סטודנט לתואר שני בפסיכולוגיה קוגניטיבית ומדעי הנתונים באוניברסיטת בן גוריון. יש לי ניסיון של שלוש שנים בניתוח נתונים כמותיים בR וPython. בשנים האלו, תרמתי למחקר בשלוש מעבדות שונות בשתי מדינות שונות, ובניתי פרויקטים רבים אחרים בזמני האישי (צפו באתר שלי!). אני מנוסה בשיטות עדכניות בלמידת מכונה כגון multilevel modeling, Bayesian inference, random forest, XGBoost, elastic-net regression, ועוד. כיום אני מתחיל לעבוד עם רשתות נוירונים עמוקים מתוך בניית מודל לנבא סיכון עתידי מהקלטות של שיחות במוקד החירום המשטרתי. הייתי שמח מאוד ליישם את הידע הזה (ולרכוש ידע חדש) אצלכם בכללית. בנוסף על כך, אני דובר אנגלית ברמת שפת-אם, ומסוגל לבצע סקירת ספרות בזריזות ובעומק. אני חושב שהיכולות שלי באנגלית, בספרות אקדמית בפרט, תכנות בR (שאותו התחלתי ללמוד כשנתיים לפני רוב הסטודנטים במגמה) יכולות להוות ערך ייחודי לחברה במהלך הפרקטיקום.\n תודה רבה על ההזדמנות,\n\nלואיס"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Louis Teitelbaum",
    "section": "",
    "text": "Load Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAmericans Have Eight Kinds of Days\n\n\n\n\n\n\n\nR\n\n\nClustering\n\n\nOpen Data\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nAdvanced Machine Learning Approaches for Detecting Trolls on Twitter\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nNLP\n\n\nLarge Language Models\n\n\nTransformers\n\n\nClassification\n\n\nPartial Least Squares\n\n\nDimensionality Reduction\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nShockingly, Most Reddit Environmentalists are not Greta Thunberg\n\n\n\n\n\n\n\nR\n\n\nData Mining\n\n\nAPI\n\n\nNLP\n\n\nLarge Language Models\n\n\nDocument Embeddings\n\n\nSentiment Analysis\n\n\nSocial Media\n\n\nMultilevel Modeling\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nComparing Four Methods of Sentiment Analysis\n\n\n\n\n\n\n\nR\n\n\nNLP\n\n\nSentiment Analysis\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nDo Employees Tend to Have the Same First Name as Their Bosses?\n\n\n\n\n\n\n\nR\n\n\nbrms\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nDesigning a Poster to Visualize the Timeline of Philosophers in the Islamic World\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nDataViz\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nThe History of Semantic Spaces\n\n\n\n\n\n\n\nPython\n\n\nHistory\n\n\nDataViz\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Questions in Dialogue\n\n\n\n\n\n\n\nR\n\n\nBayesian Statistics\n\n\nDialogue\n\n\nCognitive Science\n\n\nCognitive Linguistics\n\n\nMultilevel Modeling\n\n\nggplot2\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nTracking Cognitive Performance with Online Chess\n\n\n\n\n\n\n\nR\n\n\nCognitive Science\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nתכלית מוקד 100 להציל חיים\n\n\n\n\n\n\n\nR\n\n\nData Mining\n\n\nScraping\n\n\nGovernment\n\n\nעברית\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nStatistically Optimal Wordle\n\n\n\n\n\n\n\nR\n\n\nprobability theory\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\nNo matching items"
  }
]