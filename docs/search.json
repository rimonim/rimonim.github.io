[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Louis Teitelbaum",
    "section": "",
    "text": "Americans Have Eight Kinds of Days\n\n\n\n\n\n\n\nR\n\n\nClustering\n\n\nOpen Data\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nAdvanced Machine Learning Approaches for Detecting Trolls on Twitter\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nNLP\n\n\nLarge Language Models\n\n\nTransformers\n\n\nClassification\n\n\nPartial Least Squares\n\n\nDimensionality Reduction\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nShockingly, Most Reddit Environmentalists are not Greta Thunberg\n\n\n\n\n\n\n\nR\n\n\nData Mining\n\n\nAPI\n\n\nNLP\n\n\nLarge Language Models\n\n\nDocument Embeddings\n\n\nSentiment Analysis\n\n\nSocial Media\n\n\nMultilevel Modeling\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nComparing Four Methods of Sentiment Analysis\n\n\n\n\n\n\n\nR\n\n\nNLP\n\n\nSentiment Analysis\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nDo Employees Tend to Have the Same First Name as Their Bosses?\n\n\n\n\n\n\n\nR\n\n\nbrms\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nDesigning a Poster to Visualize the Timeline of Philosophers in the Islamic World\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nDataViz\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nThe History of Semantic Spaces\n\n\n\n\n\n\n\nPython\n\n\nHistory\n\n\nDataViz\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Questions in Dialogue\n\n\n\n\n\n\n\nR\n\n\nBayesian Statistics\n\n\nDialogue\n\n\nCognitive Science\n\n\nCognitive Linguistics\n\n\nMultilevel Modeling\n\n\nggplot2\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nTracking Cognitive Performance with Online Chess\n\n\n\n\n\n\n\nR\n\n\nCognitive Science\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nתכלית מוקד 100 להציל חיים\n\n\n\n\n\n\n\nR\n\n\nData Mining\n\n\nScraping\n\n\nGovernment\n\n\nעברית\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nStatistically Optimal Wordle\n\n\n\n\n\n\n\nR\n\n\nprobability theory\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html",
    "href": "blog/reddit_environmentalists/index.html",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "",
    "text": "Greta Thunberg may be the most well-known climate activist today. Along with her climate activism, she is known for her psychiatric diagnoses. In fact, these two aspects of her public persona often go together - autism and anxiety disorders are the superpowers that allow her to take a principled stand.\nTold by Thunberg herself, this is an inspiring and compelling claim. But is it true in general that autistic and/or anxious thought patterns cause people to take the sustainability crisis more seriously? Is this true even when it comes to smaller-scale activism than Thunberg’s - say, involvement with environmentalist groups on Reddit?"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#neutralized-ccr",
    "href": "blog/reddit_environmentalists/index.html#neutralized-ccr",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "Neutralized CCR",
    "text": "Neutralized CCR\nIn my last post, I explored various methods of quantifying psychological content in text, including contextualized construct representation (CCR)2, which leverages the power of large language models to turn established psychiatric questionnaires into embeddings - lists of activations of neurons in the model - that can be compared to the same model’s embeddings of other texts.\nToward the end of that post, I raised a potential difficulty with this method: How can I be sure that I am not measuring the extent to which people write like they’re writing a questionnaire? This is critical, since questionnaires tend to be written in formal, well organized, full sentences, and certain psychological constructs might lead to people writing less - or more - like that.\nThe solution I proposed was to create a “neutralized” version of the questionnaire average embedding, which reflects each original question and its grammatical negation. By subtracting out this neutralized embedding (which nevertheless maintains its questionnaire-ness all around) from the original embedding, I can isolate the psychological construct of interest without fear of confounds from the particular writing style of the questionnaire. This is the approach I will take here, using the Adult Social Behavior Questionnaire (ASBQ)3 for clinical assessment of autism spectrum disorder in adults. The questionnaire includes six subscales: reduced contact, reduced empathy, reduced interpersonal insight, violations of social conventions, insistence on sameness, and sensory stimulation and motor stereotypes. Excluding the last one (I have a hard time imagining how it would apply to Reddit activity), I will treat each subscale on its own.\nAs in my last post, I am using embeddings from the second to last layer of BERT (base uncased).\n\n# Adult Social Behavior Questionnaire (ASBQ); self-report, excluding 'sensory stimulation and motor stereotypes'\nreduced_contact &lt;- c(\n  \"i do not take the initiative in contacts with other people\",\n  \"i have little or no interest in socializing with others\",\n  \"i ignore invitations from others to do something with them\",\n  \"i avoid people who try to make contact with me\",\n  \"the only contact i have with others is when i have to buy something or arrange something, for example with people in a shop or in a government office\",\n  \"i am a loner, even in a group i hold myself apart\",\n  \"i do not enjoy doing things with other people, for example, doing a chore together or going somewhere together\"\n  )\nreduced_empathy &lt;- c(\n  \"i find it difficult to put myself in someone else’s shoes, for example, i can not see why someone is angry\",\n  \"i am unaware of other people’s emotional needs, for example, i do not encourage other people or reassure them\",\n  \"i find it hard to sense what someone else will like or think is nice\",\n  \"i am not really bothered by someone else in pain\",\n  \"i do not notice when someone is upset or has problems\",\n  \"the reason why i would contact others is to get things done rather than because i am interested in them\",\n  \"i do not show sympathy when others hurt themselves or are unhappy\"\n  )\nreduced_interpersonal_insight &lt;- c(\n  \"i do not get jokes\",\n  \"i take everything literally, for example, i do not understand certain expressions\",\n  \"i am very naïve; i believe everything i am told\",\n  \"it is easy to take advantage of me or get me to do other people’s dirty work\",\n  \"i do not notice when others make fun of me\",\n  \"i find it hard to follow the gist of a conversation—i miss the point\",\n  \"i need an explanation before i understand the meaning behind someone’s words\",\n  \"i give answers that are not relevant because i have not really understood the question\"\n  )\nviolations_of_social_conventions &lt;- c(\n  \"i do not differentiate between friends and strangers, for example, i do not care who i am with\",\n  \"i seek contact with anyone and everyone; i show no reserve\",\n  \"i touch people when it is not suitable, for example, i hug virtual strangers\",\n  \"the questions i ask are too personal, or i tell others things that are too personal\",\n  \"i behave the same wherever i am; it makes no difference to me whether i am at home or somewhere else (visiting others, at work, in the streets)\",\n  \"i ask strangers for things i need, for example for food or drink if i am hungry or thirsty\"\n  )\ninsistence_on_sameness &lt;- c(\n  \"i panic when things turn out differently than i am used to\",\n  \"i resist change; if it were left up to me, everything would stay the same\",\n  \"i want to do certain things in exactly the same way every time\",\n  \"i do not like surprises, for example, unexpected visitors\",\n  \"i do not like a lot of things happening at once\",\n  \"i really need fixed routines and things to be predictable\",\n  \"i hate it when plans are changed at the last moment\",\n  \"it takes me ages to get used to somewhere new\"\n  )\n#~~~~~~~~~~~~~~~~~~~~~~~~# Negated Versions\nreduced_contact_neg &lt;- c(\n  \"i take the initiative in contacts with other people\",\n  \"i have lots of interest in socializing with others\",\n  \"i do not ignore invitations from others to do something with them\",\n  \"i do not avoid people who try to make contact with me\",\n  \"i have contact with others all the time, not just when i have to buy something or arrange something, or with people in a shop or in a government office\",\n  \"i am not a loner, in a group i do not hold myself apart\",\n  \"i enjoy doing things with other people, for example, doing a chore together or going somewhere together\"\n  )\nreduced_empathy_neg &lt;- c(\n  \"i do not find it difficult to put myself in someone else’s shoes, for example, i can see why someone is angry\",\n  \"i am aware of other people’s emotional needs, for example, i encourage other people or reassure them\",\n  \"i do not find it hard to sense what someone else will like or think is nice\",\n  \"i really bothered by someone else in pain\",\n  \"i notice when someone is upset or has problems\",\n  \"the reason why i would contact others is because i am interested in them rather than to get things done\",\n  \"i show sympathy when others hurt themselves or are unhappy\"\n  )\nreduced_interpersonal_insight_neg &lt;- c(\n  \"i get jokes\",\n  \"i do not take everything literally, for example, i understand expressions\",\n  \"i am not very naïve; i do not believe everything i am told\",\n  \"it is not easy to take advantage of me or get me to do other people’s dirty work\",\n  \"i notice when others make fun of me\",\n  \"i do not find it hard to follow the gist of a conversation—i do not miss the point\",\n  \"i do not need an explanation before i understand the meaning behind someone’s words\",\n  \"i give answers that are relevant because i have really understood the question\"\n  )\nviolations_of_social_conventions_neg &lt;- c(\n  \"i differentiate between friends and strangers, for example, i care who i am with\",\n  \"i do not seek contact with anyone and everyone; i show reserve\",\n  \"i do not touch people when it is not suitable, for example, i do not hug virtual strangers\",\n  \"the questions i ask are not too personal; i do not tell others things that are too personal\",\n  \"i do not behave the same wherever i am; it makes a difference to me whether i am at home or somewhere else (visiting others, at work, in the streets)\",\n  \"i do not ask strangers for things i need, for example for food or drink if i am hungry or thirsty\"\n  )\ninsistence_on_sameness_neg &lt;- c(\n  \"i do not panic when things turn out differently than i am used to\",\n  \"i do not resist change; if it were left up to me, nothing would stay the same\",\n  \"i do not want to do things in exactly the same way every time\",\n  \"i like surprises, for example, unexpected visitors\",\n  \"i like a lot of things happening at once\",\n  \"i do not really need fixed routines or things to be predictable\",\n  \"i do not hate it when plans are changed at the last moment\",\n  \"it does not take me ages to get used to somewhere new\"\n  )\n\n#~~~~~~~~~~~~~~~~~~~~~~~~# Generate Embeddings\n\n# Mean embeddings of each subscale and its negation\nreduced_contact &lt;- textEmbed(reduced_contact)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nreduced_empathy &lt;- textEmbed(reduced_empathy)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nreduced_interpersonal_insight &lt;- textEmbed(reduced_interpersonal_insight)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nviolations_of_social_conventions &lt;- textEmbed(violations_of_social_conventions)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\ninsistence_on_sameness &lt;- textEmbed(insistence_on_sameness)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\n\nreduced_contact_neg &lt;- textEmbed(reduced_contact_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nreduced_empathy_neg &lt;- textEmbed(reduced_empathy_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nreduced_interpersonal_insight_neg &lt;- textEmbed(reduced_interpersonal_insight_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\nviolations_of_social_conventions_neg &lt;- textEmbed(violations_of_social_conventions_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\ninsistence_on_sameness_neg &lt;- textEmbed(insistence_on_sameness_neg)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\n\n# Neutralized Questionnaire - Initial - mean(Initial, Negated)\nreduced_contact_neutralized &lt;- reduced_contact - (reduced_contact + reduced_contact_neg)/2\nreduced_empathy_neutralized &lt;- reduced_empathy - (reduced_empathy + reduced_empathy_neg)/2\nreduced_interpersonal_insight_neutralized &lt;- reduced_interpersonal_insight - (reduced_interpersonal_insight + reduced_interpersonal_insight_neg)/2\nviolations_of_social_conventions_neutralized &lt;- violations_of_social_conventions - (violations_of_social_conventions + violations_of_social_conventions_neg)/2\ninsistence_on_sameness_neutralized &lt;- insistence_on_sameness - (insistence_on_sameness + insistence_on_sameness_neg)/2"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#raspergers",
    "href": "blog/reddit_environmentalists/index.html#raspergers",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "r/aspergers",
    "text": "r/aspergers\nMy second metric for autistic tendencies is less sensitive to the nuances of various ASD symptoms, but is more directly tailored to the task at hand. The r/aspergers subreddit describes itself as “the internet’s largest community of people affected by Autism Spectrum Disorder”. So here, I am simply going to compute the average embedding of posts in this group and use it as a paradigm of the construct of interest.\n\nASD_threads &lt;- find_thread_urls(subreddit = \"aspergers\")\n\nASD_threads$full_text &lt;- preprocess(paste(ASD_threads$title, ASD_threads$text))\n\n# Embedding\nASD_threads_embeddings &lt;- textEmbed(ASD_threads$full_text)$texts[[1]]\n\n# Average Embedding\nASD &lt;- ASD_threads_embeddings %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#kaggle-dataset",
    "href": "blog/reddit_environmentalists/index.html#kaggle-dataset",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "Kaggle Dataset",
    "text": "Kaggle Dataset\nI could easily take the same approach for anxiety as I did for autism. I could even use questionnaires designed specifically for OCD and selective mutism, the diagnoses mentioned by Greta Thunberg in the quote above. Nevertheless, I am going to use a different, more straightforward method: the Students Anxiety and Depression Dataset includes 733 handcoded examples of social media posts and comments that reflect anxiety. I will simply take the average BERT embedding of these as the quintessence of anxiety on social media.\n\nanxiety_texts &lt;- readxl::read_xlsx(\"dataset.xlsx\") %&gt;% \n  filter(label == 1) %&gt;% pull(text)\n\nanxiety_texts &lt;- preprocess(anxiety_texts)\n\nanxiety &lt;- textEmbed(anxiety_texts)$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#rocd",
    "href": "blog/reddit_environmentalists/index.html#rocd",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "r/OCD",
    "text": "r/OCD\nAs with ASD above, my second metric will simply be an average embedding of posts on a disorder-specific subreddit. This time, r/OCD.\n\nOCD_threads &lt;- find_thread_urls(subreddit = \"OCD\")\n\nOCD_threads$full_text &lt;- preprocess(paste(OCD_threads$title, OCD_threads$text))\n\nOCD_threads_subset &lt;- slice_sample(OCD_threads, n = 100)\n\n# Embedding\nOCD_threads_embeddings &lt;- textEmbed(OCD_threads$full_text)$texts[[1]]\n\n# Average Embedding\nOCD &lt;- OCD_threads_embeddings %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)"
  },
  {
    "objectID": "blog/reddit_environmentalists/index.html#footnotes",
    "href": "blog/reddit_environmentalists/index.html#footnotes",
    "title": "Shockingly, Most Reddit Environmentalists are not Greta Thunberg",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThunberg, Greta (24 November 2018). School strike for climate – save the world by changing the rules. TEDxStockholm. Event occurs at 1:46.↩︎\nAtari, M., Omrani, A., & Dehghani, M. (2023, February 24). Contextualized construct representation: Leveraging psychometric scales to advance theory-driven text analysis. https://doi.org/10.31234/osf.io/m93pd↩︎\nHorwitz, E., Schoevers, R.A., Ketelaars, C., Kan, C.C., Lammeren, A.V., Meesters, Y., Spek, A.A., Wouters, S., Teunisse, J.P., Cuppen, L., Bartels, A.A., Schuringa, E., Moorlag, H., Raven, D., Wiersma, D., Minderaa, R.B., & Hartman, C.A. (2016). Clinical assessment of ASD in adults using self- and other-report : Psychometric properties and validity of the Adult Social Behavior Questionnaire (ASBQ). Research in Autism Spectrum Disorders, 24, 17-28.↩︎\nTaylor, E. C., Livingston, L. A., Callan, M. J., Hanel, P. H. P., & Shah, P. (2021). Do autistic traits predict pro-environmental attitudes and behaviors, and climate change belief? Journal of Environmental Psychology, 76, Article 101648. https://doi.org/10.1016/j.jenvp.2021.101648↩︎"
  },
  {
    "objectID": "blog/troll_classification/index.html",
    "href": "blog/troll_classification/index.html",
    "title": "Advanced Machine Learning Approaches for Detecting Trolls on Twitter",
    "section": "",
    "text": "Content Warning\n\n\n\nThis report includes texts written by internet trolls, many of which are extremely offensive."
  },
  {
    "objectID": "blog/troll_classification/index.html#abstract",
    "href": "blog/troll_classification/index.html#abstract",
    "title": "Advanced Machine Learning Approaches for Detecting Trolls on Twitter",
    "section": "Abstract",
    "text": "Abstract\nSocial media platforms such as Twitter have revolutionized how people interact, share information, and express their opinions. However, this rapid expansion has also brought with it an alarming rise in malicious activities, with online trolls exploiting the platform to spread hate, misinformation, and toxicity. Detecting and mitigating such trolls have become critical in maintaining a healthy digital environment and safeguarding the well-being of users.\nIn this report, I present an exploratory investigation into the development of a cutting-edge machine learning model for the identification and classification of trolls on Twitter. In particular, I train and test three model architectures: Partial least squares (PLS) regression, boosting, and a fine-tuned transformer neural network."
  },
  {
    "objectID": "blog/troll_classification/index.html#exploratory-analysis-and-feature-selection",
    "href": "blog/troll_classification/index.html#exploratory-analysis-and-feature-selection",
    "title": "Advanced Machine Learning Approaches for Detecting Trolls on Twitter",
    "section": "Exploratory Analysis and Feature Selection",
    "text": "Exploratory Analysis and Feature Selection\nThe training data for this report consist of short texts from Twitter, each manually labeled with label indicating whether it is or is not a troll.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(tidytext)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(pls)\n\n\nAttaching package: 'pls'\n\nThe following object is masked from 'package:caret':\n\n    R2\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\nlibrary(varrank)\n\nload(\"troll_classification.RData\")\n\n# train &lt;- read_csv(\"train.csv\") %&gt;% mutate(label = factor(label))\n# test &lt;- read_csv(\"test.csv\")\n\nhead(train)\n\n# A tibble: 6 × 3\n  rowid content                                                            label\n  &lt;dbl&gt; &lt;chr&gt;                                                              &lt;fct&gt;\n1     1 'How to Talk to Girls'. I'm going to write a gay centred spinoff … 0    \n2     2 'Turns out not where but who you're with that really matters.' ;)… 0    \n3     3 'you can do it!' kick ass fdo!                                     0    \n4     4 --- each confit will make the fat saltier (it's already pretty sa… 0    \n5     5 --- that shit right there fool ... is FUCKING TIGHT!!!! --- http:… 1    \n6     6 --- you may be working from the same book as me; I'm confiting po… 0    \n\n\nThese data represent a more difficult classification task than many real-world applications, as no information is given about thread-level context or other texts produced by the same account. This report will focus purely on features of the individual text.\n\nWord Clouds\nAs an initial step in exploratory data analysis, I generated three word clouds, each on a different scope of analysis: individual words, shingles (that is, short sequences of characters), and n-grams (sequences of multiple words). It is important to perform initial analyses on different scopes, since the final tokenization method will constrain the type of features to which the model will have access. For example, it may be that trolls are more likely to use strings of punctuation like “!?!?”. A model using a word-based tokenizer may ignore punctuation altogether and miss such an informative feature. On the other hand, sequences of multiple words may reflect semantic structure in ways that 4-character shingles cannot.\n\ntroll_words &lt;- train %&gt;% \n  filter(label == 1) %&gt;%\n  mutate(clean_text = tm::removeNumbers(content),\n         clean_text = tm::removePunctuation(clean_text),\n         clean_text = tm::stripWhitespace(clean_text)) %&gt;%\n  unnest_tokens(word, clean_text, to_lower = FALSE) %&gt;%\n  count(word, sort=T) %&gt;%\n  mutate(troll_prop = n/sum(n))\n\nnotroll_words &lt;- train %&gt;% \n  filter(label == 0) %&gt;%\n  mutate(clean_text = tm::removeNumbers(content),\n         clean_text = tm::removePunctuation(clean_text),\n         clean_text = tm::stripWhitespace(clean_text)) %&gt;%\n  unnest_tokens(word, clean_text, to_lower = FALSE) %&gt;%\n  count(word, sort=T) %&gt;%\n  mutate(notroll_prop = n/sum(n))\n\nfull_words &lt;- full_join(troll_words, notroll_words, by = \"word\") %&gt;%\n  mutate(troll_prop = ifelse(is.na(troll_prop), 0, troll_prop),\n         notroll_prop = ifelse(is.na(notroll_prop), 0, notroll_prop)) %&gt;%\n  mutate(troll_notroll_diff = troll_prop - notroll_prop,\n         color = ifelse(troll_notroll_diff &gt; 0, \"red\", \"blue\"),\n         abs = abs(troll_notroll_diff)) %&gt;%\n  arrange(desc(abs))\nWords displaying the greatest difference in usage frequency between troll and non-troll texts.\n\n\nwordcloud(words = full_words$word, freq = full_words$abs, min.freq = 0,\n          max.words = 100, random.order = FALSE, rot.per = 0.3, \n          colors=full_words$color, ordered.colors=TRUE)\n\n\n\n\nThe above word cloud makes it clear that certain words are extremely indicative of troll text, and they are nearly all obscenities and/or insults. It also seems clear that trolls write in the third person more often.\nNotably, there do look to be a number of “stopwords” (e.g. “u”, “ur”, “a”, “he”, “hes” and “her”) with predictive properties on the troll side, and “i”, “to”, “the”, and “if” on the non-troll side. These short, high frequency words are often removed in pre-processing. Here though, they seem to have important predictive value.\nFinally, it looks like question words (e.g. “who”, “what”, “how”) might be negative indicator of trolls. This will be further investigated below.\n\ntroll_shingles &lt;- train %&gt;% \n  filter(label == 1) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(shingle, clean_text, token = \"character_shingles\", n = 4L,\n                strip_non_alphanum = FALSE, to_lower = FALSE) %&gt;%\n  count(shingle, sort=T) %&gt;%\n  mutate(troll_prop = n/sum(n))\n\nnotroll_shingles &lt;- train %&gt;% \n  filter(label == 0) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(shingle, clean_text, token = \"character_shingles\", n = 4L,\n                strip_non_alphanum = FALSE, to_lower = FALSE) %&gt;%\n  count(shingle, sort=T) %&gt;%\n  mutate(notroll_prop = n/sum(n))\n\nfull_shingles &lt;- full_join(troll_shingles, notroll_shingles, by = \"shingle\") %&gt;%\n  mutate(troll_prop = ifelse(is.na(troll_prop), 0, troll_prop),\n         notroll_prop = ifelse(is.na(notroll_prop), 0, notroll_prop)) %&gt;%\n  mutate(troll_notroll_diff = troll_prop - notroll_prop,\n         color = ifelse(troll_notroll_diff &gt; 0, \"red\", \"blue\"),\n         abs = abs(troll_notroll_diff)) %&gt;%\n  arrange(desc(abs))\n\n\nwordcloud(words = full_shingles$shingle, freq = full_shingles$abs, min.freq = 0,\n          max.words = 250, random.order = FALSE, rot.per = 0.3, \n          colors=full_shingles$color, ordered.colors=TRUE)\n\n\n\n\nWords displaying the greatest difference in usage frequency between troll and non-troll texts.\n\n\n\n\nThere are a number of capitalizations, long strings of repeating letters (which shingles are more likely to capture), and punctuation (e.g. ?!?!). The shingles scope of analysis seems like it is capturing some important details. This will be worthwhile if I can leverage these features in the dimensionality reduction process.\n\ntroll_ngrams &lt;- train %&gt;% \n  filter(label == 1) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(ngram, clean_text, token = \"skip_ngrams\", n = 3L, k = 1) %&gt;%\n  count(ngram, sort=T) %&gt;%\n  mutate(troll_prop = n/sum(n))\n\nnotroll_ngrams &lt;- train %&gt;% \n  filter(label == 0) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(ngram, clean_text, token = \"skip_ngrams\", n = 3L, k = 1) %&gt;%\n  count(ngram, sort=T) %&gt;%\n  mutate(notroll_prop = n/sum(n))\n\nfull_ngrams &lt;- full_join(troll_ngrams, notroll_ngrams, by = \"ngram\") %&gt;%\n  mutate(troll_prop = ifelse(is.na(troll_prop), 0, troll_prop),\n         notroll_prop = ifelse(is.na(notroll_prop), 0, notroll_prop)) %&gt;%\n  mutate(troll_notroll_diff = troll_prop - notroll_prop,\n         color = ifelse(troll_notroll_diff &gt; 0, \"red\", \"blue\"),\n         abs = abs(troll_notroll_diff)) %&gt;%\n  arrange(desc(abs))\n\n\nwordcloud(words = full_ngrams$ngram, freq = full_ngrams$abs, min.freq = 0,\n          max.words = 150, random.order = FALSE, rot.per = 0.3, \n          colors=full_ngrams$color, ordered.colors=TRUE)\n\n\n\n\nWords displaying the greatest difference in usage frequency between troll and non-troll texts.\n\n\n\n\nOn the level of n-grams, the most informative predictors look to be the same vulgar slurs found on the single-word level. Nevertheless, there are many multi-word sequences in this word cloud. Especially striking is the common appearance of the word “you” on both the troll and non-troll sides, in varying contexts. Whereas in single-word analysis “YOU” and “your” seemed to be indicative of trolls, n-gram level analysis makes it clear that certain phrases such as “would you”, “do you think”, “have you ever”, and “if you” are in fact much highly indicative of non-trolls. This suggests that allowing n-grams may increase the predictive abilities of the model, providing the dimentionality reduction works properly.\n\n\nOther Important Features\nWhile single words, shingles, and n-grams seem to cover a lot of differences between troll and non-troll texts, I can think of a few more features that may be relevant but will not be detected by any of the levels of tokenization analysis above. Here are some things that will not be captured in tokenization, but might be indicative of trolls:\n\nuse of all-caps text\nuse of punctuation in normal/unconventional ways (e.g. period at the end of sentence, three exclamation points, ***, …, quotes)\nemoticons (e.g. “:-)”, “&lt;3”, “:3”)\nuser tags\nsame character many times in a row\nreadability, as measured by various algorithms (e.g. “Scrabble”, “SMOG.simple”, “Traenkle.Bailer2”, “meanWordSyllables”)\n\n\n# Get list of emoticons and add escapes for use as regex\nemoticons &lt;- str_replace_all(str_replace_all(lexicon::hash_emoticons$x, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\"), \"([.|()^{}+$*?]|\\\\[|\\\\])\", \"\\\\\\\\\\\\1\")\ncount_emoticons &lt;- function(x){\n  count &lt;- rep_len(0L, length(x))\n  for (i in 1:length(emoticons)) {\n    count &lt;- count + str_count(x, emoticons[i])\n  }\n  count\n}\nquestion_words &lt;- c(\"who\", \"what\", \"when\", \"where\", \"how\", \"why\", \"whose\",\n                    \"Who\", \"What\", \"When\", \"Where\", \"How\", \"Why\", \"Whose\",\n                    \"Would\", \"Have\", \"Do\", \"Does\", \"Did\", \"Didn't\", \n                    \"Didnt\", \"Are\", \"Aren't\", \"Arent\")\ncount_question_words &lt;- function(x){\n  count &lt;- rep_len(0L, length(x))\n  for (i in 1:length(question_words)) {\n    count &lt;- count + str_count(x, question_words[i])\n  }\n  count\n}\nprofanity &lt;- str_replace_all(str_replace_all(lexicon::profanity_banned, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\"), \"([.|()^{}+$*?]|\\\\[|\\\\])\", \"\\\\\\\\\\\\1\")\ncount_profanity &lt;- function(x){\n  count &lt;- rep_len(0L, length(x))\n  for (i in 1:length(profanity)) {\n    count &lt;- count + str_count(str_to_lower(x), profanity[i])\n  }\n  count\n}\n\ntrain_features &lt;- train %&gt;% \n  mutate(ncaps = str_count(content, \"[A-Z]\"), # capital Letters\n         allcaps_words = str_count(content, \"\\\\b[A-Z]{2,}\\\\b\"), # words of ALLCAPS text\n         conventional_periods = str_count(content, \"[:alnum:]\\\\.[:space:]\"), # conventionally used periods\n         ellipses = str_count(content, \"\\\\.\\\\.\"), # ...\n         exclamation = str_count(content, \"\\\\!\\\\!\"), # !!\n         emoticons = count_emoticons(content),\n         question_words = count_question_words(content),\n         profanity = count_profanity(content),\n         noprofanity = as.integer(profanity == 0),\n         urls = str_count(content, \"http://\"),\n         words = str_count(content, '\\\\w+'),\n         quotations = str_count(content, '\".+\"'))\n\n# Readability measures\ntrain_features &lt;- train_features %&gt;% \n  bind_cols(quanteda.textstats::textstat_readability(train_features$content, \n                                                     measure = c(\"Scrabble\", \n                                                                 \"SMOG.simple\", \n                                                                 \"Traenkle.Bailer\",\n                                                                 \"meanWordSyllables\")) %&gt;% select(-document))\n\n\ntrain_features %&gt;% \n  mutate(label = if_else(label == 1, \"Trolls\", \"Non-trolls\")) %&gt;% \n  pivot_longer(ncaps:meanWordSyllables, names_to = \"feature\", values_to = \"value\") %&gt;% \n  group_by(label, feature) %&gt;% \n  mutate(mean_value = mean(value, na.rm = TRUE),\n         quantile_value_hi = quantile(value, probs = .95, na.rm = TRUE),\n         quantile_value_lo = quantile(value, probs = .05, na.rm = TRUE)) %&gt;% \n  slice_sample(n = 1000) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(label, value)) +\n    ggbeeswarm::geom_quasirandom(alpha = .1) +\n    geom_point(aes(y = quantile_value_hi), color = \"skyblue\", size = 3) + \n    geom_point(aes(y = quantile_value_lo), color = \"skyblue\", size = 3) + \n    geom_point(aes(y = mean_value), color = \"red\", size = 3) + \n    facet_wrap(~feature, scales = \"free\") +\n    labs(x = \"\", y = \"\") +\n    theme_bw()\n\n\n\n\nSome of these (e.g. conventional periods, ellipses, exclamation marks) will in fact be automatically captured by shingles. I’ll keep the ones that won’t (total emoticons, allcaps words, quotations, ncaps, question_words, profanity, lack of profanity, urls).\nLet’s take a closer look at the number of words in each text:\n\ntrain_features %&gt;% \n  mutate(label = if_else(label == 1, \"Trolls\", \"Non-trolls\")) %&gt;% \n  ggplot(aes(words, fill = label)) +\n    geom_density(alpha = .5) +\n    scale_x_continuous(limits = c(0, 50)) +\n    theme_bw()\n\nWarning: Removed 44 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nThese distributions do have notably different shapes: Non-troll texts are very commonly around 6 words long, and fall off sharply above that. Troll texts, on the other hand, are more evenly distributed between 5 and 25 words in length. This means that texts around 6 words long are disproportionately likely not to be trolls, whereas texts that are 14-26 words long are disproportionately likely to be trolls. I will therefore create two binary variables: short_text for texts under 13 words long, and med_text for texts 14-26 words long.\n\ntrain_features &lt;- train_features %&gt;% \n  mutate(short_text = as.integer(words &lt; 13),\n         med_text = as.integer((words &gt; 13) & (words &lt; 27))) %&gt;% \n  select(-words)"
  },
  {
    "objectID": "blog/troll_classification/index.html#retrain-best-model-on-full-training-set",
    "href": "blog/troll_classification/index.html#retrain-best-model-on-full-training-set",
    "title": "Advanced Machine Learning Approaches for Detecting Trolls on Twitter",
    "section": "Retrain best model on full training set",
    "text": "Retrain best model on full training set\nNow that the PLS model incorporating n-grams and shingles is established as the superior one, I will retrain it on the full training dataset before submitting it to the Kaggle competition.\n\n# Full-set PLS Model (10-fold cross-validation)\n# Using feature set with ngrams\nset.seed(123)\npls_mod_final &lt;- train(\n  label~., data = select(train_allfeatures, -c(rowid, content, clean_text)), method = \"pls\",\n  scale = TRUE,\n  trControl = trainControl(\"cv\", number = 10),\n  tuneLength = 4\n  )\n\nplot(pls_mod_final) # Still best with 2 components\n\n# Identify features found in train but not test set\ntest &lt;- read_csv(\"test.csv\")\n\ntest_shingles &lt;- test %&gt;% \n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(shingle, clean_text, token = \"character_shingles\", n = 4L,\n                strip_non_alphanum = FALSE, to_lower = FALSE) %&gt;%\n  count(shingle, sort=T)\nirrelevant_shingles &lt;- setdiff(top_shingles, test_shingles$shingle) # none missing!\n\ntest_ngrams &lt;- test %&gt;% \n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  unnest_tokens(ngram, clean_text, token = \"skip_ngrams\", n = 3L, k = 1) %&gt;%\n  count(ngram, sort=T)\nirrelevant_ngrams &lt;- setdiff(top_ngrams, test_ngrams$ngram) # list of 7\n\n# Add all features to test set\n\ntest_features &lt;- test %&gt;% \n  mutate(ncaps = str_count(content, \"[A-Z]\"), # capital Letters\n         allcaps_words = str_count(content, \"\\\\b[A-Z]{2,}\\\\b\"), # words of ALLCAPS text\n         conventional_periods = str_count(content, \"[:alnum:]\\\\.[:space:]\"), # conventionally used periods\n         ellipses = str_count(content, \"\\\\.\\\\.\"), # ...\n         exclamation = str_count(content, \"\\\\!\\\\!\"), # !!\n         emoticons = count_emoticons(content),\n         question_words = count_question_words(content),\n         profanity = count_profanity(content),\n         noprofanity = as.integer(profanity == 0),\n         urls = str_count(content, \"http://\"),\n         words = str_count(content, '\\\\w+'),\n         quotations = str_count(content, '\".+\"'),\n         short_text = as.integer(words &lt; 13),\n         med_text = as.integer((words &gt; 13) & (words &lt; 27)),\n         clean_text = tm::stripWhitespace(content)) %&gt;% \n  # Readability measures and quantized length\n  bind_cols(quanteda.textstats::textstat_readability(test_features$content, \n                                                     measure = c(\"Scrabble\", \n                                                                 \"SMOG.simple\", \n                                                                 \"Traenkle.Bailer\",\n                                                                 \"meanWordSyllables\"))) %&gt;% \n  select(c(rowid, content, allcaps_words, emoticons, question_words, profanity, noprofanity, quotations, SMOG.simple, Traenkle.Bailer, short_text, med_text, clean_text)) %&gt;%\n  ## Compute shingles\n  unnest_tokens(shingle, clean_text, token = \"character_shingles\", n = 4L,\n                strip_non_alphanum = FALSE, to_lower = FALSE, drop = FALSE) %&gt;% \n  # replace everything but top 1000 with placeholder\n  mutate(shingle = if_else(shingle %in% top_shingles, shingle, \"shingle\")) %&gt;% \n  group_by(across(everything())) %&gt;% summarise(n = n()) %&gt;% ungroup() %&gt;% \n  # pivot shingles to columns\n  pivot_wider(id_cols = rowid:clean_text, names_from = \"shingle\", values_from = \"n\", names_prefix = \"shingle_\", values_fill = 0L) %&gt;% \n  mutate(across(everything(), ~replace_na(.x, 0))) %&gt;% \n  ungroup()\n\n# add ngrams to other features as sparse features\ntest_features &lt;- test_features %&gt;% \n  select(c(rowid, content)) %&gt;%\n  mutate(clean_text = tm::stripWhitespace(content)) %&gt;%\n  ## Compute ngrams\n  unnest_tokens(ngram, clean_text, token = \"skip_ngrams\", n = 3L, k = 1) %&gt;%\n  # replace everything but top 1000 with placeholder\n  mutate(ngram = if_else(ngram %in% top_ngrams, ngram, \"ngram\")) %&gt;% \n  group_by(across(everything())) %&gt;% summarise(n = n()) %&gt;% ungroup() %&gt;% \n  # pivot ngrams to columns\n  pivot_wider(id_cols = rowid:content, names_from = \"ngram\", values_from = \"n\", names_prefix = \"ngram_\", values_fill = 0L) %&gt;% \n  mutate(across(everything(), ~replace_na(.x, 0))) %&gt;% \n  full_join(test_features %&gt;% select(-c(clean_text)))\n\n# Add train-unique features in with all zeros\npaste0(\"ngram_\", irrelevant_ngrams)\ntest_features &lt;- test_features %&gt;% \n  mutate(`ngram_fake fake fake` = 0, `ngram_fake fake` = 0,\n         `ngram_whore whore` = 0, `ngram_it fuck` = 0,\n         `ngram_fuck u` = 0, `ngram_a a a` = 0,\n         `ngram_lick` = 0)\n\n# Predictions to csv\ndata.frame(Id = test_features$rowid,\n           Category = predict(pls_mod_final,\n                              ncomp = pls_mod_final$bestTune$ncomp,\n                              newdata = test_features)) %&gt;%  \n  write_csv(\"~/Downloads/pls_mod_predictions.csv\")"
  },
  {
    "objectID": "blog/police_exploratory/index.html",
    "href": "blog/police_exploratory/index.html",
    "title": "תכלית מוקד 100 להציל חיים",
    "section": "",
    "text": "פוסט זה הוא ניטוח אקספלורטיבי-ראשוני. באו איתי להרפתקה ללמוד מה שאפשר ללמוד על מוקד 100 ועמידתו ביעוד המוצהר שלו, להציל חיים!\nלפני שנתחיל לנתח נתונים ישראליים, באו נלמד כמה נתונים בסיסיים מהמערכת האמריקאית. הנתונים האלה ישמשו לנו כהשערות אפריוריות בניתוח הנתונים הישראליים. חשוב שתהיה לנו הבנה אפריורית מבוססת מכיוון שהנתונים הישראלים שנמצאים ברשותנו כרגע הם לא רק דלים מאוד אלא גם לעתים גם לא ברורים מצד משמעות המשתנים שבם. לעומת זאת, יש לנו מערך נתונים נקי וברור עם מידע מארבעה ערים אמריקאיות (ניו אורלינס, דלס, דטרויט, וצ’רלסטון) על סיווג האירועים, ייזום השיחות (מהמשטרה או מהציבור), תוצאת האירוע, והרבה יותר.\n\n# Data manipulation\n  library(tidyverse)\n# Graphics that can deal with Hebrew text\n  library(ragg)\n  locale(\"he\")\n\n## For the sake of reasonable priors, let's look at some patterns in American 911 data\n\n# American data from Charleston, Detroit, New Orleans, and Dallas \n# (sampled for faster computation - the dataset is 6,347,478 rows long):   \namerican &lt;- read_csv(\"/Users/louisteitelbaum/Documents/all_calls.csv\")[sample(1:6347478, 20000), -(1:26)]\n\nכמה שאלות ראשוניות: - כמה אירועים נפתחו בעקבות קריאה למוקד מהציבור (לעומת זהויים עצמאיים של שוטרים)? - כמה שיחות הגיעו למוקד אך לא נפתח אירוע תגובה בעקבותם? בכמה מהם האירוע הסתיים בעקבות השיחה/לא היה צורך בתגובה/הטרדה וכו’? - כמה פניות למוקד התגלו כמקרי חירום? ממקרי החירום, כמה קיבלו מענה משטרטי? - האם זה נפוץ שיש שינוי בין זיהוי הראשון של סוג האירוע ודיווח הסופי? יש סוגי אירוע שבהם הסיכוי של סיווג מוטעה גבוה יותר?\nהגעת לחלק החופר, כאשר עדיין לא התמקמתי בנתונים. אם אתה לא רוצה ללכת איתי בהרפתקאה הזאת, אתה מוזמן לדלג עד הכותרת “סיכום הנתונים האמריקאים”.\nנתחיל בשאלת הייזום של השיחות.\n\n# What percentage of 911 calls are citizen-initiated?\namerican %&gt;% \n  filter(city %in% c(\"Detroit\", \"NewOrleans\")) %&gt;%\n  group_by(self_initiated) %&gt;% \n  summarise(freq = n()/nrow(.))\n      #&gt; No (Citizen-initiated):  0.443\n      #&gt; Yes (Police-initiated):  0.328\n      #&gt; Other:                   0.228\n\n44.3 אחוז של פניות הגיעו מהציבור בלי הצטרפות המשטרה. חשוב להזכיר שהנתונים האלה מגיעים משתי ערים גדולות (רק דטרויט וניו אורלינס מעניקים את המשתנה הזה). הייתי מנכש שהאחוז יהיה גבוה יותר במקומות פחות עירניים, איפה שיש פחות פעילות משטרתית באופן כללי. גם ראוי לציין שאני לא יודע מה המשמעות של “Other” בהקשר הזה. נראה לי שהכוונה למערכות אזעקות וכדומה, שיכולים ליזום אירוע משרתי בלי קשר גם לשוטרים וגם לאזרחים. רק לניו אורלינס יש את הקטגוריה הזאת. לדטרויט יש רק כן או לא ייזום פנימי.\nמה עם תוצאות השיחות?\n\namerican %&gt;%\n  filter(is.na(disposition) == F, city != \"Detroit\") %&gt;%\n  group_by(disposition) %&gt;%\n  summarise(percent = 100*n()/nrow(.)) %&gt;%\n  arrange(desc(percent)) %&gt;%\n  ungroup() %&gt;%\n  mutate(disposition = factor(disposition, levels = disposition)) %&gt;%\n  ggplot(aes(disposition, percent, fill = disposition)) +\n    geom_bar(stat = \"identity\") +\n    guides(x = guide_axis(angle = 45)) +\n    scale_fill_brewer(palette = \"Spectral\") +\n    theme_minimal() +\n    labs(title = \"Outcome of 911 calls in New Orleans, Charleston, and Dallas\", x = \"\", y = \"Percent of Total Cases\") +\n    theme(plot.title = element_text(hjust = .5), legend.position = \"none\")\n\n\nאחלה תרשים! מה זה אומר? שאלה טובה - יש הרבה קטגוריות פה, ולא ברור מה הם בדיוק. נעבור אחד אחד ונתרגם קצת: - פעילות אכיפה (Enforcement Activity) 39.9 % : שליחת שוטרים לאתר בלא מעצר - יצירת דו”ח (Report Generated) 16.8 % : אני חושב שזה אומר שלא שלחו ניידת, אבל דיווחו על פשיעה או על פעילות חשודה - לא מבוסס (Unfounded) 15.7 % - פעילות ללא מעצר (Non-Arrest Activity) 10.8 % : אני לא בטוח מה ההבדל בין זה לבין “Enforcement Activity” - מבוטל/נסוג (Cancellation/Withdrawn) 9.67 % - חוזר (Repeat) 2.1 % : כלומר, משהו כבר התקשר בנוגע לאורוע הזה? אותו ב”א כבר התקשר? - מעצר (Arrest Issued) 1.9 % - נתינת אזהרה (Warning Issued) 1.3 % - נתינת דוח (Citation Issued) 0.9 %\n- תלונת שוא (False Complaint) 0.7 % - אחר (other) 0.2 %\nאוקיי, עדיין לא ברור לגמרי. למרבה המזל, מייצרי מערך הנתונים הסבירו את החשיבה שלהם פה. הם כותבים, “דלס השתשמה ב12 קטגוריות להתייחס לתוצאת השיחה. 80% סווגו כ”מבוטל”… עוד 18% הותאמו ל”מעצר”. אנחנו מאמינים שזה מהווה מוגבלות של הנתונים ולא תכונה ייחודית של הקהילה בדלס, מכיוון שפניות שזוהו כגרמים לתגובה מינורית כולם סווגו כ”מבוטל. בסגנון דומה בצד שני של הספקטרום, 60% של הערכים מניו אורלינס סווגו כ”פעילות דרושה ננקטה,” שהתאמנו ל”פעילות אכיפה”. אבל באמת, קטגוריה זאת יכולה להתאים לכל תגובה חמורה או קלה.”\n…ועוד הרבה יותר. לפי דבריהם, הבעיה היא ש”בתוך עיר מסויימת, הקטגוריות בהן נעשה שימוש לסוג פנייה מסוים לעתים קרובות משתנות משנה לשנה, וגם הגדרות משתנות ככל שעובר הזמן… לא מפתיע, כי הנתונים האלה מגיעים ממערכת שמתוכננת לנווט שוטרים לאירועים ולספק תמיכה כאשר הם חוקרים פנייה. בתוך המערכת הזאת, ערכים אינם מתוכננות להקל על ניתוח הנתונים אלא להבטיח נגישות למידע בשביל השוטרים המגיבים לפנייה”.\nהנה החלוקה לפי ערים:\n\nאז העבודה שלנו מסובכת יותר ממה שציפינו. אבל עדיין יש הרבה ללמוד. צריך לזכור שכרגע אנחנו מחפשים רק רקע ככלי כדי להבין את הנתונים הישראליים. אני אציג פה עוד כמה גרפים, ואז אשתדל לסכם את מה שיכלתי ללמוד מהם.\nאם לוקחים רק את הפניות שבוודאי לא מגיעות מהמשטרה, התמונת התוצאות נהיה פשוטה יותר (אשמה של צ’רלסטון, שמסבכת את כל עבודת הסיווג אבל אינה מדווחת על ייזום השיחות).\n\nרוב הפניות מסתיימות בשליחת שוטרים לאתר ללא מעצר. ביותר מ20% של מקרים, המוקדן מדווח על האירוע אבל אין פעילות משטרתית. בעוד 20%, הפנייה חסרה ביסוס בכלל.\nראינו את תוצאות הפניות למוקד. מה עם סוגי הפניות לכתחילה?\n\nהנה סיאטל, להשוואה:\n\nהקבוצה ההכי גדולה (כ35%) היא דברים מגוונים שלא ידעו איך למיין אותם. אחרי זה, “תלונות/תנאים סביבתיים” תופס יותר מ15%. רכוש (גניבה, ונדליזם) ועבירות תנועה שניהם מתקרבים ל10%. פשע אלים מהווה פחות מ5% אבל עדיין אינה זנוחה. בריאות הנפש מגיעה ל1 אחוז. שריפות ומקרי חירום רפואים הם זנוחים לגמרי. סדר הנתונים בסיאטל הוא דומה, רק בלי ההסתפקות בחלק הגדול ביותר.\nעכשיו נחזור לתוצאות האירועים - אולי אפשר להבין יותר על הגדרות הקטגוריות על בסיס סוגי האירועים שמתגלגלים אליהם. התרשים הבא מייצג רק את ניו אורלינס - דלס, דטרויט, וצ’רלסטון כולם חסרות או ייזום השיחות או תוצאות.\n\n# Maybe we can learn more from the overlap between call type and outcome\namerican %&gt;%\n  mutate(call_type = str_wrap(call_type, width = 18)) %&gt;%\n  filter(is.na(disposition) == F, \n         is.na(call_type) == F,\n         disposition != \"Unknown\", \n         self_initiated %in% c(\"No\", \"other\")) %&gt;%\n  ggplot(aes(x = \"\", fill = disposition)) +\n    facet_wrap(~call_type, nrow = 4) +\n    geom_bar(position = \"fill\") +\n    scale_fill_brewer(name = \"Outcome\", palette = \"Paired\") +\n    coord_polar(\"y\") +\n    theme_void()\n\n\nפניות הקשורות לעבירות רכוש, עבירות תנועה, אלימות במשפחה, ופשע אלים מסתיימים הרבה פעמים בפעילות אכיפה, אבל יותר נפוץ שהם מתגלים כחסרי ביסוס, והרבה יותר נפוץ שהם מסתיימים (מבחינת המשטרה) ביצירת דו”ח. אני מניח שבהקשר הזה “יצירת דו”ח” אומר שהאירוע הסתיים כך שהמצב כבר לא היה דחוף, ולכן דיווחו על האירוע לצורך טיפול עתידי. מקרי חירום רפואי נראים דומים, רק בלי הסיכוי שהם לא מבוססים. כאן אבל, הייתי רוצה להאמין שב”יצירת דו”ח” הכוונה להתרעה בפני הגופים הרלוונטיים - אמבולנס וכו’. כנ”ל לגבי יצירת דו”חות בשריפות, אלא ששוטרים נוטים כן להגיע לשריפות. נעדרים, עבירות מין, חשד, ואזעקות כולם מחולקים בין פעילות אכיפה לחסר ביסוס. קריאות הקשורות לבריאות הנפש נוטים להענות עם “פעילות אכיפה”, אבל לפעמים רק ביצירת דו”ח.\nלוודא שמה שאנחנו רואים ניתן להכללה, נעיין רגע בנתונים המקבילים מסיאטל, שלא היה מוצג לעיל, ושהקטגוריות שלה הן קצת יותר מפורטות:\n\nseattle &lt;- read_csv(\"/Users/louisteitelbaum/Downloads/Seattle.csv.zip\")[sample(1:4206691, 20000),]\nseattle %&gt;%\n  mutate(call_type = str_wrap(call_type, width = 18)) %&gt;%\n  filter(is.na(disposition) == F, \n         is.na(call_type) == F,\n         disposition != \"Unknown\", \n         self_initiated %in% c(\"No\", \"other\")) %&gt;%\n  ggplot(aes(x = \"\", fill = disposition)) +\n  facet_wrap(~call_type, nrow = 4) +\n  geom_bar(position = \"fill\") +\n  coord_polar(\"y\") +\n  theme_void() +\n  labs(title = \"Incident Outcomes by Type in Seattle\") +\n  theme(plot.title = element_text(hjust = .5) , legend.title = element_blank())\n\n\nלעיניי, התמונה פה דומה למה שראינו בניו אורלינס. חידוש אחד שאפשר לזהות: במקום החלק הגדול של “לא מבוסס” שראינו מקודם (21.2% בניו אורלינס), פה יש מבוטל (6.6% סה”כ), תלונת שוא (3.6%), לא מבוסס (1.5%), והכי מפתיע - שגיאת מיקום (10.1%). ע”פ זה נראה שאחוז משמעותי של אירועים - ובמיוחד מקרי חירום רפואי - מסתיימים במצב בו המשטרה לא מצליח למקם את האירוע. אני לא יודע מה לעשות עם זה.\nשאלה אחרונה: רציתי לדעת אם זה נפוץ שזיהוי הראשוני של סוג האירוע מתגלה כשגוי. לענות על זה, נצטרך למצוא את הנתונים הגולמיים מסיאטל, שמדווחת על שני הזיהויים.\n\nseattle_raw &lt;- read_csv(\"/Users/louisteitelbaum/Downloads/Call_Data_Seattle.csv\")[sample(1:4842110, 20000),]\n\nדי מהר אחרי שפתחתי את המערך הזה, ראיתי את הבעיה:\n\nseattle_raw %&gt;%\n  group_by(`Initial Call Type`, `Final Call Type`) %&gt;%\n  summarise() %&gt;% \n  group_by(`Initial Call Type`) %&gt;%\n  summarise(ntypes = n()) %&gt;%\n  ggplot(aes(ntypes)) +\n    geom_histogram(binwidth = 1) + \n    labs(title = \"Number of Unique Final Call Types Per Initial Call Type\", \n         subtitle = \"Seattle\",\n         x = \"Number of Unique Final Call Types\",\n         y = \"Count of Initial Call Types\") +\n    theme_minimal()\n\nseattle_raw %&gt;%\n  group_by(`Initial Call Type`, `Final Call Type`) %&gt;%\n  summarise() %&gt;% \n  group_by(`Final Call Type`) %&gt;%\n  summarise(ntypes = n()) %&gt;%\n  ggplot(aes(ntypes)) +\n  geom_histogram(binwidth = 1) + \n  labs(title = \"Number of Unique Initial Call Types Per Final Call Type\", \n       subtitle = \"Seattle\",\n       x = \"Number of Unique Initial Call Types\",\n       y = \"Count of Final Call Types\") +\n  theme_minimal()\n\n \nבמילים אחרות, פרדיגמות הסיווג בזיהוי ראשוני וזיהוי סופי הם שונים לגמרי. יש הרבה סוגים ראשוניים שמתחלקים לעשר ואפילו עשרים סוגים סופיים, ויש הרבה סוגים סופיים שכוללים אפילו שלושים סוגים ראשוניים. ניתן עוד לעקוב אחרי השאלה הזאת (למשל, עם cosine similarity למדוד את הדמיון הלשוני בין ראשוני לסופי), אבל אין לי את הזמן או את הרצון כרגע.\nאני מתחיל להרגיש יותר בבית בנתונים האלה. אפשר לסכם את מה למדנו.\n\nסיכום הנתונים האמריקאים\n\nאחוז משמעותי של אירועים במוקד מגיעים מזיהויים עצמאיים של שוטרים, אבל לא רוב המוחלט (ניו אורלינס - 20%, סיאטל - 41%, דטרויט - 55%)\nקרוב לחצי מהשיחות שמגיעות למוקד לא מובילות לפעילות משטרתית אקטיבית (סיאטל - 52%, ניו אורלינס - 45%). עם זאת, רוב מוחלט מהמקרים האלה נסגרים או בדיווח משטרתי (לפעמים לאמבולנס או לכבאות) או באי-יכולת לאתר את האירוע. אולי 5% מתגלים כתלונת שוא כלשהי. כ7% בוטלות תוך כדי השיחה.\nקשה לדעת כמה פניות אפשר להחשב כמקרי חירום, כאשר הרבה מהאירועים מסוג הזה כבר הסתיימו בזמן השיחה ולכן לא קיבלו מענה משטרתי. אפשר להגיד, אבל, שאירועים הקשורים לבריאות הנפש מייצגים כ3% של שיחות (רובם ב”א אבדני, חולה נפש מסתובב ברחוב באופן לא מסוכן, או מנת יתר). חירום רפואי מייצג פחוז מ1%.\nקשה לדעת כמה זהויים שגויים יש, אבל כן אפשר להגיד שיש זיהוי ראשוני ע”י המוקדן, וזיהוי סופי אחרי סיום האירוע.\nבכללי, כל הנתונים מהמוקד קיימים בראש ובראשונה לעזור לפעילות המוקד. כתוצאה מזאת, אין שום סטנדרטיזציה, וקשה לקבל שום תמונה רחבה שהיא על העניינים.\n\n\n\nהנתונים הישראליים\nנתונים על מוקד 100 מפה, פה, ופה. על אוכלוסייה מפה.\n\n# Incoming calls by region, city, day of week (Jan 1 - March 21, 2020)\nday.place.1 &lt;- read_csv(\"/Users/louisteitelbaum/Documents/moked_janfebmar.csv\",\n                        locale = locale(date_names = \"he\", encoding = \"UTF-8\"))\n\n# Incoming calls by region, city, day of week (March 21 - April 30, 2020)\nday.place.2 &lt;- read_csv(\"/Users/louisteitelbaum/Documents/moked_marapr.csv\",\n                        locale = locale(date_names = \"he\", encoding = \"UTF-8\"))\n\n# General Population Demographics\ndemographics &lt;- read_csv(\"/Users/louisteitelbaum/Documents/demographics/demographics.csv\")\ndemographics &lt;- demographics[-(1:11), c(2, 8, 11)]\nnames(demographics) &lt;- c(\"ir\", \"pop\", \"arabs\")\ndemographics$pop[demographics$pop == \"-\"] &lt;- 0\ndemographics$arabs[demographics$arabs == \"-\"] &lt;- 0\ndemographics$pop &lt;- as.numeric(gsub(\",\", \"\", demographics$pop))\ndemographics$arabs &lt;- as.numeric(gsub(\",\", \"\", demographics$arabs))\n\n# Incidents (unclear what this means exactly) by city, type, subtype, year, and quarter (2015-2017)\ntype.place.year &lt;- read_csv(\"/Users/louisteitelbaum/Documents/moked100.csv\")\ntype.place.year &lt;- type.place.year[-(1:6), c(1:3, 5:6, 11, 16)]\nnames(type.place.year) &lt;- c(\"ir\", \"type\", \"subtype\", \"total\", \"2015\", \"2016\", \"2017Q1-2\")\ntype.place.year &lt;- type.place.year %&gt;%\n  mutate(across(4:7, ~replace(., . == \"$\", \"1\"))) %&gt;%\n  mutate(across(4:7, ~replace(., . == \"-\", \"0\"))) %&gt;%\n  type_convert()\n\nאם רוצים לדעת על סכנת מוות, ובמיוחד במישור ההלכתי, יש לנו הזדמנות בישראל שלא היתה לנו בארה”ב: התנהגות הדתיים. אם בדרך כלל יש אחוז משעותי של פניות למוקד שהם אינם מהווים סכנת מוות (ולכן אסורים בשבת על פי הלכה כפי שהיא נהוגה), נוכל לצפות שמספר הפנות ירד באופן משמעותי בשבת, ובמיוחד במקומות אם אוכלוסייה דתית יותר.\nלצערנו, השנתון הסטטיסטי של משטרת ישראל לא מדווח על חלוקה של פניות לימות השבוע. הוא כן מחלק ככה את אירועי התגובה (“מענה לאירוע עקב קריאה שנתקבלה מהציבור או זיהוי עצמי של שוטר אשר פתח אירוע במוקד”). הנה הנתונים משנת 2020:\n\nקשה לי לראות מה קורה בטבלה כזאת. הנה אותו דבר בתרשים:\n\nweekdays &lt;- tibble(type = c(\"תנועה\", \"איכות חיים\", \"אלימות\", \"חירום וסכנות לציבור\", \"בטחון\", \"פעילות משטרתית\", \"רכוש\", \"סדר ציבורי\", \"סמים ואלכוהול\", \"מין\", \"עבירות ברשת\",   \"היסטורי\", \"פח'ע\"),\n                   א = c(106065, 38568, 43801, 57382, 20330, 83383, 44352, 5345, 6002, 1577, 127, 18, 6276),\n                   ב = c(106998, 34000, 43729, 56949, 20217, 84631, 42905, 5681, 5903, 1478, 79, 13, 5245),\n                   ג = c(111376, 36151, 44429, 57119, 20393, 83880, 44268, 5730, 5821, 1449, 113, 17, 5096),\n                   ד = c(113958, 36685, 44582, 59439, 19669, 86445, 44976, 5272, 5994, 1467, 101, 22, 2031),\n                   ה = c(122195, 49062, 46911, 60205, 19652, 85625, 46262, 6653, 5740, 1506, 110, 23, 5132),\n                   ו = c(100986, 58084, 44784, 56144, 14522, 66167, 40251, 5955, 5099, 1265, 77, 12, 3158),\n                   שבת = c(61737, 52357, 38406, 47621, 12583, 58115, 28104, 7552, 4471, 1075, 59, 10, 2439))\nweekdays &lt;- weekdays %&gt;%\n  pivot_longer(2:8, names_to = \"day\", values_to = \"n\") %&gt;%\n  mutate(day = factor(day, levels = c(\"שבת\", \"ו\", \"ה\", \"ד\", \"ג\", \"ב\", \"א\")))\noptions(scipen=10000)\n\nggplot(weekdays, aes(day, n, fill = type)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  theme(legend.title = element_blank()) +\n  labs(x = \"יום\",\n       y = \"מספר אירועים\")\n\n\nלא צריך כלים סטטיסטים לראות שמספר האירועים יורד בשבת. עדיין קצת קשה לראות את הפרופורציות. הנה השוואה בין פרופוציות הממוצעות של ימי השבוע לבין הפרופורציות של שבת:\n\nweekdays %&gt;%\n  mutate(shabbat = recode(if_else(day == \"שבת\", \"Y\", \"N\"), \"Y\" = \"שבת\", \"N\" = \"ימות השבוע\")) %&gt;%\n  group_by(shabbat, type) %&gt;%\n  summarise(n = sum(n)) %&gt;%\n  group_by(shabbat) %&gt;%\n  summarise(type = type, prop = n/sum(n)) %&gt;%\n  ggplot(aes(x = \"\", prop, fill = type)) +\n    geom_bar(stat = \"identity\", color = \"white\") +\n    facet_wrap(~shabbat) +\n    theme_minimal() +\n    theme(axis.text.y = element_blank(), axis.title = element_blank(), legend.title = element_blank())\n\n\nבשבת יש פרופורציה קטנה יחסית של אירועי תנועה. זה הגיוני. יש גם פרופורציה גדולה יחסית של אירועי איכות חיים. אפשר לנחש שזה מייצג את הבלגן והרעש של סופ”ש, מצורף עם כמות נמוכה של כל הסוגים האחרים. אף אחד מאלה לא מוכיחים שמספר האירועים הכללי בשבת הוא תוצאה של התנהגות הדתיים במדינה. סביר להניח שגם חילינום נוסעים פחות בשבת - אין עבודה - ולכן יש פחות אירועים במשטרה. גם יכול להיות שהמשטרה היא קצת יותר עצלנית בסופ”שים ולכן מטפלת בקצת פחות אירועים בכל הקטגוריות (תזכר שאנחנו מסתכלים רק על אירועי תגובה כרגע).\nלמרבה המזל, יש לנו נתונים על פניות למוקד מחולקים גם לימי השבוע וגם לערים. האם אפשר לראות שלערים עם אוכלוסייה דתית יותר יש הפרש גדול יותר בין הפניות בשבת לבין ימי החול? נסתכל על כמה ערים.\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# WRANGLE DATA (Incoming calls by region, city, day of week)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Variable names into English\nnames(day.place.1) &lt;- c(\"mahoz\", \"hodesh\", \"ir\", \"yom\", \"n\") -&gt; names(day.place.2)\n\n# Only cities for which we have info on all weekdays for each month\nday.place.1 &lt;- day.place.1 %&gt;%\n  group_by(mahoz, hodesh, ir) %&gt;%\n  filter(n() == 7) %&gt;%\n  ungroup()\n\nday.place.2 &lt;- day.place.2 %&gt;%\n  group_by(mahoz, hodesh, ir) %&gt;%\n  filter(n() == 7) %&gt;%\n  ungroup()\n\n# Combine day.place.1 and day.place.2\nday.place &lt;- bind_rows(day.place.1, day.place.2)\nrm(list = c(\"day.place.1\", \"day.place.2\"))\n\n# Remove summing rows\nday.place &lt;- day.place[day.place$mahoz != 'סה\"כ' & day.place$mahoz != \"Total\", ]\nday.place &lt;- day.place[day.place$hodesh != \"Total\" & day.place$ir != \"Total\" & day.place$yom != \"Total\", ]\n\n# Rename + Factorize days of the week\nday.place &lt;- day.place %&gt;%\n  mutate(yom = factor(yom, \n                       levels = c(unique(day.place$yom)[1], \n                                  unique(day.place$yom)[6], \n                                  unique(day.place$yom)[7], \n                                  unique(day.place$yom)[2],\n                                  unique(day.place$yom)[5], \n                                  unique(day.place$yom)[4],\n                                  unique(day.place$yom)[3]), \n                       labels = c(\"שבת\", \"ו\", \"ה\", \"ד\", \"ג\", \"ב\", \"א\")))\n\n# One case per weekday per city\nday.place &lt;- day.place %&gt;%\n  group_by(mahoz, ir, yom) %&gt;%\n  summarise(n = sum(n)) %&gt;%\n  ungroup()\n\nday.place &lt;- day.place[day.place$ir != \"לא ידוע\", ]\n\n# Add variables to see proportion of calls coming in on each day\nday.place &lt;- day.place %&gt;%\n  left_join(demographics) %&gt;%\n  group_by(ir) %&gt;%\n  mutate(daypercent = 100*n/sum(n),\n         callspercap = n/pop) %&gt;%\n  ungroup()\n  \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# VISUALIZE \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nday.place[day.place$ir %in% unique(day.place$ir)[sample(1:56, 25)], ] %&gt;%\n  ggplot(aes(yom, daypercent, fill = (yom == \"שבת\") )) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ir) +\n  theme(legend.position = \"none\", \n        plot.title = element_text(hjust = 0.5)) +\n  labs(x = \"יום\",\n       y = \"אחוז פניות\",\n       title = \"2020 מינואר לאפריל 100 פניות למוקד \")\n\n\nאני לא רואה שום נטייה פה. אולי אני פשות לא מכיר כמה דתיים יש בכל עיר. אם נדע את גודל האוכלוסייה הדתית בכל עיר (בייחס לאוכלוססיה הככלית כמובן) נוכל לבנות מודל סטטיסטי ולראות אם אוכלוסייה דתית יותר גורמת להנמחת הפניות למוקד בשבת. בליתי הרבה זמן בחיפוש הנתונים האלה. מפקד האוכלוסין סופר כמה יהודים יש בכל עיר, אבל לא כמה דתיים. מרכז המחקר פיו מונה את הדתיים למיניהם אבל מחלק רק למחוזות, לא ערים. נצטרך למצוא proxy - משתנה שמתואם מספיק ליעד. באתר ‘כיפה’ יש רשימות של כל בתי הכנסת וכל המקוואות בארץ, לפי ערים. אלך על זה.\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# WEB SCRAPING\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Number of synagogues by city in Israel\nshuls_link &lt;- url(\"https://www.kipa.co.il/%D7%91%D7%AA%D7%99-%D7%9B%D7%A0%D7%A1%D7%AA/%D7%91%D7%AA%D7%99-%D7%9B%D7%A0%D7%A1%D7%AA-%D7%9C%D7%A4%D7%99-%D7%A2%D7%99%D7%A8/\", \"rb\")\nshuls_page &lt;- read_html(shuls_link)\nshul_cities &lt;- shuls_page %&gt;% html_elements(\".mikve-list a\") %&gt;% html_text()\nshul_city_links &lt;- shuls_page %&gt;% html_elements(\".mikve-list a\") %&gt;% html_attr(\"href\")\n\nget_shuls &lt;- function(city_link){\n  city_page &lt;- read_html(city_link)\n  nshuls &lt;- city_page %&gt;% html_elements(\"thead+ tbody .clickable-row td:nth-child(1)\") %&gt;% length()\n  npages &lt;- city_page %&gt;% html_elements(\".pager a\") %&gt;% html_text()\n  npages &lt;- max(na.omit(as.numeric(head(npages, -1))))\n  if(npages &gt; 0){\n    last_page &lt;- read_html(paste(city_link, \"page/\", npages, \"/\", sep = \"\"))\n    nshuls_last &lt;- last_page %&gt;% html_elements(\"thead+ tbody .clickable-row td:nth-child(1)\") %&gt;% length()\n    nshuls &lt;- nshuls + 20*(npages-2) + nshuls_last\n  }\n  nshuls\n}\n    # I tried this with sapply and got an error 410 somewhere along the line. This loop is my way of sidestepping that error.\nshuls &lt;- tibble(ir = shul_cities,\n                shuls = rep(NA, length(shul_city_links)))\nfor (city in 1:length(shul_cities)) {\n  try(\n    shuls$shuls[city] &lt;- get_shuls(shul_city_links[city])\n  )\n}\n\n# Number of mikvaot by city in Israel\nmikvaot_link &lt;- \"https://www.kipa.co.il/%D7%9E%D7%A7%D7%95%D7%95%D7%90%D7%95%D7%AA/%D7%9E%D7%A7%D7%95%D7%95%D7%90%D7%95%D7%AA-%D7%9C%D7%A4%D7%99-%D7%A2%D7%99%D7%A8/\"\nmikvaot_page &lt;- read_html(mikvaot_link)\nmikva_cities &lt;- mikvaot_page %&gt;% html_elements(\".synagog a\") %&gt;% html_text()\nmikva_city_links &lt;- mikvaot_page %&gt;% html_elements(\".synagog a\") %&gt;% html_attr(\"href\")\n\nget_mikvaot &lt;- function(city_link){\n  city_page &lt;- read_html(city_link)\n  nmikvaot &lt;- city_page %&gt;% html_elements(\"td:nth-child(1)\") %&gt;% length()\n  npages &lt;- city_page %&gt;% html_elements(\".pager a\") %&gt;% html_text()\n  npages &lt;- max(na.omit(as.numeric(head(npages, -1))))\n  if(npages &gt; 0){\n    last_page &lt;- read_html(paste(city_link, \"page/\", npages, \"/\", sep = \"\"))\n    nmikvaot_last &lt;- last_page %&gt;% html_elements(\"thead+ tbody td:nth-child(1)\") %&gt;% length()\n    nmikvaot &lt;- nmikvaot + 20*(npages-2) + nmikvaot_last\n  }\n  nmikvaot\n}\nmikvaot &lt;- tibble(ir = mikva_cities,\n                  mikvaot = as.vector(sapply(mikva_city_links, FUN = get_mikvaot)))\n\nלפני שאסמוך על המקוואות ובתי הכנסת, צריך לוודא שהנתונים נראים איך שהיינו מצפים. הם מתואמים אחד אל השני לפחות?\n\nshuls %&gt;%\n  left_join(mikvaot) %&gt;%\n  left_join(demographics) %&gt;%\n  mutate(\n    shulspercap = shuls/pop,\n    mikvaotpercap = mikvaot/pop\n  ) %&gt;% \n  summarise(cor = cor(shulspercap, mikvaotpercap, use = \"pairwise.complete.obs\"))\n\nכן. מספר בתי הכנסת בעיר מתואם עם מספר המקוואות (שניהם מחולקים לאוכלוסייה הכללית) r=.64. זה הגיוני - יותר בתי כנסת לאדם, יותר מקוואות לאדם. אפשר גם לראות את מספר בתי הכנסת מול אוכלוסייה הכללית של העיר:\n\nshulspopmod &lt;- lm(shuls~pop, data = (shuls%&gt;%left_join(demographics)%&gt;%filter(ir != \"ירושלים\")))\n\nshuls %&gt;%\n  left_join(demographics) %&gt;%\n  filter(ir != \"ירושלים\", is.na(pop) == F, is.na(shuls) == F) %&gt;%\n  mutate(shulspopresid = rstandard(shulspopmod)) %&gt;%\n  ggplot(aes(pop, shuls)) +\n    geom_point(aes(color = (shulspopresid &gt; 0))) +\n    geom_smooth(method = \"lm\", color = \"black\", alpha = 0, size = 1) +\n    theme_minimal() + \n    theme(legend.position = \"none\") +\n    scale_x_continuous(breaks = c(100000, 200000), labels = c(\"100,000\", \"200,000\")) + \n    labs(x = \"אוכלוסייה\",\n         y = \"מספר בתי כנסת בעיר\")\n\n\nאפשר להניח שערים שיש להם יותר בתי כנסת ביחס לאוכלוסייה שלהם (צבועים בתכלת) הם אלה שיש להם קהילה דתית מובהקת יותר. אפשר לראות כמה ערים בחלק העליון של הגרף - הם כדוגמת צפת, שיש לה הרבה יותר בתי כנסת מאשר מגיע לה. הערים האדומות בחלק התחתון הם היותר חילוניים או ערביים.\nעכשיו אפשר לשאול את השאלה: האם אוכלוסייה דתית יותר גורמת להנמחת הפניות למוקד בשבת?\n\n# Does more religious Jewish population predict a lower proportion of 100 calls on shabbat?\nrequire(MASS)\n\nmikvaotpopmod &lt;- lm(mikvaot~pop, data = (mikvaot%&gt;%left_join(demographics)%&gt;%filter(ir != \"ירושלים\")))\nmikvaot &lt;- mikvaot %&gt;% left_join(demographics) %&gt;% filter(ir != \"ירושלים\", is.na(pop)==F, is.na(mikvaot)==F) %&gt;% mutate(mikvaotpopresid = rstandard(mikvaotpopmod))\n\nshab.place &lt;- shuls %&gt;% \n  left_join(demographics) %&gt;%\n  filter(ir != \"ירושלים\", is.na(pop) == F, is.na(shuls) == F) %&gt;%\n  mutate(\n    shulspopresid = rstandard(shulspopmod)\n  ) %&gt;%\n  left_join(mikvaot) %&gt;%\n  right_join(day.place) %&gt;%\n  filter(pop &gt; 1000) %&gt;%\n  filter(yom == \"שבת\")\n\n\nmikvamod &lt;- lm(daypercent~mikvaotpopresid, data = shab.place)\nvisualize(mikvamod)\nsummary(mikvamod)\nrobustmikvamod &lt;- rlm(daypercent~mikvaotpopresid, data = shab.place)\nsummary(robustmikvamod)\ncompare.fits(daypercent~mikvaotpopresid, data = shab.place, mikvamod, robustmikvamod)\n\nshulmod &lt;- lm(daypercent~shulspopresid, data = shab.place)\nvisualize(shulmod)\nsummary(shulmod)\nrobustshulmod &lt;- rlm(daypercent~shulspopresid, data = shab.place)\nsummary(robustshulmod)\ncompare.fits(daypercent~shulspopresid, data = shab.place, shulmod, robustshulmod)\n\nבלי להכנס לכל הטכניקה… התשובה היא לא. לא רואים שום קשר לינארי בין אוכלוסייה דתית לפניות למוקד בשבת. אני עוד לא יודע מה לעשות עם זה. אכן, היו נתונים לכל ימי השבוע רק ב42 ערים, וגם אז בהרבה מהם לא היו נתונים לכל אורך הזמן בין ינואר לאפריל 2020. אולי זאת הבעיה? מה שגם מוסיף קושי זה שהנתונים אינם מחולקים לסוגי האירוע - רק לימים. לו היה לי מערך נתונים מחולק גם לימים וגם לערים, אולי הייתי מוצא משהו.\nמתוך ייאוש מוחלט, אני אנסה עם דיסקרטיזציה לשני חלקים - התכלת והאדום שאינו לעיל על גרף האוכלוסייה ומספר בתי הכנסת."
  },
  {
    "objectID": "blog/hire_your_clone/index.html",
    "href": "blog/hire_your_clone/index.html",
    "title": "Do Employees Tend to Have the Same First Name as Their Bosses?",
    "section": "",
    "text": "I was talking to my dad on the phone a few weeks ago, and he brought up a question: Do employers have a bias against hiring employees who have the same first name as they do? Surely, bosses must try to protect their unique name in the same way that fashionable dressers might be annoyed when another party-goer shows up with an identical outfit to their own. And that’s not to mention the opportunity for confusion: How will members of the team distinguish them in conversation? For these reasons, my dad figured there must be some bias against same-name hiring.\nBut how can we check this empirically?\nThe first step was to find some data. The best I could find came from The Official Board, which maintains detailed corporate organizational charts for tens of thousands of companies worldwide. I downloaded data on 34,615 employee/boss pairs across 2,292 US companies. This includes 101 instances of bosses and employees with the exact same first name. Here’s an example of the data after some cleaning:\n\n\n# A tibble: 6 × 5\n  first_name last_name      boss_first_name boss_last_name company\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;  \n1 Zach       Kirkhorn       \"Elon \"         Musk           Tesla  \n2 Franz      von Holzhausen \"Elon \"         Musk           Tesla  \n3 Tom        Zhu            \"Elon \"         Musk           Tesla  \n4 Nagesh     Saldi          \"Elon \"         Musk           Tesla  \n5 James      Bickford       \"Elon \"         Musk           Tesla  \n6 Bert       Somsin         \"Elon \"         Musk           Tesla  \n\n\nNow we arrive at the first two problems:\n\nThe data only reflect people who did get hired, not people who did not. This means that we are blind to the true probabilities behind the hiring process.\nEmployees were not always hired by their current bosses. Sometimes bosses are hired after their employees, and sometimes hiring is managed by other people in the organizational structure. So the boss-employee name match-up we’re looking at might not always be the relevant one.\n\nProblem 1 is not solvable with the data I have here, but that’s OK. It just means that we can’t answer any questions about, say, whether having a particular name makes you more or less likely to get hired in general. We can only compare the dataset to itself: Do same-name matchups occur more or less often than they would if the same set of bosses and employees were matched up randomly?\nProblem 2 is also not solvable. It means we have to be a bit careful, and remember that the analysis is about the probability of certain boss-employee matches existing. This is probably mostly a result of certain kinds of bosses hiring certain kinds of employees, but not necessarily.\nThis brings us to the final problem - one that I can, at least partially, solve: Gender and ethnicity will confound the results: Male bosses might be more likely to have male employees. Since names tend to be gendered, this will make them that much more likely to have the same same, even if there is no bias at all for the names themselves. The same goes for ethnicity: White bosses might be more likely to have white employees, making them more likely to both have common white names. We need to account for these other biases before we can know anything about the one we’re interested in.\nThis is a tricky problem to solve, since the data do not include the gender and ethnicity of each employee and boss. Thankfully, it’s usually not too hard to guess someone’s gender and ethnicity from on their name. After some shopping, I found this tool for predicting ethnic background, which is available through the ethnicolr python module. This is a decision tree classifier trained on about 130,000 names from Wikipedia. It’s categorization strategy is a bit strange - for example, African American names are considered “British”. This is presumably because many African Americans have names of British origin (as do many white Americans) and are therefore hard to tell apart from those of white people of British ancestry. For my purposes, I further lumped some of the categories together, to get eight: African, Chinese or Korean, Eastern European, Hispanic, Indian, Jewish, Muslim, and Western European or African American. The model outputs probabilities of each ethnic category, I can easily combine categories using the sum rule from probability theory. When I incorporate these probabilities into my model, I can use the product rule to generate, for example, the probability that both the employee and the boss are African.\nFor gender, I used the predictrace R package, which uses a much more straightforward approach (first names only) based on data from the US census. All of this is of course not perfect, but it should be good enough to account for most of the effects of gender and ethnicity.\n\n\n\nProportions of Men and Women in the Raw Data\n\n\nWhile I was at it, I decided to throw in one more possible confounder: name popularity. I know some people with very uncommon names, and they often profess solidarity with other bearers of rare names, having to constantly correct others on pronunciation and spelling. I calculated name popularity based on data from the US Social Security Administration and cutoff at 1 in 1000 to bin names into either “common” or “uncommon”.\n\n\n\nProportions of Common and Uncommon Names in the Raw Data\n\n\nBefore building the model, there is one more problem to tackle: differences between bosses, and differences between companies. This is important, since companies might have different cultures or priorities that cause them to vary on how they hire vis-a-vis gender and ethnicity. Since bosses often have multiple employees, and companies generally have multiple bosses, this is a job for multilevel modeling.\nNow it’s time to build the model. This takes some creativity, for two reasons: First, the dataset is enormous. Second, as noted above, I need to analyze the difference between the true data and a randomly shuffled version, but I can’t include ALL possible random combinations of bosses and employees, since that would make the dataset really really enormous.\nHere’s my stategy:\n\nChoose 250 companies from the real dataset at random. Take the data from those companies and label it real = 1.\nSubset the chosen companies, shuffle the boss-employee pairings randomly (company labels stick with bosses, not necessarily employees), and sample as many rows as are in the real data. Label these rows real = 0.\nRun a Bayesian multilevel logistic regression (predicting log odds that a pairing is real) with 8000 iterations of Markov Chain Monte Carlo sampling, and skeptical priors.\nRepeat steps 1-3 ten times. This iterative resampling process will ward off overfitting (an important concern with so many parameters) and make training more efficient as a bonus.\nCombine all samples of the posterior distributions for each fixed effect (I don’t care about the individual differences between bosses or companies). The ability to combine resampled models like this is a huge advantage of Bayesian methods.\n\nFull code for all of this can be found here.\nThe model itself looks like this, as formulated in brms:\n\nmod_america_bayes1 &lt;- brm(real ~ 0 + same_first_name + both_uncommon_names + both_male + boss_male + boss_female + both_female + both_EastAsian + both_EastEuropean + both_Japanese + both_Indian + both_African + both_Muslim + both_WestEuropean + WE_Asian + WE_Indian + WE_Muslim + WE_Hispanic + WE_Jewish + boss_female:both_WestEuropean + both_Jewish + both_Hispanic + both_Italian + boss_female:WE_Asian + boss_female:WE_Indian + boss_female:WE_Muslim + boss_female:WE_Hispanic + boss_female:WE_Jewish + both_female:both_WestEuropean + both_female:WE_Asian + both_female:WE_Indian + both_female:WE_Muslim + both_female:WE_Hispanic + both_female:WE_Jewish + both_male:both_WestEuropean + both_male:WE_Asian + both_male:WE_Indian + both_male:WE_Muslim + both_male:WE_Hispanic + both_male:WE_Jewish + boss_male:both_WestEuropean + boss_male:WE_Asian + boss_male:WE_Indian + boss_male:WE_Muslim + boss_male:WE_Hispanic + boss_male:WE_Jewish + (0 + same_first_name + both_uncommon_names + both_male + boss_male + boss_female + both_female + both_EastAsian + both_EastEuropean + both_Japanese + both_Indian + both_African + both_Muslim + both_WestEuropean + both_Jewish + both_Hispanic + both_Italian + WE_Asian + WE_Indian + WE_Muslim + WE_Hispanic + WE_Jewish | company_ID),\n                           data = d_america_train,\n                           family = bernoulli(),\n                           prior = c(\n                             prior(normal(0, 1), class = \"b\"),\n                             prior(student_t(4, 0, 1), class = \"sd\")\n                           ),\n                           iter = 6000,\n                           warmup = 2000,\n                           cores = 4)\n\nboth_EastAsian is the probability that both boss and employee are East Asian, based on their full names. WE_Asian is the probability that the boss is Western European or African American and the employee is East Asian or Japanese.\nIn total there are 45 fixed effects: 1. same first name (binary) 2. both uncommon names (binary) 3-6. gender of boss and employee (four categories, one-hot coded) 7-14. both the same ethnicity (7 ethnic categories, continuous probabilities) 15-19. boss is Western European or African American and the employee is a minority (5 minority groups, continuous probabilities) 20-45. interactions between gender and pairs involving Western European or African American bosses\nNotice that there are no intercepts, fixed or random. This is because the dataset is balanced - there is a 50% baseline chance that any pairing will be real, since half of the pairings are real. Similarly, there is a known 50% chance that any given boss or company will be in a real or fake pairing, because of the random sampling process. In log odds, 50% is 0, so the model should be run without intercepts at all.\n\nResults\nAfter waiting for all this to run overnight, we have results!\nBelow are the posterior parameter estimates for the first six fixed effect parameters. The dark blue lines represent 95% credible intervals.\n\nThe model is very unsure about the effect of sharing the same name. This makes sense, since it only happens very rarely. Nevertheless, the model thinks that it’s more likely than not that the effect is positive! In other words, these data constitute evidence that bosses are more likely than chance to have employees that share their first name exactly, even when accounting for name popularity, gender, and ethnicity. In particular, the model’s median estimate for the same_first_name parameter is 0.173. Translating from log odds, this means that, according to the model, having the same name as your boss increases your odds of working together by about 19%.\nIt is still possible that this is due to confounding effects of gender or ethnicity, either because the AI-based guessing was not good enough, the categories were not specific enough, or because I did not account for interactions between gender and ethnicity.\nBut what about the other parameters?\nThe effect of both having an uncommon name is eminently positive, amounting to an odds increase of more that 500%. The parameters for gender and ethnicity are much more difficult to interpret, since 1. they are essentially categorical but not coded in relation to a particular reference group, and 2. they are involved in various interactions. The best way to understand a model like this is to consider particular counterfactuals. For this purpose, I designed an interactive web app. You can play with it right here:"
  },
  {
    "objectID": "blog/time_use/index.html",
    "href": "blog/time_use/index.html",
    "title": "Americans Have Eight Kinds of Days",
    "section": "",
    "text": "The American Time Use Survey takes a wide sample of Americans and interviews them in great detail about what they did the day before. They have done this each year going back to 2003. With a little discretion, I can group the activities into broader categories and produce the following graph:\n\n\n\n\n\nThere is a lot to unpack in this graph. I may come back to some of the trends here in future posts, but right now I want to take an entirely different approach to the data: cluster analysis.\n\n\n\n\n\n\nTip\n\n\n\nCode for this article is available here.\n\n\nLooking at average daily hours spent is enlightening, but it obscures the fundamental experience of time-use: What kinds of days do Americans have? A 1 hour-a-day average for socializing could mean that every American spends one hour each day socializing, or it could mean that 5% of Americans spend 20 hours a day socializing and the rest don’t socialize at all. It could also mean that half of Americans spend 7 hours a day socializing on weekends, and the rest don’t socialize at all. My point: Averages don’t tell us much about the ways in which individual Americans spend their days.\nCluster analysis can solve this problem. Clustering is a family of machine learning techniques that ask the question: What types of cases are there? It can help us say things like “There are two kinds of people in the world…” or “There are three types of days that Americans have”.\nI will be using the k-means clustering algorithm - probably the most popular clustering method. Given the number of clusters k, it finds a “center” for each cluster - the average of all cases in the cluster - so that each case is assigned to its nearest center. This makes it very easy to visualize the clusters, since each cluster’s center is the paradigmatic example for all of its members. But k-means does have one tricky aspect: The algorithm only figures out what the clusters are like, not how many of them there should be. The right number of clusters needs to be figured out in advance.\nA popular, sophisticated method for finding the optimal number of clusters (the optimal k) is the gap statistic method, introduced by Tibshirani et al. (Standford University, 2001). This metric compares the total within-cluster dispersion for each k (how spread out the cluster members are) with the dispersion that could be expected if the data were totally random and uncluttered. The “gap” is the difference between these two numbers. The chosen number of clusters is the lowest one for which one higher would result in a significantly smaller gap.\nRunning this algorithm on a subset of the time use data results in the following graph:\n\n\n\n\n\nThe maximum gap statistic is identified here at k = 20, the maximum k tested. Nevertheless, it is clear from the full graph that 3 is the lowest reasonable number of clusters. A pattern similar to this, in which the gap statistic continues to rise after an initial local maximum, is discussed by Tibshirani et al. in the original paper:\n\nIn cases where there are smaller subclusters within larger well-separated clusters, it can exhibit non-monotone behaviour. Hence it is important to examine the entire gap curve rather than simply to find the position of its maximum.\n\nIn our case, this means that there are a lot of fine distinctions in the ways people spend their days, but k = 3 may be a good start for a very general breakdown. For now then, there are three types of days that Americans have!\n\n\n\n\n\nI’ve named the three clusters “Leisure With Responsibilities”, “Leisure Without Responsibilities”, and “Work Day.” A day of leisure with responsibilities has basically no work and a fair amount of relaxing, watching TV, or surfing the internet, but it also has time spent caring for children, preparing food, and doing housework. It also has a notable amount of socializing. A day of leisure without responsibilities is similar, but with none of the household responsibilities and none of the socializing. Almost all time not spent sleeping or eating is spent relaxing and in front of the TV or computer. A work day has about 8 hours of work, but otherwise looks more similar to the day of leisure with responsibilities, minus the housework and home maintenance.\nHow are the clusters distributed over the week?\n\n\n\n\n\nSure enough, workdays in America mostly happen from Monday through Friday. On weekends, Americans tend to have days of leisure with responsibilities. The proportion of leisure without responsibilities also goes up a bit on weekends, but not dramatically. I get the sense that the kind of people who have days of leisure without responsibilities are often not the kind of people who have day jobs. Can we see this with a breakdown by age group?\n\n\n\n\n\nSure enough, the most avid practitioners of leisure without responsibilities are senior citizens. Children under 18 also have a few more of those than the working-age adults, but not much. This suggests to me that the algorithm has identified school days as leisure with responsibilities. To investigate this possibility further, it may be worthwhile to try out a breakdown into more clusters. Before I do that, though, I’d like to recreate the first graph in this post, but now with types of days instead of averages for individual activities.\n\n\n\n\n\nIf you squint, the 2008 financial crisis is recognizable as a drop in workdays and rise in days of leisure without responsibilities. COVID is even more dramatic. The 2020 fall in days of leisure with responsibilities among children under 18 confirms my suspicion that school days count.\nInterestingly, there looks to be a steady rise in days of leisure without responsibilities, especially among the elderly. This is accompanied by a steady decline in days of leisure with responsibilities across all age groups.\nTo see some finer detail, I’d like to re-cluster with a higher k and see what it comes up with. I’m not sure I could make sense of 18 different categories, so I’ll chose k = 8.\n\n\n\n\n\nHere is the school day in its full glory! Now we also have two types of leisure days without responsibilities, a parenting day, a non-parental household responsibilities day, a day of socializing, and a day with extra sleep.\n\n\n\n\n\nWe can now see that school days and work days both tend to happen on Monday through Friday. Days of extra sleeping happen most often on Sundays, likely reflecting a sleep deficit from the work week. Social days are most common on Saturdays.\n\n\n\n\n\nAgain we see the prominent 2020 drop in both school days (for children) and work days (for working-age people), corresponding with a rise in days of leisure of all sorts. It looks like the younger people in 2020 spiked more dramatically toward days of TV/Radio/Computer use, whereas older people found other ways to kick back. The slow trends are more interesting though. Days of household responsibilities are clearly on a long and slow decline, as are days of socializing. Days of sleeping in look to be on the rise. Again the slow rise in days of leisure without responsibilities is most evident among the elderly, but not all days of leisure without responsibilities - only ones spend entirely on the TV/Radio/Computer. The other sort of leisure days are on the decline.\nI’m curious to see the same graph, but broken up by income rather than age.\n\n\n\n\n\nThis looks like all the trends observed above are true of all income brackets. Looking at these together with the individual-activity graph from the beginning of this post, the most dramatic long-term trends are:\n\nA fall in days of household responsibilities, corresponding to the falls in average time spent on home maintenance, shopping, and housework (though food prep is on the rise).\nA fall in days of socializing, corresponding to a fall in time spent socializing generally. We should be careful with this one though, since “socializing” here only includes time explicitly devoted to socializing. In practice of course, a fair bit of socializing happens at work, school, or during other activities. In particular, the internet has become an increasingly social place during the time period under investigation here.\nRise in days of TV/computer use, especially for the elderly. This corresponds to a rise in average time spent watching TV, listening to the radio, or on the computer.\nAlso, it looks like people with higher income sleep in less. Is this because their jobs are less exhausting? Or because their jobs are more regular, so they have no time to sleep in?\n\nFinally, I wonder how these clusters are distributed between sexes.\n\n\n\n\n\nMales have more workdays, and more days spent entirely on the TV/Radio/Computer. Females have more parenting days and more days of household responsibilities. Females look to have slighty more social days than males do, but not by much. In 2020, males increased their days doing household responsibilities more than did females, presumably since household responsibilities replaced their workdays. This is all basically what I expected. Interestingly, it does look like females are increasing their days sleeping in faster than are males. I don’t have a good explanation for why that might be."
  },
  {
    "objectID": "blog/chess_brain/index.html",
    "href": "blog/chess_brain/index.html",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "",
    "text": "The tragedy of an undergraduate psychology degree: I sit in lecture after lecture, learning about human mind/brain/behavior. It’s all fascinating of course, but I didn’t come here to learn about human mind/brain/behavior. I came here to learn about my mind/brain/behavior. Or at least those of people I interact with. Unfortunately, interesting psychology research tends to be conducted in laboratories with trained researchers. Even after the research stage, good psychometric evaluations are preformed by good clinicians in controlled environments. I don’t have access to those things at this stage of my life.\nLike, for example, I’ve read lots of interesting papers about sleep and circadian rhythms, but have no more than anecdotal evidence about my own cognitive performance throughout the day, or the amount of sleep I really need per night.\nIt felt silly to do so much learning about quantified psychology without even trying to quantify my own psychology, so I made myself a daily survey. I kept it short–no room for multi-item measures–but it has questions for measuring how stressed I am, how happy I am, and how much sleep I got the previous night, among other things. At this point I’ve been filling it out for a few months.\nThis is, of course, terrible science. The samples are non-independent, I’m biased in my answers by whatever hypotheses I might be thinking about, and I’m not even consistent about the time of exact time of day at which I take the survey. My stress and mood obviously fluctuate a lot throughout the day. I try to think through the whole day when I fill out the survey, but the measures are rough at best.\nOne thing my survey doesn’t measure is cognitive performance. I don’t even ask myself how tired I’m feeling. I don’t ask about this because I think I have a better way to measure it: Chess."
  },
  {
    "objectID": "blog/chess_brain/index.html#is-online-chess-a-good-psychometric-tool",
    "href": "blog/chess_brain/index.html#is-online-chess-a-good-psychometric-tool",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "Is Online Chess a Good Psychometric Tool?",
    "text": "Is Online Chess a Good Psychometric Tool?\nI play online chess almost every day. I also like to play super-fast timed games, so I tend to play a lot of games per day. The site I play on, lichess.org, automatically records game and rating statistics.\nChess performance is closely tied to general cognitive capabilities, as is evident from this paper, this paper, and common sense. Can I use my chess performance to measure how cognitively capable I am day to day? How about hour to hour?\nLet’s start with the basics: How do we measure chess performance? I could get a really good AI chess algorithm to evaluate every individual move I make, but the easier option is just to look at rating. Lichess uses the Glicko 2 rating sytem, which uses all sorts of statistical tricks to estimate how good a player is at chess. At the conclusion of each game, both players’ ratings are updated. The winner gains some points and the loser loses some. The amount that they are updated depends on both their previous ratings and how certain it is that those ratings are accurate. So if I gain a lot of points after a game, that means that I did exceptionally well (by my own standards) by beating my opponent. Since I often play many games per day, the average change in my rating per game I play in a day should be a decent measure of how good my chess playing is that day.\nBefore we get to days, let’s start with hours in the day. Circadian rhythms–the body’s clock–have been found to modulate many basic cognitive processes. Since my current occupation allows me to play chess at various hours of the day, we can see if I tend to do better or worse at certain times.\nFor context, here are results from this study showing one person’s performance on a variety of cognitive tasks at various times throughout the day.\n\n\n\nDiurnal Variation in Cognitive Performance\n\n\nHere is my chess performance over the past few months.\n\n\n\nMy Chess Performance by Hour of the Day\n\n\nLooks about right! I bet if I stayed up later than usual and played chess into the night, there would be a gradual dropoff in performance, just like in subject H. K.’s multiplication speed above.\nThe results so far lend some initial validation to my measure. Let’s bring in the data from my survey and ask some more questions."
  },
  {
    "objectID": "blog/chess_brain/index.html#how-does-the-amount-of-sleep-i-get-at-night-affect-my-chess-performance",
    "href": "blog/chess_brain/index.html#how-does-the-amount-of-sleep-i-get-at-night-affect-my-chess-performance",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "How does the amount of sleep I get at night affect my chess performance?",
    "text": "How does the amount of sleep I get at night affect my chess performance?\n\n\n\nMy Chess Performance by Hours of Sleep the Previous Night\n\n\nRemember, this represents my rough recolection in late afternoon/evening of when I went to sleep the last night and woke up in the morning.\nThe effect looks to be mildy positive, with a peak around 8 hours. Does this mean I’m at my best when I get 8 hours of sleep? Unclear. I’m surprised at how weak this relationship seems to be. For some reason, the curve looks much more like what I expected when I limit it to data from my updated survey version, which I’ve been using only since December.\n\n\n\nMy Chess Performance by Hours of Sleep the Previous Night\n\n\nThis difference could be due to chance, or some pitfall of my measurement system, or something about my schedule since December as opposed to before."
  },
  {
    "objectID": "blog/chess_brain/index.html#how-do-my-affect-and-anxiety-affect-the-amount-of-chess-i-play",
    "href": "blog/chess_brain/index.html#how-do-my-affect-and-anxiety-affect-the-amount-of-chess-i-play",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "How do my affect and anxiety affect the amount of chess I play?",
    "text": "How do my affect and anxiety affect the amount of chess I play?\n\n\n\nMy Chess Performance by Affect\n\n\nI play less chess when I’m feeling good.\n\n\n\nMy Chess Performance by Anxiety"
  },
  {
    "objectID": "blog/chess_brain/index.html#how-these-things-affect-the-way-in-which-i-losewin-my-games",
    "href": "blog/chess_brain/index.html#how-these-things-affect-the-way-in-which-i-losewin-my-games",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "How these things affect the way in which I lose/win my games?",
    "text": "How these things affect the way in which I lose/win my games?\n \nIt looks like I’m a bit faster when I’m feeling happy and less anxious. That translates to winning more games on timeout (i.e. my opponent runs out of time) and losing fewer on timeout. It’s a small effect though. For now, I think chess should remain a fun game and not a diagnostic tool."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Louis Teitelbaum",
    "section": "",
    "text": "hire_your_clone\n\n\n\nDataViz\n\n\nShiny\n\n\nWeb Apps\n\n\n\n\n\n\n\nLouis Teitelbaum\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwormsplot\n\n\n\nR\n\n\nggplot2\n\n\nDataViz\n\n\n\n\n\n\n\nLouis Teitelbaum\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy #TidyTuesday contributions\n\n\n\nR\n\n\nggplot2\n\n\nDataViz\n\n\n\n\n\n\n\nLouis Teitelbaum\n\n\nMar 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npaper_provenance\n\n\n\nPython\n\n\nMeta-Science\n\n\nDataViz\n\n\nStreamlit\n\n\nWeb Apps\n\n\n\n\n\n\n\nLouis Teitelbaum\n\n\nMar 28, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/paper_provenance.html",
    "href": "projects/paper_provenance.html",
    "title": "paper_provenance",
    "section": "",
    "text": "An interactive web app for visualizing the history of a field leading up to a paper.\nThe app is now deployed at https://rimonim-paper-provenance-paper-provenance-qo5yhn.streamlit.app"
  },
  {
    "objectID": "projects/hire_your_clone.html",
    "href": "projects/hire_your_clone.html",
    "title": "hire_your_clone",
    "section": "",
    "text": "An interactive web app for exploring patterns in boss-employee pairings among high-level executives.\nThe app is now deployed at https://rimonim.shinyapps.io/hire_your_clone/"
  },
  {
    "objectID": "projects/wormsplot.html",
    "href": "projects/wormsplot.html",
    "title": "wormsplot",
    "section": "",
    "text": "A worms plot is an elegant way to visualize historical (or fictional!) characters as they move from place to place. Expanding on ggplot2, this package provides a new geom, geom_worm, which allows for an arbitrary number of moves within each worm, and takes intuitively structured data input. The package also includes a convenience function, wormsplot, for generating aesthetic plots with minimal effort."
  },
  {
    "objectID": "projects/wormsplot.html#installation",
    "href": "projects/wormsplot.html#installation",
    "title": "wormsplot",
    "section": "Installation",
    "text": "Installation\nYou can install the development version of wormsplot from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"rimonim/wormsplot\")"
  },
  {
    "objectID": "projects/wormsplot.html#example",
    "href": "projects/wormsplot.html#example",
    "title": "wormsplot",
    "section": "Example",
    "text": "Example\nHere is a plot of the lives of scientists who won the Nobel Prize for physics between 1901 and 1907.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\nlibrary(wormsplot)\n\ndata(nobel_physicists)\n\nnobel_physicists %&gt;%\n  filter(name %in% head(unique(name), 10)) %&gt;%\n  mutate(country = forcats::fct_drop(country)) %&gt;%\n  wormsplot('year', 'country', 'name', worm.color = 'initial',\n            worm.args = list(linewidth = 5.1, lineend = 'round'), region.label.width = 22, label.args = list(size = 3.6)) +\n  labs(title = \"The Lives of Winners of the Nobel Prize in Physics 1901-1907\")\n\n\n\n\nPlots can also be built from scratch using geom_worm() or stat_worm():\n\nlibrary(colorspace)\nlibrary(ggborderline)\n\ndata &lt;- data.frame(\n  x = c(5, 10, 25, 30, 15, 20, 25),\n  y = c(1, 2, 1.5, 1.5, 2.2, 1.2, 1.2),\n  person = c('Me', 'Me', 'Me', 'Me', 'You', 'You', 'You')\n  )\n\nggplot(data, aes(x, y, group = person, color = person, bordercolor = after_scale(darken(colour, .2)))) +\n  stat_worm(linewidth = 10, shorten_lines = 10, geom = 'borderline', lineend = 'round') +\n  theme_minimal() +\n  scale_y_continuous(limits = c(0, 3))"
  },
  {
    "objectID": "projects/tidy_tuesday.html",
    "href": "projects/tidy_tuesday.html",
    "title": "My #TidyTuesday contributions",
    "section": "",
    "text": "#TidyTuesday is a data visualization weekly challenge. Every week, the organizers post a raw dataset and challenge the community to tell a story with it.\nFind out more\nClick on a plot to see the code I used to make it."
  },
  {
    "objectID": "projects/tidy_tuesday.html#time-zones",
    "href": "projects/tidy_tuesday.html#time-zones",
    "title": "My #TidyTuesday contributions",
    "section": "2023-04 - Time Zones",
    "text": "2023-04 - Time Zones"
  },
  {
    "objectID": "projects/tidy_tuesday.html#baby-names",
    "href": "projects/tidy_tuesday.html#baby-names",
    "title": "My #TidyTuesday contributions",
    "section": "2022-12 - Baby Names",
    "text": "2022-12 - Baby Names"
  },
  {
    "objectID": "design.html",
    "href": "design.html",
    "title": "Louis Teitelbaum",
    "section": "",
    "text": "Art\n\n\nIn design, form follows function. In art…\n\n\n\n\n\n\n\n\n\n\n\n\n\nFliers\n\n\nInvitations to events, designed to be attention-grabbing, beautiful, and informative.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogos\n\n\nBrand logos for products, people, companies, or clubs.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/semantic_history/index.html",
    "href": "blog/semantic_history/index.html",
    "title": "The History of Semantic Spaces",
    "section": "",
    "text": "SemanticHistory\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n    \n    \n    \n    \n\n\n\n\n\n\n\n\nGoogle's Ngram viewer is awesome. Among other things, it's useful for studying historical interest in particular ideas. The 2011 Science paper has some fascinating examples of this. Here's one of the least impressive examples from that paper:\n\nThe problem with this approach is that it's impossible to tell the difference between intrest in the idea of God and usage of the word \"God\". This is elegantly demonstrated with another example from that paper:\n\nIf we had searched only for \"the Great War\", we might have thought that interest in that topic waned after the start of World War II. The truth, though, is that people became more interested in the Great War. They just started calling it \"World War I\" instead.\nThe Science paper shows off some elegant solutions to this problem, but all are both extremely labor-intensive and applicable only to the question they were designed to answer. Wouldn't it be nice to have an automated way to search for semantic spaces rather than having to guess at the particular Ngrams that represent them?\nI think I can make that happen.\nThe Plan¶My plan is to leverage cutting-edge semantic embeddings to generate a list of closely related words from a search term. Then I'll get timeseries data from the Ngram viewer for each of those words. Finally, I'll aggredate the data across words in the set, weighting them by their semantic similarity to the search term. This weighted sum will thus give an account of historical interest in the whole semantic space surrounding the search term.\nStep 1: Defining Semantic Spaces¶The first step is to get a list of closely related words, along with a quantification of their semantic similarity to the target. This is made trivial by the gensim package, which comes with a number of pretrained model vector embeddings. I'll use the \"glove-wiki-gigaword-200\" dataset, which is trained on Wikipedia (as it was in 2014) and the Gigaword newswire archive. As such, it provide a reasonable estimate of the kind of associations I (as a literate person living around 2014) might have with any given word - with some bias toward newsworthiness.\n\n\n\n\n\n\n\n\n\nIn [12]:\n\n     \nimport numpy as np\nimport pandas as pd\nimport gensim.downloader as gs\n\nglove_vectors = gs.load('glove-wiki-gigaword-200')\nglove_vectors.sort_by_descending_frequency()\n\n\n     \n\n\n\n\n\n\n\n\n\n\n\nNow we can get our list of most closely related words. For now I'm not going to worry about exactly what \"closely related\" means, but this will become important later.\n\n\n\n\n\n\n\n\n\nIn [5]:\n\n     \nglove_vectors.most_similar(['science', 'math'], topn = 10)\n\n\n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    Out[5]:\n\n\n\n\n\n[('mathematics', 0.768941342830658),\n ('physics', 0.6959351301193237),\n ('biology', 0.6668677926063538),\n ('teaching', 0.6357982158660889),\n ('curriculum', 0.635124921798706),\n ('chemistry', 0.6340017318725586),\n ('sciences', 0.6332299709320068),\n ('education', 0.630456268787384),\n ('graduate', 0.6268227696418762),\n ('academic', 0.6119447350502014)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Scraping¶The next prerequisite is the ability to get ngram data with code. The code below does this, outputing a nicely formated pandas DataFrame with a column for each ngram.\n\n\n\n\n\n\n\n\n\nIn [6]:\n\n     \nimport requests\n\ndef get_ngrams(query, start_year = 1800, end_year = 2019):\n    # Define Parameters for Request\n    params = {\n    \"content\": query,\n    \"year_start\": start_year,\n    \"year_end\": end_year\n    }\n    headers = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.87 Safari/537.36\",\n    }\n    # Make Request\n    html = requests.get(\"https://books.google.com/ngrams/json\", params=params, headers=headers, timeout=30).text\n\n    # Clean Data\n    df = pd.read_json(html)\n    df = pd.DataFrame(df['timeseries'].tolist(), columns = np.arange(params['year_start'], params['year_end']+1), index = df['ngram'])\n    df.index.name = \"year\"\n    df = df.transpose()\n    return df\n\n\n     \n\n\n\n\n\n\n\n\n\nIn [7]:\n\n     \ndf = get_ngrams(\"the Great War, World War I, World War II\")\ndf.head()\n\n\n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    Out[7]:\n\n\n\n\n\n\n\n\n\n\nyear\nthe Great War\nWorld War I\nWorld War II\n\n\n\n\n1800\n8.974028e-09\n8.142913e-08\n1.477712e-07\n\n\n1801\n7.179222e-09\n6.514331e-08\n1.182170e-07\n\n\n1802\n5.982685e-09\n5.428609e-08\n9.902061e-08\n\n\n1803\n5.479016e-09\n4.653093e-08\n8.487481e-08\n\n\n1804\n4.345168e-09\n3.009015e-08\n5.482785e-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Putting it Together¶The class SemanticHistory takes as input a search term (or list of terms whose vector embeddings are to be averaged) and generates a list of similar words (100 by default) along with their cosine similarity to the input, self.most_similar. It feeds this list of words into the Ngram viewer, and creates a dataframe with weighted frequencies for each word in the list, self.raw_data. The plot method displays a stacked area plot.\n\n\n\n\n\n\n\n\n\nIn [8]:\n\n     \nimport plotly.express as px\n\nclass SemanticHistory:\n    def __init__(self, search = [], sample_n = 20, start_year = 1800, end_year = 2019):\n        if type(search) == list:\n            self.search = [str.lower(str(s)) for s in list(search)]\n        else:\n            self.search = [str.lower(str(search))]\n        self.sample_n = sample_n\n\n        if not any(\" \" in s for s in search):\n            self.most_similar = glove_vectors.most_similar(self.search, topn = sample_n)\n            words = self.search + [word[0] for word in self.most_similar]\n            query = ', '.join(words)\n            self.raw_data = get_ngrams(query, start_year, end_year)\n        else:\n            raise Exception(\"Input must be a single word or list of single words.\")\n    def plot(self, weighted = True):\n        if weighted == True:\n            data = self.raw_data.multiply(([1]*len(self.search) + [word[1] for word in self.most_similar]))\n        else:\n            data = self.raw_data\n        fig = px.area(data,\n                      labels = {'value':('Weighted '*weighted + 'Frequency'), 'year':'Word', 'index':'Year'},\n                      template = 'plotly_white')\n        fig.update_layout(xaxis_title = None,\n                          yaxis_showticklabels = False,\n                          legend=dict(title=None, orientation = \"h\"))\n        fig.show()\n\n\n     \n\n\n\n\n\n\n\n\n\n\n\nLet's try exploring some semantic spaces!\n\n\n\n\n\n\n\n\n\nIn [10]:\n\n     \ngender_history = SemanticHistory(['gender', 'sex'])\nhonor_history = SemanticHistory(['honor', 'dignity'])\nequality_history = SemanticHistory(\"equality\")\neating_history = SemanticHistory(\"eating\")\nliberalarts_history = SemanticHistory(['literature', 'arts', 'poetry'])\ndarkness_history = SemanticHistory(['dark', 'gloomy'])\n\n\n     \n\n\n\n\n\n\n\n\n\nIn [11]:\n\n     \ngender_history.plot(weighted = False)\nhonor_history.plot(weighted = False)\nequality_history.plot(weighted = False)\neating_history.plot(weighted = False)\nliberalarts_history.plot(weighted = False)\ndarkness_history.plot(weighted = False)\n\n\n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n\n\n\n\n        \n        \n\n\n\n\n\n\n    \n    \n\n\n\n\n                                                \n\n\n\n\n\n\n    \n    \n\n\n\n\n                                                \n\n\n\n\n\n\n    \n    \n\n\n\n\n                                                \n\n\n\n\n\n\n    \n    \n\n\n\n\n                                                \n\n\n\n\n\n\n    \n    \n\n\n\n\n                                                \n\n\n\n\n\n\n    \n    \n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI'm delighted with this concept, but this is just the beginning. I'd like to turn this into web app that anyone can use online. Before that happens, though, here are some features I plan on adding:\n\nRemove stopwords from corpus, so that words like \"and\" don't clutter the chart.\nPaired Comparisons (e.g. Male AS OPPOSED TO Female).\nSupport for 2-grams and 3-grams (i.e. multi-word phrases)"
  },
  {
    "objectID": "blog/minecraft_cogsci/index.html",
    "href": "blog/minecraft_cogsci/index.html",
    "title": "Predicting Questions in Dialogue",
    "section": "",
    "text": "The Minecraft Dialogue Corpus (Narayan-Chen et al., 2019) is a collection of 509 text-based conversations between players collaborating to build structures. The “Architect” is shown a target structure and needs to explain to the “Builder” how to build it. The Architect can observe the Builder but cannot place blocks.\nFor now, I am using this corpus as practice to learn Bayesian statistics as I go through Statistical Rethinking, by Richard McElreath with its wonderful brms/tidyverse translation by A Solomon Kurz.\nCode and data for this project can be found here.\nA sample dialogue from the Minecraft Corpus:\nAll three of the Builder’s turns in this dialogue (lines 2, 5, and 8) are questions. They have some qualities in common: they all request something of the interlocutor, and they all signal their status as questions with a question mark at the end. This latter feature makes them easy to identify with one line of code:\nminecraftcorpusdf %&gt;% mutate(questionmark = grepl(\"\\\\?\", text))\nWhat causes people to ask questions in the Minecraft collaborative building task? Let’s start by thinking about the Builder’s questions from the excerpt above.\nThe question on line 2, “Now what?” does not have an obvious antecedent - the Builder seems to have understood and excecuted the previous instruction and is now merely moving the conversation along by asking for another one.\nThe questions on lines 5 and 8, on the other hand, do have clear antecedents in the conversation. Specifically, they each refer to the architect’s immediately preceding instruction and request clarification thereof. These questions can therefore be considered repair initiations, turns in talk that identify trouble (i.e. a need for clarification) in a preceding turn or turns uttered by an interlocutor.\nHence the first theoretical answer to my question: What causes people to ask questions? The need for clarification.\nOf course not all questions are repair initiations, and not all repair initiations are presented as questions. Nevertheless, the structure of the Minecraft collaborative building task makes the correlation very high. In the task, the Architect has access to all of the information that the Builder needs to proceed (the design of the target structure) and information is the only thing that the Builder can get from the Architect. This means that pretty much all of the Builder’s questions are aimed at clarifying information coming from the Architect.\nFor these reasons, and since I have no way of identifying true repair iniitations other than going through the whole corpus myself, I will operationally define repair as any Builder’s utterance that includes a question mark.\nminecraftcorpusdf %&gt;% mutate(repair = questionmark & role == \"B\")\nTo convince you that almost all Builder questions in the corpus really are repair initiations, here are 10 randomly selected repairs:\nminecraftcorpusdf %&gt;%\n  filter(repair == TRUE) %&gt;%\n  select(text) %&gt;%\n  sample_n(10)\n  \n#&gt; 1785    like that?\n#&gt; 4312    like so? or like that?\n#&gt; 14821   Are they on the ground?\n#&gt; 15121   here?\n#&gt; 628     like that?\n#&gt; 3642    ooooh like this?\n#&gt; 1571    it might be easier to describe all one color? then build from there?\n#&gt; 182     is the purple supposed to be on the third level? i think i could make it float..\n#&gt; 12521   Facing towards the middle?\n#&gt; 2951    is this correct?\nEven without looking at context, it seems clear at a glance that all but line 1571 here are indeed repairs."
  },
  {
    "objectID": "blog/minecraft_cogsci/index.html#how-to-predict-repair",
    "href": "blog/minecraft_cogsci/index.html#how-to-predict-repair",
    "title": "Predicting Questions in Dialogue",
    "section": "How to Predict Repair",
    "text": "How to Predict Repair\nBefore I start exploring predictor variables, I need to figure out how to model the probability of repair. My first instinct was use Builder turns as cases and model whether of not they are repairs. There’s a fatal flaw with this plan though: the natural alternative to initiating repair is probably not initiating something other than repair. The alternative is more likely to be not saying anything. In other words, we only have data about whether or not texts the Builder sent were repairs. We have no data about texts the Builder didn’t send.\nI’ll draw that more formally as a DAG:\n\nlibrary(ggdag)\nshorten_dag_arrows &lt;- function(tidy_dag, shorten_distance){\n  # Update underlying ggdag object\n  tidy_dag$data &lt;- dplyr::mutate(tidy_dag$data, \n                                 proportion = shorten_distance/sqrt((xend - x)^2 + (yend - y)^2),\n                                 xend = (1-proportion/2)*(xend - x) + x, \n                                 yend = (1-proportion/2)*(yend - y) + y,\n                                 xstart = (1-proportion/2)*(x - xend) + xend,\n                                 ystart = (1-proportion/2)*(y-yend) + yend) %&gt;% select(!proportion)\n  return(tidy_dag)\n}\n\n# Length -&gt; TF-IDF Sum -&gt; Probability of Repair -&gt; Question Mark at End\n# Turns since Last Repair -&gt; Probability of Repair -&gt; Question Mark at End\ndag_coords_0 &lt;-\n  tibble(name = c(\"C\", \"U\", \"R\", \"Q\"),\n         x    = c(1, 2, 3, 4),\n         y    = c(2, 2, 2, 2))\n\ndagify(U ~ C,\n       R ~ U,\n       Q ~ R,\n       coords = dag_coords_0) %&gt;%\n  tidy_dagitty() %&gt;%\n  shorten_dag_arrows(.25) %&gt;%\n  dag_label(labels = c(\"C\" = \"\", \n                       \"U\" = str_wrap(\"Builder Responds\", 10), \n                       \"R\" = \"Repair\", \n                       \"Q\" = str_wrap(\"Question Mark\", 10))) %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"R\"), size = 30, show.legend = F) +\n  geom_dag_text(aes(label = label), size = 4) +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +\n  geom_dag_edges(aes(x = xstart, y = ystart)) +\n  theme_dag()\n\n\n\n\n? -&gt; Builder Responds -&gt; Probability of Repair -&gt; Question Mark at End\n\n\nAs yet undiscussed variables predict whether or not the Builder will respond to an instruction from the Architect, which in turn predicts whether the response will be a repair initiation (if the Builder doesn’t respond there’s zero probability of the response being anything), which in turn influences our proxy variable, the presence of a question mark.\nAll of this means that my inital stategy would probably wash out any effect of predictors on repair, since we would already be stratifying by whether the Builder responds. I think the simplest way to solve this problem is to use Architect turns as cases and predict whether or not the next turn will be a Builder repair. Sometimes the next turn will be another Architect turn (when Builder did not respond), and sometimes it will be a non-repair Builder turn.\nTime to code a few predictor variables.\n\n# Orthographic Length of Previous Turn\n# Mean TF-IDF of previous turn (word-fanciness? information density?)\n# Sum TF-IDF of previous turn (lexical complexity? information?)\n# Turns since last question asked\n\nd1 &lt;- convert(minecraftcorpus_tfidf, to = \"data.frame\") %&gt;% \n  select(!doc_id) %&gt;%\n  transmute(tfidfsum = rowSums(across())) %&gt;%\n  bind_cols(minecraftcorpusdf) %&gt;%\n  select(!c(file, session)) %&gt;%\n  mutate(repair = grepl(\"\\\\?\", text) & role == \"B\",\n         repair_next = NA,\n         length = nchar(text),\n         charssincerepair = 0,\n         partner = if_else(role == \"B\", \n                           str_remove_all(str_sub(conversation, -2, -1), \"A\"),\n                           str_remove_all(str_sub(conversation, 2, 3), \"-\")))\n\ncharssincerepair &lt;- 0L\n\nfor (n in 2:nrow(d1)) {\n  if(d1$conversation[n-1L] != d1$conversation[n]) {\n    charssincerepair &lt;- 0L\n  }\n  if(d1$repair[n] == TRUE & d1$role[n] == \"B\"){\n    charssincerepair &lt;- 0L\n  }\n  d1$charssincerepair[n] &lt;- charssincerepair\n  charssincerepair &lt;- charssincerepair + d1$length[n]\n}\n\nfor (n in 1:(nrow(d1)-1L)) {\n  if(d1$conversation[n+1L] == d1$conversation[n]) {\n    d1$repair_next[n] &lt;- d1$repair[n+1]\n  }\n}\n\n# Repair as Factor, + Case Index\nd1 &lt;- d1 %&gt;%  \n  mutate(repair_next  = factor(repair_next, levels = c(FALSE, TRUE)),\n         case = factor(1:n()))\n\nd1 &lt;- d1 %&gt;%\n  filter(role == \"A\",\n         charssincerepair != 0,\n         tfidfsum != 0) %&gt;%\n  mutate(charssincerepair_log = log(charssincerepair),\n         charssincerepair_log_s = (charssincerepair_log-mean(charssincerepair_log, na.rm = T))/sd(charssincerepair_log, na.rm = T),\n         tfidfsum_log = log(tfidfsum),\n         tfidfsum_log_s = (tfidfsum_log - mean(tfidfsum_log, na.rm = T))/sd(tfidfsum_log, na.rm = T))"
  },
  {
    "objectID": "blog/minecraft_cogsci/index.html#length-of-previous-turn",
    "href": "blog/minecraft_cogsci/index.html#length-of-previous-turn",
    "title": "Predicting Questions in Dialogue",
    "section": "Length of Previous Turn",
    "text": "Length of Previous Turn\nI have already theorized that the likelihood of a given Builder turn being a repair initiation is increased by the need for clarification of previous turns. Are there certain types of instructions that need to be clarified more often? How about long and complicated ones?\nHere’s a quick and dirty graph of repair against length of the previous turn, with repair formatted as numeric and a Loess line running between No and Yes:\n\nlibrary(ggbeeswarm)\n\nd1 %&gt;%\n  ggplot(aes(length, (as.numeric(repair_next)-1))) +\n  geom_quasirandom(method = \"pseudorandom\", \n                   width = .2,\n                   groupOnX = F, \n                   alpha = .1, \n                   varwidth = T) +\n  geom_smooth() +\n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(breaks = c(0, .25, .5, .75, 1),\n                     labels = c(\"No\", .25, .5, .75, \"Yes\")) +\n  labs(x = \"Length of Turn (characters, log scale)\",\n       y = \"Probability of Next-Turn Repair\") +\n  theme_minimal()\n\n\n\n\nRepair X Length of Previous Turn\n\n\nLooks promising! It’s hard to tell just by looking at the data points, but the regression line seems to think that longer previous turns are associated wih more repairs. It is worth noting at this point that there are very few next-turn repairs and very many next-turn non-repairs. This means that when we model this formally we will have to be careful interpreting the regression coefficients.\nWe might do a bit better if, rather than counting the number of characters, we had a measure more closely related to how much information is being conveyed. TF-IDF (Term Frequency * Inverse Document Frequency) fits the bill. The TF-IDF of a word describes how rare it is in the whole corpus vs. how common it is in its own turn. Presumably, rarer words are less predictable and therefore more informative and more confusing. The sum of TF-IDF scores of all words in an turn should tell us something about how much new semantic material is included in each turn.\n\n\n\nRepair X Length of Previous Turn\n\n\nThis looks similar to the first one. Indeed, TF-IDF Sum and Turn length are correlated in the corpus at r = 0.956. Nevertheless, I’m going to stick with TF-IDF Sum because it makes more sense to me as a theoretical predictor.\n\nBayesian Modeling\nI’ll start by simulating reasonable priors. I’ll let the slope be positive or negative. Less than 15% of Builder turns are repairs, so I’ll lower the intercepts. After playing around with the parameters a bit, I settled on this:\n\n# Simulating Reasonable Priors\nd1 %&gt;%\n  group_by(repair_next) %&gt;%\n  summarise(perc = 100*n()/nrow(.))   # 14.3% of Architect turns are immediately followed by a Builder repair initiation\n\npriors &lt;- \n  tibble(n = 1:50,\n         a = rnorm(50, -1.5, 1),\n         b = rnorm(50, 0, 1)) %&gt;% \n  expand(nesting(n, a, b), x = seq(from = -3, to = 3, length.out = 200)) %&gt;% \n  mutate(p = inv_logit_scaled(a+b*x)) %&gt;%\n  arrange(n) %&gt;%\n  mutate(n = factor(n)) \npriors %&gt;%\n  ggplot(aes(x, p, group = n)) +\n  geom_line(alpha = .5)\n\nLet’s set up the model.\n\nlibrary(brms)\nlibrary(tidybayes)\n\ntfidf_mod &lt;- brm(\n  repair_next ~ 1 + tfidfsum_log_s,\n  data = d1,\n  family = bernoulli,\n  prior = c(prior(normal(-1.5, 1), class = Intercept),\n            prior(normal(0, 1), class = b)),\n  sample_prior = \"yes\")\n\nHere’s the model summary:\n\nprint(tfidf_mod))\n\n\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + tfidfsum_log_s \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept         -1.82      0.03    -1.87    -1.77 1.00     3175     2569\n## tfidfsum_log_s     0.26      0.03     0.20     0.31 1.00     2970     2172\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNow I can sample the posteriors and see what the models thinks.\n\nlibrary(ggbeeswarm)\n\nd1$tfidfsum_log_s[d1$tfidfsum == min(d1$tfidfsum, na.rm = T)] # -2.87\nd1$tfidfsum_log_s[d1$tfidfsum == max(d1$tfidfsum, na.rm = T)] # 2.51\n\n# Sample Prior and Posterior\n\ntfidf_mod_priors &lt;- as_draws_df(tfidf_mod, c(\"prior_Intercept\", \"prior_b\"))[1:100,] %&gt;%\n  as_tibble() %&gt;%\n  mutate(n = factor(1:100)) %&gt;%\n  expand(nesting(n, prior_Intercept, prior_b), x_log_s  = seq(from = -2.9, to = 2.51, length.out = 200)) %&gt;%\n  mutate(p = inv_logit_scaled(prior_Intercept+prior_b*x_log_s),\n         x_log = x_log_s * sd(d1$tfidfsum_log) + mean(d1$tfidfsum_log),\n         x = exp(x_log))\n\nn_iter &lt;- 50\ntfidf_mod_fitted &lt;-\n  fitted(tfidf_mod,\n         newdata  = tibble(tfidfsum_log_s = seq(from = -2.9, to = 2.51, length.out = 200)),\n         summary  = F,\n         nsamples = n_iter) %&gt;% \n  as_tibble() %&gt;%\n  mutate(iter = 1:n_iter) %&gt;% \n  pivot_longer(-iter) %&gt;% \n  mutate(tfidfsum_log_s = rep(seq(from = -2.9, to = 2.51, length.out = 200), times = n_iter)) %&gt;% \n  mutate(tfidfsum_log = tfidfsum_log_s * sd(d1$tfidfsum_log) + mean(d1$tfidfsum_log),\n         tfidfsum = exp(tfidfsum_log_s * sd(d1$tfidfsum_log) + mean(d1$tfidfsum_log)))\n\n\ntfidf_mod_postpredict &lt;- tfidf_mod_fitted %&gt;%\n  ggplot(aes(x = tfidfsum)) +\n    geom_hline(yintercept = .5, color = \"red\") +\n    geom_line(aes(y = value, group = iter), color = \"blue\", alpha = .1) +\n    geom_line(data = tfidf_mod_priors,\n              aes(x, p, group = n), color = \"black\", alpha = .08) + \n    geom_quasirandom(data = d1,\n                     aes(x = tfidfsum,\n                         y = as.integer(repair_next)-1),\n                     alpha = 1/10,\n                     groupOnX = F,\n                     width = 1/10,\n                     method = \"pseudorandom\",\n                     varwidth = T) +\n    scale_x_continuous(trans = \"log10\", minor_breaks = seq(10, 100, by = 10)) +\n    scale_y_continuous(breaks = c(0, .25, .5, .75, 1),\n                       labels = c(\"No\", .25, .5, .75, \"Yes\")) +\n    labs(title = \"Data with Prior and Posterior Predictions\",\n         y = \"Probability of Next-Turn Repair\", \n         x = \"Sum TF-IDF (Log Scale)\") +\n    theme_minimal()\n\ntfidf_mod_postpredict\n\nThe faint grey lines are 100 samples from the prior distribution. In blue are 50 samples from the posterior.\n\n\n\nTF-IDF Model Data with Prior and Posterior Predictions\n\n\nThe posterior predictions look almost like a straight line on the logarithmic scale - for very short, simple instructions from the Architect, the Builder’s response is most likely not to be a repair. As total TF-IDF goes up, the probability of repair does too, at first rapidly, then more slowly."
  },
  {
    "objectID": "blog/minecraft_cogsci/index.html#time-since-last-repair",
    "href": "blog/minecraft_cogsci/index.html#time-since-last-repair",
    "title": "Predicting Questions in Dialogue",
    "section": "Time Since Last Repair",
    "text": "Time Since Last Repair\nAs described in Dingemanse et al. (2015), another predictor of repair is the time elapsed since the last repair. People don’t tend to initiate repair twice in a row. In lieu of using actual timestamps, I’ll use the total number of characters typed as a proxy for time elapsed. I’ll use the same priors as before.\n\ncharssincerepair_mod &lt;- brm(\n  repair_next ~ 1 + charssincerepair_log_s,\n  data = d1,\n  family = bernoulli,\n  prior = c(prior(normal(-1.5, 1), class = Intercept),\n            prior(normal(0, 1), class = b)),\n  sample_prior = \"yes\")\n\n\nprint(charssincerepair_mod)\n\n\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + charssincerepair_log_s \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept                 -1.81      0.03    -1.87    -1.76 1.00     3148     2719\n## charssincerepair_log_s    -0.24      0.03    -0.29    -0.19 1.00     3209     2570\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\ncharssincerepair_mod_priors &lt;- as_draws_df(charssincerepair_mod, c(\"prior_Intercept\", \"prior_b\"))[1:100,] %&gt;%\n  as_tibble() %&gt;%\n  mutate(n = factor(1:100)) %&gt;%\n  expand(nesting(n, prior_Intercept, prior_b), x_log_s  = seq(from = -4, to = 2.63, length.out = 200)) %&gt;%\n  mutate(p = inv_logit_scaled(prior_Intercept+prior_b*x_log_s),\n         x_log = x_log_s * sd(d1$charssincerepair_log) + mean(d1$charssincerepair_log),\n         x = exp(x_log))\n\nn_iter &lt;- 50\ncharssincerepair_mod_fitted &lt;-\n  fitted(charssincerepair_mod,\n         newdata  = tibble(charssincerepair_log_s = seq(from = -4, to = 2.63, length.out = 200)),\n         summary  = F,\n         nsamples = n_iter) %&gt;% \n  as_tibble() %&gt;%\n  mutate(iter = 1:n_iter) %&gt;% \n  pivot_longer(-iter) %&gt;% \n  mutate(charssincerepair_log_s = rep(seq(from = -4, to = 2.63, length.out = 200), times = n_iter)) %&gt;% \n  mutate(charssincerepair_log = charssincerepair_log_s * sd(d1$charssincerepair_log) + mean(d1$charssincerepair_log),\n         charssincerepair = exp(charssincerepair_log_s * sd(d1$charssincerepair_log) + mean(d1$charssincerepair_log)))\n\n\ncharssincerepair_mod_postpredict &lt;- charssincerepair_mod_fitted %&gt;%\n  ggplot(aes(x = charssincerepair)) +\n  geom_hline(yintercept = .5, color = \"red\") +\n  geom_line(aes(y = value, group = iter), color = \"blue\", alpha = .1) +\n  geom_line(data = charssincerepair_mod_priors,\n            aes(x, p, group = n), color = \"black\", alpha = .08) + \n  geom_quasirandom(data = d1,\n                   aes(x = charssincerepair,\n                       y = as.integer(repair_next)-1),\n                   alpha = 1/10,\n                   groupOnX = F,\n                   width = 1/10,\n                   method = \"pseudorandom\",\n                   varwidth = T) +\n  scale_x_continuous(trans = \"log10\", minor_breaks = seq(10, 100, by = 10)) +\n  scale_y_continuous(breaks = c(0, .25, .5, .75, 1),\n                     labels = c(\"No\", .25, .5, .75, \"Yes\")) +\n  labs(title = \"Data with Prior and Posterior Predictions\",\n       y = \"Probability of Next-Turn Repair\", \n       x = \"Total Characters Since Last Repair (Log Scale)\") +\n  theme_minimal()\n\ncharssincerepair_mod_postpredict\n\n\n\n\nCharacters Since Last Repair Model Data with Prior and Posterior Predictions\n\n\nHmmm. The model thinks that repairs get less likely as time goes on after the last repair. I have a hard time believing that. Where did I go wrong?\nAnswer: I didn’t use a multilevel model.\nWhat I think is happening is this: most of the variation in likelihood of next-turn repair is between participants. As it happens, pairs of participants in which the Builder is more likely to initiate next-turn repair are also more likely to have short turns and therefore fewer total characters since the last repair. Or something like that.\nBefore I get into complicated modeling to test this directly, I’ll drive home the main point with a simple, intercept-only multilevel model.\n\nbysubj_mod &lt;- \n  brm(data = d1, \n      family = bernoulli,\n      repair_next ~ 1 + (1 | partner),\n      prior = c(prior(normal(-1.5, 1), class = Intercept), \n                prior(exponential(1), class = sd)),\n      iter = 5000, chains = 4, cores = 2,\n      sample_prior = \"yes\")\n\nplot(bysubj_mod)\nprint(bysubj_mod)\n\nbysubj_mod_post &lt;- as_draws(bysubj_mod)\n\nbysubj_mod_post_mdn &lt;- \n  coef(bysubj_mod, robust = T)$partner[, , ] %&gt;% \n  data.frame() %&gt;% \n  mutate(post_mdn = inv_logit_scaled(Estimate),\n         post_97.5 = inv_logit_scaled(Q97.5),\n         post_2.5 = inv_logit_scaled(Q2.5),\n         partner = unique(d1$partner)) %&gt;%\n  right_join(d1) %&gt;%\n  group_by(Estimate, Est.Error, post_2.5, post_97.5, post_mdn, partner) %&gt;%\n  summarise(prop_repair_next = sum(repair_next == T)/n())\n\nView(bysubj_mod_post)\n\nbysubj_mod_post_mdn %&gt;%\n  ggplot(aes(partner)) +\n    geom_hline(yintercept = inv_logit_scaled(median(bysubj_mod_post$`1`$b_Intercept)), \n               linetype = 2, color = \"orange2\") +\n    geom_point(aes(y = post_mdn, color = \"Model Intercepts\"), size = 3) +\n    geom_errorbar(aes(ymin = post_2.5, ymax = post_97.5, color = \"Model Intercepts\")) +\n    geom_point(aes(y = prop_repair_next, color = \"Empirical Proportions\"), size = 3, shape = 1) +\n    labs(title = \"Individual Differences in Repair Behavior\",\n         subtitle = str_wrap(\"The dashed line represents the model average intercept. \n                             Error bars represent 95% confidence interval.\", 60),\n         x = \"Builder ID\",\n         y = \"Probability of Next-Turn Repair\") +\n    scale_color_manual(name = element_blank(), \n                       values = c(\"black\", \"orange2\")) +\n    theme_minimal()\n\n\n\n\nIndividual Differences in Repair Behavior\n\n\nAs expected, different Builders have vastly different likelihoods of initiating next-turn repair.\nNow for the monster model. I just showed that different Builders have different likelihoods of initiating next-turn repair, but the same is probably true for different Architects. An Architect who likes to type a lot of turns in a row will lower the probability of any given next-turn being a repair, just as a Builder who initiates repair a lot will raise the probability. Clustering by both Architect and Builder gets complicated, and for now I’m only interested in group-level effects, so I’m just going to cluster by conversation and leave the model agnostic about whether it’s the Builder, the Architect, or some interaction between them that’s causing any difference. I’m also allowing both intercept and slope to vary, and telling the model to estimate the correlation between them.\n\ncharssincerepair_bysubj_mod &lt;- \n  brm(data = d1, \n      family = bernoulli,\n      repair_next ~ 1 + charssincerepair_log_s + (1 + charssincerepair_log_s | conversation),\n      prior = c(prior(normal(-1.5, 1), class = Intercept), \n                prior(normal(0, 1), class = b),\n                prior(exponential(1), class = sd),\n                prior(lkj(2), class = cor)),\n      iter = 5000, chains = 4, cores = 2)\n\nprint(charssincerepair_bysubj_mod)\n\n\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + charssincerepair_log_s + (1 + charssincerepair_log_s | conversation) \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 5000; warmup = 2500; thin = 1;\n##          total post-warmup draws = 10000\n## \n## Group-Level Effects: \n## ~conversation (Number of levels: 64) \n##                                       Estimate Est.Error l-95% CI u-95% CI Rhat\n## sd(Intercept)                             0.76      0.08     0.62     0.94 1.00\n## sd(charssincerepair_log_s)                0.13      0.06     0.01     0.25 1.00\n## cor(Intercept,charssincerepair_log_s)     0.29      0.28    -0.33     0.79 1.00\n##                                       Bulk_ESS Tail_ESS\n## sd(Intercept)                             2003     3424\n## sd(charssincerepair_log_s)                1927     2051\n## cor(Intercept,charssincerepair_log_s)     7747     4847\n## \n## Population-Level Effects: \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept                 -1.87      0.10    -2.08    -1.68 1.00     1365     2679\n## charssincerepair_log_s     0.02      0.04    -0.05     0.09 1.00     7615     7229\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nBingo. The population-level effect of characters since repair is now slightly positive! Let’s visualize that:\n\n\n\nData with Multilevel Model Estimates\n\n\nThe blue line is the model’s estimate of the average effect of characters since last repair on repair probability within groups, with its 95% confidence interval. The grey lines are estimates for each conversation. This time I’m convinced - the effect of content elapsed since the last repair is small if it exists at all. The graph I linked to above from Dingemanse et al. (2015) shows the probability that repair will have occured, which of course rapidly approaches 1 as time goes on. The probability that any given turn will be a repair initiation doesn’t seem to change much, at least in the Minecraft corpus.\nAll this makes me want to re-do my analysis of TF-IDF Sum as a multilevel model. Here’s what that looks like:\n\ntfidfsum_bysubj_mod &lt;- \n  brm(data = d1, \n      family = bernoulli,\n      repair_next ~ 1 + tfidfsum_log_s + (1 + tfidfsum_log_s | conversation),\n      prior = c(prior(normal(-1.5, 1), class = Intercept), \n                prior(normal(0, 1), class = b),\n                prior(exponential(1), class = sd),\n                prior(lkj(2), class = cor)),\n      iter = 5000, chains = 4, cores = 2)\n      \nprint(tfidfsum_bysubj_mod)\n\n\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + tfidfsum_log_s + (1 + tfidfsum_log_s | conversation) \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 5000; warmup = 2500; thin = 1;\n##          total post-warmup draws = 10000\n## \n## Group-Level Effects: \n## ~conversation (Number of levels: 64) \n##                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)                     0.76      0.08     0.62     0.93 1.00     1843\n## sd(tfidfsum_log_s)                0.12      0.06     0.01     0.23 1.00     2173\n## cor(Intercept,tfidfsum_log_s)    -0.15      0.31    -0.72     0.50 1.00     9390\n##                               Tail_ESS\n## sd(Intercept)                     3034\n## sd(tfidfsum_log_s)                3267\n## cor(Intercept,tfidfsum_log_s)     5410\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept         -1.92      0.10    -2.11    -1.72 1.00     1431     2332\n## tfidfsum_log_s     0.27      0.04     0.20     0.34 1.00     8006     6499\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nDidn’t change much here, but it does look like the population-level effect is slightly higher than with the fixed-effects model. Interestingly enough, the model estimates that the intercept and beta are slightly negatively correlated. That means that pairs who initiate repair more in general also show less effect of utterance length (or TF-IDF sum, to be exact) on their rates. It seems possible to me that, in highly informational contexts like the Minecraft collaborative building task, some people have a top-down push to just initiate repair as much as possible, regardless of whether or not there’s an obvious need for it. All of this is pretty speculative though, since the estimated error on the correlation parameter is enormous.\nHere’s what the new model looks like as a graph:\n\n\n\nData with Multilevel Model Estimates"
  },
  {
    "objectID": "blog/text_comparison/index.html",
    "href": "blog/text_comparison/index.html",
    "title": "Comparing Four Methods of Sentiment Analysis",
    "section": "",
    "text": "Acknowledgement\n\n\n\nThank you Almog for a well-organized and scholarly introduction to this topic.\nSentiment analysis refers to the use of computational methods to extract emotional (or otherwise subjective) content from text. We ask: How happy is this text? Or: How anxious is this text?\nMany different methods for sentiment analysis exist, and the recent rise of large language models has opened the door to even more. This post will compare four methods: word-counts, distributed dictionary representation (DDR), and two methods of contextualized construct representation (CCR).\nI will pit these methods against each other at a simple classification task: identifying depressive/anxious (“poisonous”) comments in the Mental Health Corpus.\n# Mental Health dataset -&gt; Sample 500 documents from each category (without replacement)\nd &lt;- read_csv('mental_health.csv') %&gt;% \n  group_by(label) %&gt;% \n  slice_sample(n = 500, replace = FALSE) %&gt;% \n  ungroup() %&gt;% \n  # add docID\n  mutate(ID = as.character(1:n()))\n\n# No preprocessing required, since texts are already lowercase and without punctuation\nhead(d)\n\n# Convert to quanteda corpus object\nd_corpus &lt;- corpus(d, docid_field = \"ID\", text_field = \"text\")"
  },
  {
    "objectID": "blog/text_comparison/index.html#word-counts---valence-based-sentiment-analysis",
    "href": "blog/text_comparison/index.html#word-counts---valence-based-sentiment-analysis",
    "title": "Comparing Four Methods of Sentiment Analysis",
    "section": "1. Word counts - Valence-based sentiment analysis",
    "text": "1. Word counts - Valence-based sentiment analysis\nThis method takes an existing dictionary of words, each given a score on the extent to which it reflects a particular emotional category. I will use the “pleasure” category of the Affective Norms for English Words (ANEW) dictionary, included in quanteda.sentiment.\nThis method is simple and elegant: get the “pleasure” score of each word and compute the average for each comment in the dataset.\n\n# A Peak at the Dictionary\nprint(data_dictionary_ANEW, max_nval = 8)\n\nDictionary object with 3 key entries.\nValences set for keys: pleasure, arousal, dominance \n- [pleasure]:\n  - abduction, able, abortion, absent, absurd, abundance, abuse, accept [ ... and 2,463 more ]\n- [arousal]:\n  - abduction, able, abortion, absent, absurd, abundance, abuse, accept [ ... and 2,463 more ]\n- [dominance]:\n  - abduction, able, abortion, absent, absurd, abundance, abuse, accept [ ... and 2,463 more ]\n\nlapply(valence(data_dictionary_ANEW), head, 8)\n\n$pleasure\nabduction      able  abortion    absent    absurd abundance     abuse    accept \n     2.76      6.74      3.50      3.69      4.26      6.59      1.80      6.80 \n\n$arousal\nabduction      able  abortion    absent    absurd abundance     abuse    accept \n     5.53      4.30      5.39      4.73      4.36      5.51      6.83      5.53 \n\n$dominance\nabduction      able  abortion    absent    absurd abundance     abuse    accept \n     3.49      6.83      4.59      4.35      4.73      5.80      3.69      5.41 \n\n# Compute sentiment for each document and rejoin to original dataframe\nd &lt;- d_corpus %&gt;% \n  textstat_valence(dictionary = data_dictionary_ANEW[\"pleasure\"]) %&gt;% \n  rename(ID = doc_id, pleasure_valence = sentiment) %&gt;% \n  right_join(d)\n\nJoining with `by = join_by(ID, pleasure_valence)`"
  },
  {
    "objectID": "blog/text_comparison/index.html#distributed-dictionary-representations-ddr---embeddings-of-word-lists",
    "href": "blog/text_comparison/index.html#distributed-dictionary-representations-ddr---embeddings-of-word-lists",
    "title": "Comparing Four Methods of Sentiment Analysis",
    "section": "2. Distributed Dictionary Representations (DDR) - Embeddings of word lists",
    "text": "2. Distributed Dictionary Representations (DDR) - Embeddings of word lists\nThis method is similar to the last one, but instead of just averaging the pleasure scores in each comment, I will run both the dictionary words and the full comments through a cutting-edge large language model, and extract the model’s internal embedding of the input (basically, how the model thinks about the text). Then I can see how similar each comment embedding is to the average embedding of “pleasure” items in the dictionary.\nFor consistency, I’ll use the same dictionary here too, with embeddings from the second to last layer of BERT (base uncased) using the text package. I’ll get contextualized embeddings of the documents, and decontextualized embeddings for the words in the dictionary. I’ll the compute the embedding of the full dictionary as a weighted average of its embeddings by standardized valence. Finally, I will compute cosine similarity between each document embedding in the corpus and this dictionary embedding.\n\n# Embeddings for documents\nd_embeddings &lt;- textEmbed(d$text)\n\n# Embeddings for words in the Dictionary\ndict_embeddings &lt;- textEmbed(names(valence(data_dictionary_ANEW)[[1]]))\ndict_embeddings &lt;- dict_embeddings$texts[[1]] %&gt;% \n  mutate(ID = 1:n())\n\n# center valence, remove negative values, join to embeddings, and find weighted average\ndict_avg_embedding &lt;- valence(data_dictionary_ANEW)[[1]] %&gt;% enframe() %&gt;% \n  mutate(weight = value - mean(value, na.rm = TRUE),\n         ID = 1:n()) %&gt;% \n  filter(weight &lt; 0) %&gt;% \n  left_join(dict_embeddings, by = \"ID\") %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, ~weighted.mean(.x, weight))) %&gt;%\n  unlist(use.names=FALSE)\n\n# cosine similarity of each text to dictionary average embedding\nd$pleasure_similarity &lt;- NA\nfor (row in 1:nrow(d_embeddings$texts[[1]])) {\n  doc_embedding &lt;- slice(d_embeddings$texts[[1]], row) %&gt;% \n    unlist(use.names = FALSE)\n  d$pleasure_similarity[row] &lt;- lsa::cosine(doc_embedding, dict_avg_embedding)\n  }"
  },
  {
    "objectID": "blog/text_comparison/index.html#contextualized-construct-representation-ccr---embeddings-of-questionnaires",
    "href": "blog/text_comparison/index.html#contextualized-construct-representation-ccr---embeddings-of-questionnaires",
    "title": "Comparing Four Methods of Sentiment Analysis",
    "section": "3. Contextualized Construct Representation (CCR) - Embeddings of questionnaires",
    "text": "3. Contextualized Construct Representation (CCR) - Embeddings of questionnaires\nThis method uses the same large language model embeddings from the last one. Now though, rather than comparing comment embeddings to the average embedding of a dictionary, I will compare them to the average embedding of an experimentally validated psychological questionnaire. Here I will use the Hospital Anxiety And Depression Scale (HADS; Snaith, 2003), treating anxiety and depression as a single construct. Positively-coded items will be left as-is, and negatively coded ones will be negated (e.g. “i feel tense or wound up” -&gt; “i do not feel tense or wound up”).\nRemember that BERT is trained to predict upcoming words. So you can think of what I’m measuring here as: Are the the person who would write the kind of statement in the questionnaire and the person who would write this comment likely to say similar sorts of things afterward?\n\nhads_items &lt;- c(\n  \"i still enjoy the things i used to enjoy\",\n  \"i can laugh and see the funny side of things\",\n  \"i look forward with enjoyment to things\", \n  \"i feel cheerful\", \n  \"i can sit at ease and feel relaxed\",  \n  \"i can enjoy a good book or radio or tv program\",\n  \n  \"i do not feel tense or wound up\",\n  \"i do not feel as if i am slowed down\",\n  \"i do not get a sort of frightened feeling like butterflies in the stomach\",\n  \"i do not get a sort of frightened feeling as if something awful is about to happen\",\n  \"i have not lost interest in my appearance\",\n  \"i do not feel restless as I have to be on the move\",\n  \"worrying thoughts do not go through my mind\",\n  \"i do not get sudden feelings of panic\"\n  )\n\nhads_embedding &lt;- textEmbed(hads_items)\n\n# Mean Embedding\nhads_avg_embedding &lt;- hads_embedding$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\n\n# cosine similarity of each text to HADS average embedding\nd$hads_similarity &lt;- NA\nfor (row in 1:nrow(d_embeddings$texts[[1]])) {\n  doc_embedding &lt;- slice(d_embeddings$texts[[1]], row) %&gt;% \n    unlist(use.names = FALSE)\n  d$hads_similarity[row] &lt;- lsa::cosine(doc_embedding, hads_avg_embedding)\n  }"
  },
  {
    "objectID": "blog/text_comparison/index.html#ccr-corrected-for-questionaire-ness.",
    "href": "blog/text_comparison/index.html#ccr-corrected-for-questionaire-ness.",
    "title": "Comparing Four Methods of Sentiment Analysis",
    "section": "4. CCR, Corrected for Questionaire-ness.",
    "text": "4. CCR, Corrected for Questionaire-ness.\nOne possible problem with the last method is that there’s a difference between agreeing with statements in a questionnaire and actually writing like a questionnaire. How do I know that I’m measuring how anxious/depressed the comments are and not how questionnaire-y they are? This problem can be remedied with the magic of vector embeddings: I will reverse all of the statements in the questionnaire (just like I did for reverse-coded items before), add them to the original questionnaire, and get the average embedding of this anxiety- and depression-neutral questionnaire. Then I will subtract this embedding from the questionnaire average embedding I computed for the last method, yielding (hopefully) an embedding of pure depression and anxiety, with all the questionnaire-ness sucked out of it.\n\nhads_items_reversed &lt;- c(\n  \"i do not still enjoy the things i used to enjoy\",\n  \"i cannot laugh and see the funny side of things\",\n  \"i do not look forward with enjoyment to things\", \n  \"i do not feel cheerful\", \n  \"i cannot sit at ease and feel relaxed\",  \n  \"i cannot enjoy a good book or radio or tv program\",\n  \n  \"i feel tense or wound up\",\n  \"i feel as if i am slowed down\",\n  \"i get a sort of frightened feeling like butterflies in the stomach\",\n  \"i get a sort of frightened feeling as if something awful is about to happen\",\n  \"i have lost interest in my appearance\",\n  \"i feel restless as I have to be on the move\",\n  \"worrying thoughts go through my mind\",\n  \"i get sudden feelings of panic\"\n  )\n\nhads_embedding_reversed &lt;- textEmbed(hads_items_reversed)\n\n# Mean Embedding of Negated Questionnaire\nhads_avg_embedding_reversed &lt;- hads_embedding_reversed$texts[[1]] %&gt;% \n  summarise(across(Dim1_texts:Dim768_texts, mean)) %&gt;% \n  unlist(use.names=FALSE)\n\n# Mean Embedding of Neutralized Questionnaire\nquestionnaireness_avg_embedding &lt;- (hads_avg_embedding + hads_avg_embedding_reversed)/2\n\n# Subtract from initial\nhads_avg_embedding_purified &lt;- hads_avg_embedding - questionnaireness_avg_embedding\n\n# cosine similarity of each text to HADS average embedding\nd$hads_similarity_purified &lt;- NA\nfor (row in 1:nrow(d_embeddings$texts[[1]])) {\n  doc_embedding &lt;- slice(d_embeddings$texts[[1]], row) %&gt;% \n    unlist(use.names = FALSE)\n  d$hads_similarity_purified[row] &lt;- lsa::cosine(doc_embedding, hads_avg_embedding_purified)\n  }"
  },
  {
    "objectID": "blog/text_comparison/index.html#t-tests-and-effect-sizes",
    "href": "blog/text_comparison/index.html#t-tests-and-effect-sizes",
    "title": "Comparing Four Methods of Sentiment Analysis",
    "section": "T-tests and Effect Sizes",
    "text": "T-tests and Effect Sizes\nTime to see how each method did! Rather than running inference, I’ll stick with simple standardized effect sizes measuring the difference between “poisonous” and “non-poisonous” comments in terms of the four metrics computed above. Whichever method produces the largest effect size wins!\n\n# Sentiment Analysis\nt.test(d$pleasure_valence, d$label)\n\n\n    Welch Two Sample t-test\n\ndata:  d$pleasure_valence and d$label\nt = 108.32, df = 1252.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 4.974908 5.158443\nsample estimates:\nmean of x mean of y \n 5.566675  0.500000 \n\ncohens_d(pleasure_valence ~ label, data = d) # 0.11\n\nCohen's d |        95% CI\n-------------------------\n0.11      | [-0.02, 0.23]\n\n- Estimated using pooled SD.\n\nggplot(d, aes(label, pleasure_valence, group = label)) +\n  geom_boxplot() +\n  theme_bw()\n\n\n\n# DDR\nt.test(d$pleasure_similarity, d$label)\n\n\n    Welch Two Sample t-test\n\ndata:  d$pleasure_similarity and d$label\nt = 9.1938, df = 1021.5, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.1150409 0.1774743\nsample estimates:\nmean of x mean of y \n0.6462576 0.5000000 \n\ncohens_d(pleasure_similarity ~ label, data = d) # 0.49\n\nCohen's d |       95% CI\n------------------------\n0.49      | [0.37, 0.62]\n\n- Estimated using pooled SD.\n\nggplot(d, aes(label, pleasure_similarity, group = label)) +\n  geom_boxplot() +\n  theme_bw()\n\n\n\n# CCR\nt.test(d$hads_similarity, d$label)\n\n\n    Welch Two Sample t-test\n\ndata:  d$hads_similarity and d$label\nt = 15.626, df = 1017.3, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.2171395 0.2795070\nsample estimates:\nmean of x mean of y \n0.7483233 0.5000000 \n\ncohens_d(hads_similarity ~ label, data = d) # -0.10\n\nCohen's d |        95% CI\n-------------------------\n-0.10     | [-0.22, 0.02]\n\n- Estimated using pooled SD.\n\nggplot(d, aes(label, hads_similarity, group = label)) +\n  geom_boxplot() +\n  theme_bw()\n\n\n\n# CCR corrected\nt.test(d$hads_similarity_purified, d$label)\n\n\n    Welch Two Sample t-test\n\ndata:  d$hads_similarity_purified and d$label\nt = -25.085, df = 1004.6, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.4284731 -0.3663013\nsample estimates:\nmean of x mean of y \n0.1026128 0.5000000 \n\ncohens_d(hads_similarity_purified ~ label, data = d) # 0.43\n\nCohen's d |       95% CI\n------------------------\n0.43      | [0.31, 0.56]\n\n- Estimated using pooled SD.\n\nggplot(d, aes(label, hads_similarity_purified, group = label)) +\n  geom_boxplot() +\n  theme_bw()\n\n\n\n\nThe results are in! Traditional sentiment analysis worked all right (d = 0.11), but DDR was much better (d = 0.49). Plain CCR was actually worse than nothing - it identified poisonous comments as less depressed and anxious! This might be because it was measuring questionnaire-ness instead of depression/anxiety. It does make sense that non-poisonous texts sound a bit more like psychiatric questionnaires than poisonous ones do. Indeed, the corrected CCR did much better - almost as good as DDR.\nThis was not quite a fair match, since the dictionary used for DDR was much larger than the questionnaire used for CCR. With a longer depression/anxiety questionnaire, the corrected CCR method might surpass DDR.\nThe moral of the story: Always think twice about what exactly you are measuring. Even if you carefully designed the method to measure one thing, you just might be measuring something else entirely."
  },
  {
    "objectID": "blog/wordle/index.html",
    "href": "blog/wordle/index.html",
    "title": "Statistically Optimal Wordle",
    "section": "",
    "text": "DISCLAIMER: A few days after I posted this, my favorite math YouTuber, 3Blue1Brown, came out with his own treatment of optimal Wordle. His treatment is much better informed and more elegantly presented than mine.\nWordle is all the rage these days. If you’re reading this, you probably already know the rules of the game. You get six guesses at a hidden five-letter word randomly selected each day. Each guess must itself be a legitimate five-letter word, and as feedback, you learn whether each letter in your guess is 1. in the target word and in the correct location (green), 2. in the target word but not in the correct location (yellow), or 3. not in the target word at all (black).\nThis is my Wordle game from yesterday:\n\nAround the Internet, strategy articles abound with titles like “THE 20 BEST WORDLE STARTING WORDS ACCORDING TO SCIENCE”, “How to crack Wordle: 5-letter words to use first”, and “Best Wordle start words to help you win at Wordle”.\nPretty much all of these articles are thinking about letter frequency; the best starting word is the one that has all the highest frequency letters in the dictionary. This is a pretty tempting way to go. In fact, if Wordle only told you whether or not each letter was in the target word, it might be pretty close to optimal. The thing is, Wordle also tells you about placement. If a letter is green, you know it’s in that place. If it’s yellow, you know it’s not in that place. This is really valuable information and it would be a shame to throw it out. Unfortunately, it also makes the whole thing a lot more complicated. For example, I bet words with ts in the fifth place are better guesses than words with ts in the third place. Since more words in the dictionary have t in the fifth place, you’ll rule out a lot more if it’s not there, plus you’re more likely to hit the jackpot with a green tile.\nSo let’s start by looking at some letter frequencies in the Wordle dictionary of possible answers (as scraped from their website and represented below as “answer_dictionary_vec”). This time though, let’s pay attention to where the letters are in the word.\n\nlibrary(tidyverse)\nlibrary(tidytext)\n\n\nplaces &lt;- data.frame(letter = answer_dictionary_vec) %&gt;%\n  separate(letter, into = c(\"blank\", \"1\", \"2\", \"3\", \"4\", \"5\"), sep = \"\") %&gt;%\n  select(2:6) %&gt;%\n  pivot_longer(cols = 1:5, names_to = \"place\", values_to = \"letter\") %&gt;%\n  count(place, letter)\n\nplaces %&gt;%\n  group_by(place) %&gt;%\n  arrange(desc(n), .by_group = T) %&gt;%\n  top_n(10, n) %&gt;%\n  ungroup() %&gt;%\n  mutate(letter = factor(paste(letter, place, sep = \"__\"), levels = rev(paste(letter, place, sep = \"__\")))) %&gt;%\n  ggplot(aes(letter, n)) +\n    geom_bar(stat = \"identity\", show.legend = FALSE) +\n    facet_wrap(~place, scales = \"free\") +\n    coord_flip() +\n    scale_x_discrete(labels = function(x) gsub(\"__.+$\", \"\", x)) +\n    theme_minimal() +\n    labs(title = \"Letter Frequencies by Place\", x = NULL, y = \"Occurences\")\n\nThe top ten most common letters in each letter-place: \nThis is interesting. e is the most common (and therefore most useful in a guess) for the last two letters of the word. For the first letter though, we should prefer a consonant like s. s is almost never at the end because Wordle answers are never plurals (even though plurals are allowed in guesses). We could start developing a scoring system to find the words with the most common letters in their rightful places, but I have a better idea. As long as we’re allowing computers to help us with this guessing game, let’s try to get straight to the probabilities involved instead of stopping at frequencies and saying “good enough”.\nI went on a lot of long roadtrips as a kid, and the most popular game in our family car was “20 Questions”. One person thinks of a specific thing–a species of animal, a place, a household appliance–and everyone else has to ask yes-or-no questions to try to guess what that person is thinking of. If they can’t get it after 20 questions, the thinker wins. Anyone who has spent any time as a guesser in 20 Questions knows that you shouldn’t actually start thinking about what the specific thing is until there are only two possible options of what it could be. Before that, your goal is to narrow down the possibilities as much as possible.\nSince Wordle doesn’t allow you to guess categories–only individual five-letter words–the narrowing-down game is much trickier than in 20 Questions. On the other hand, Wordle gives you a whole lot more feedback than a yes-or-no answer. Wheras in 20 Questions the optimal question narrows the possibilities by a half, in Wordle we should be able to do much better than that.\nHow much better?\nHow good a guess is (i.e. how much does it narrow down the remaining possibilities) depends on what the actual answer is. For example, if your first guess is treat and the true answer is tread, you’re only left with one possibility. There is only one word that begins with trea-and does not end in t, and it’s tread. But it your first guess is treat and the true answer is boozy, all you know is that the word doesn’t include a, e, r, or t.\nHere’s a little function that, for a given guess, true answer, and list of currently possible answers, will give you a narrowed-down list of possible answers based on Wordle’s feedback on your guess.\n\n  # input guess (5 item vector), answer (5 item vector), and prior dictionary (list of 5 item vectors)\n\ndictionary_update &lt;- function(guess, answer, dictionary) {\n  for (n in 1:5) {\n    if (guess[n] %in% answer){\n      if (guess[n] == answer[n]) {\n        dictionary &lt;- dictionary[sapply(dictionary, \"[\", n) == guess[n]]\n      }else{\n        dictionary &lt;- dictionary[sapply(dictionary, \"[\", n) != guess[n]\n                                 & sapply(dictionary, function(x) any(guess[n] %in% x))]\n      }\n    }else{\n      dictionary &lt;- dictionary[sapply(dictionary, function(x) !any(guess[n] %in% x))]\n    }\n  }\n  dictionary\n}\n\nSo now we can give exact numbers. How many possibilities are left if your first guess is treat and the true answer is boozy?\n\ndictionary_update(guess = unlist(strsplit(\"treat\", \"\")), answer = unlist(strsplit(\"boozy\", \"\")), dictionary = answer_dictionary)\n\nThe output list is 332 words long, beginning with sissy, humph, and blush. That’s actually not too bad, given that Wordle’s list of possible answers has 2,315 words. Going from 2,315 possible answers before you start to 332 possible answers after the first guess is an 85.6% reduction!\nThis is all fine and lovely, but when we’re playing Wordle, we don’t know what the solution is. It could be boozy or tread or any of 2,313 others. So without knowing ahead of time what the solution is, how good of a guess is treat?\nIt’s time to look at some probability density plots.\n\n# Function to create a dataframe with how much the given guess would narrow the possibilities for each possible answer.\nguess_quality &lt;- function(guess) {\n  guess &lt;- unlist(strsplit(guess, \"\"))\n  distribution_table &lt;- data.frame(answer = rep(NA, length(answer_dictionary)),\n                                   posterior_dictionary_length = rep(NA, length(answer_dictionary)))\n  for (n in 1:length(answer_dictionary)) {\n    answer &lt;- answer_dictionary[[n]]\n    posterior_dictionary_length &lt;- length(dictionary_update(guess, answer, answer_dictionary))\n    distribution_table[n, 1] &lt;- paste(answer, collapse = \"\")\n    distribution_table[n, 2] &lt;- posterior_dictionary_length\n  }\n  distribution_table\n}\n\n# Let's graph it!\nlibrary(tidyverse)\n\nguess_quality(\"treat\") %&gt;%\n  mutate(\n    dictionary_reduction = 100-(100*(posterior_dictionary_length/length(answer_dictionary)))\n  ) %&gt;%\n  ggplot(aes(x = dictionary_reduction)) +\n    geom_density(size = 1) +\n    theme_classic() +\n    labs(title = \"Expected Reduction in Possible Answers for First Guess 'treat'\",\n         x = \"Percent Reduction\")\n\nSide note: The fact that all 2,315 possible answers are equally likely to be the true one makes this a whole lot easier. Some might say it means this endeavor is not technically “statistics”. That’s ok with me.\n\nThink of “density” here as the probability that, after your guess, you’ll get a particular percent decrease in remaining possibilities. On the whole, treat is clearly a good first guess, with the highest density of possible outcomes up toward 98% reduction, and the worst possible outcome around 85%. Either way, you’re ruling out the vast majority of possibilities. Presumably that’s because about 85% of words in the Wordle dictionary include the letters a, e, r, or t and are therefore ruled out if the correct answer turns out not to include any of them. In between the extremes though, it looks like you’re very unlikely to get an 87% reduction, much more likely to get a 92% reduction, and then less likely again to get a 95% reduction.\nThat curve is so irregular that it makes me curious to see some more.\nThis article recommends starting with adieu.\n\nHuh. Two peaks. Looks like you have about equal chances of adieu being either a super super helpful guess or a measly very helpful guess. Your worst outcome is less likely, and is still not bad at 87.5%.\nWhat about a really terrible first guess?\n\nThat about checks out. Your most likely single outcome here is that none of the letters of boozy are in the answer, in which case you’ve ruled out about half. But it’s a coin flip, because there are some possible answers that are super similar to boozy, and if one of those is the answer you’ve made a lot of progress with your risky first guess.\nI think the most interesting thing about these charts is the spread. Some guesses have a worst-case scenario that’s actually pretty good, but above that it’s a bit of a tossup. That’s what we saw with adieu. On the other hand we saw treat, which is on the whole worse than adieu (and has a worse worst-case scenario) but which carries a greater probability of getting really close, with possibility reduction in the 98% range. Ok, maybe these aren’t the best examples ever, but the point is: Are you playing for the best Wordle batting average or are you playing for a few really, spectacularly good games? In a few cases, it’s possible that there’s a high-stakes Wordle player would opt for the guess with the biggest mound all the way to the right of the graph, regardless of how long the tail is in the other direction.\nBefore moving on to real recommendations, I want to play with these graphs a little bit more. I claimed earlier that letter order matters a lot and that it’s a mistake to reduce the best-first-guess problem to a letter frequency contest. Let’s test that claim now.\n   \nrates, stare, resat, and taser all have the same five letters. We can see that their worst-case scenarios are all identical. This makes sense - the worst case scenario is that none of those letters are anywhere in the word. But the curves above that worst-case scenario are different. Which is the best? A conservative Wordle player might wish to know which has the highest average reduction. In other words, how much does each guess narrow our possibilities down, on average? Let’s see.\n\nrates: 96.83%\nstare: 96.92%\nresat: 96.72%\ntaser: 96.92%\n\nWe could even add couple more obscure anagrams:\n\naster: 96.63%\ntares: 96.91%\n\nOk fine, they’re all pretty similar. But there is a difference! I think e in the fifth place is a big bonus–especially combined with t, r, and s– since so many words end in -se, -te, or -re.\n\n\nWhat’s the Best Starting Word?\nNow we have our metric: What percentage of possible answers do we expect to rule out with this guess? All we have to do is apply this to every one of 12,972 legal guesses! (Yes, there are way more legal guesses than there are possible answers).\nThe problem here is that for each one of those 12,972 legal guesses, the computer will have to test out 2,315 possible answers to see how helpful the guess is in each instance. 12,972 * 2,315 = 30,030,180. Combined with my lack of CS skills and my five-year-old computer, this amounts to a more waiting than I’d like. To speed this up a bit, let’s only look at a random sample of 100 possible answers for each of the 12,972 guesses. It’ll introduce a bit of randomness into the results, but we can fix that later.\n\n# Faster guess assessment\nguess_quality_minimal_fast &lt;- function(guess, dictionary = answer_dictionary) {\n  guess &lt;- unlist(strsplit(guess, \"\"))\n  answer_sample &lt;- answer_dictionary[sample(1:length(answer_dictionary), 100)]\n  distribution &lt;- rep(NA, length(answer_sample))\n  for (n in 1:length(answer_sample)) {\n    answer &lt;- answer_sample[[n]]\n    distribution[n] &lt;- length(dictionary_update(guess, answer, dictionary))\n  }\n  mean(distribution)\n}\n\n# Make dataframe of expected answer narrowing\nexpected_reduction_table &lt;- data.frame(guess = rep(NA, length(guess_dictionary)),\n                                       expected_reduction = rep(NA, length(guess_dictionary)))\nfor (n in 1:length(guess_dictionary)) {\n  guess &lt;- paste(guess_dictionary[[n]], collapse = \"\")\n  expected_reduction_table$guess[n] &lt;- guess\n  expected_reduction_table$expected_reduction[n] &lt;- 100-(100*(guess_quality_minimal_fast/length(answer_dictionary)))\n}\n\nHere are the worst first guesses:\n\nexpected_reduction_table &lt;- expected_reduction_table %&gt;%\n  arrange(desc(expected_reduction))\n\ntail(expected_reduction_table)\n\n#&gt;       guess           expected_reduction\n#&gt; 12967 yuppy           61.77192\n#&gt; 12968 immix           60.55162\n#&gt; 12969 kudzu           60.28942\n#&gt; 12970 fuffy           57.36328\n#&gt; 12971 jugum           56.97495\n#&gt; 12972 qajaq           56.40821\n\nHere are the best first guesses:\n\nhead(expected_reduction_table)\n\n#&gt;   guess           expected_reduction\n#&gt; 1 roate           97.78920\n#&gt; 2 irate           97.73002\n#&gt; 3 soare           97.62505\n#&gt; 4 later           97.47257\n#&gt; 5 ariel           97.42851\n#&gt; 6 arise           97.42462\n\nBefore we discuss these results, though, let’s get rid of that randomness. I’m going to take just the top 200 results of our semi-randomized analysis and rerun it with the full set of possible answers. Unless the very best first guess somehow made it out of the top 200, this should give us the true, statistically optimal best guess.\n\nguess_dictionary_top &lt;- strsplit(head(expected_reduction_table$guess, n = 200), \"\")\n\nexpected_reductions_exact &lt;- data.frame(guess = rep(NA, 200),\n                                       expected_reduction = rep(NA, 200))\nfor (n in 1:200) {\n  guess &lt;- paste(guess_dictionary_top[[n]], collapse = \"\")\n  expected_reductions_exact$guess[n] &lt;- guess\n  distribution_table &lt;- guess_quality(guess) %&gt;%\n    mutate(dictionary_reduction = 100-(100*(posterior_dictionary_length/length(answer_dictionary))))\n  expected_reductions_exact$expected_reduction[n] &lt;- mean(distribution_table$dictionary_reduction)\n}\n\nexpected_reductions_exact &lt;- expected_reductions_exact %&gt;%\n  arrange(desc(expected_reduction))\nhead(expected_reductions_exact, 10)\n\n#&gt;     guess           expected_reduction\n#&gt;  1  roate           97.38987\n#&gt;  2  raise           97.36497\n#&gt;  3  raile           97.35072\n#&gt;  4  soare           97.30881\n#&gt;  5  arise           97.24727\n#&gt;  6  irate           97.24496\n#&gt;  7  orate           97.24014\n#&gt;  8  ariel           97.17980\n#&gt;  9  arose           97.14811\n#&gt;  10 raine           97.10341\n\nThere’s been quite a bit of shifting around, but roate is still on top! Roate, the cumulative net earnings after taxes available to common shareholders, adjusted for tax-affected amortization of intangibles, for the calendar quarters in each calendar year in a specified period of time divided by average shareholder’s tangible common equity! Notice also that roate’s expected reduction has gone down since our randomly-sampled round, from 97.79% to 97.39%. This makes sense and is similar to regression toward the mean–the first set had some randomness and the most extreme results of that randomness were selected to be on top. The second set had no randomness and therefore no boost on the top end.\nThere you have it. The statistically optimal Wordle starting word is roate.\n\n\nCmon, I wanna see the optimal Wordle playing Bot!\nOk fine. Here it is:\n\n    # This is the same as dictionary_update, but it prints the outcome of the guess with nice colors\ndictionary_update_printed &lt;- function(guess, answer, dictionary) {\n  for (n in 1:5) {\n    if (guess[n] %in% answer){\n      if (guess[n] == answer[n]) {\n        dictionary &lt;- dictionary[sapply(dictionary, \"[\", n) == guess[n]]\n        cat(paste0(\"\\033[48;5;46m\", guess[n]))\n      }else{\n        dictionary &lt;- dictionary[sapply(dictionary, \"[\", n) != guess[n]\n                                 & sapply(dictionary, function(x) any(guess[n] %in% x))]\n        cat(paste0(\"\\033[48;5;226m\", guess[n]))\n      }\n    }else{\n      dictionary &lt;- dictionary[sapply(dictionary, function(x) !any(guess[n] %in% x))]\n      cat(paste0(\"\\033[48;5;249m\", guess[n]))\n    }\n  }\n  dictionary\n}\n\n    # Returns average posterior dictionary length\nguess_quality_minimal &lt;- function(guess, dictionary = answer_dictionary) {\n  guess &lt;- unlist(strsplit(guess, \"\"))\n  distribution &lt;- rep(NA, length(dictionary))\n  for (n in 1:length(distribution)) {\n    answer &lt;- dictionary[[n]]\n    distribution[n] &lt;- length(dictionary_update(guess, answer, dictionary))\n  }\n  mean(distribution)\n}\n\n  # THE BOT\n\nplay &lt;- function(answer){\n  stopifnot(length(answer) == 5)\n  # Make the first guess (always \"roate\") and update the dictionary\n  dictionary &lt;- dictionary_update_printed(unlist(strsplit(\"roate\", \"\")), answer, answer_dictionary)\n  cat(\"\\n\")\n  # Loop for the other 5 guesses\n  for (try in 1:5) {\n    # If there's only one option left, guess it and end the game.\n    if (length(dictionary) == 1) {\n      cat(paste0(\"\\033[48;5;46m\", paste(dictionary[[1]], collapse = \"\")))\n      cat(\"\\n\")\n      break\n    }\n    # If there are two options left, guess the first. If that's the answer, end the game. If not, guess the next one and end the game.\n    if (length(dictionary) == 2) {\n      if (paste(dictionary[[1]], collapse = \"\") == paste(answer, collapse = \"\")){\n        dictionary &lt;- dictionary_update_printed(dictionary[[1]], answer, dictionary)\n        cat(\"\\n\")\n        break\n      }else{\n        dictionary &lt;- dictionary_update_printed(dictionary[[1]], answer, dictionary)\n        dictionary &lt;- dictionary_update_printed(dictionary[[1]], answer, dictionary)\n        cat(\"\\n\")\n        break\n      }\n    }\n    # Loop through possible guesses and evaluate how many possible answers we would expected to have after guessing them. Then pick the lowest one and guess it.\n    guess_remainders &lt;- rep(NA, length(guess_dictionary))\n    for (n in 1:length(guess_dictionary)) {\n      guess &lt;- paste(guess_dictionary[[n]], collapse = \"\")\n      guess_remainders[n] &lt;- guess_quality_minimal(guess, dictionary)\n    }\n    dictionary &lt;- dictionary_update_printed(guess_dictionary[[which.min(guess_remainders)]], answer, dictionary)\n    cat(\"\\n\")\n    if (paste(guess_dictionary[[which.min(guess_remainders)]], collapse = \"\") == paste(answer, collapse = \"\")) {\n      break\n    }\n  }\n}\n\nObviously the bot always starts by guessing “roate”. After each new guess, the expected helpfulness of each possible next guess is reassessed based on the shortened list of answer possibilities. Other than that, the whole process is the same one I’ve been working with all along; each guess is the one that is expected to narrow down the remaining possibilities the most.\nLet’s see it play a few games!\n           \nCan we learn any sage advice from this digital Wordle master? Just by looking at the games, it’s hard to say. I’m surprised by its willingness to use rarish letters in the second guess, but that doesn’t amount to a general statement about what kind of words make good second guesses. If we really want to learn some grandmaster-level Wordle skills, we’ll have to return to our probability density plots.\nHere’s one for our champion first guess:\n\nLooks pretty good, but still not clear how to squeeze strategy from this. The problem is that I have no idea how those peaks and valleys correspond to actual situations we might face in the Wordle battlefield. I don’t want to know what by what percentages I’m likely to narrow the search–I want to know what second guesses I’m likely to need after roate! It’s time to reveal the ugly monster lurking behind the density plot: the bar chart.\n\nYup. There are a few discrete outcomes that are more likely than the other ones. Well what are they? And what does the bot say to do in those situations?\nAs I’ve mentioned before, the tallest bar all the way to the left (i.e. the single most likely outcome) is definitely the case in which none of the letters in roate are in the target word - you turn up with all black squares. In that case, the computer says that the optimal next guess is slimy.\nHere are the top 5 most likely outcomes of the first guess roate, along with the optimal second guess:\n\n (8.4% chance). Optimal second guess: slimy\n (6.1% chance). Optimal second guess: bludy\n (5.4% chance). Optimal second guess: shunt\n (4.8% chance). Optimal second guess: lysin\n (4.6% chance). Optimal second guess: silen\n\nSo there you are. If you’ve ever looked longingly at a chess grandmaster and thought, “I wish I could just memorize all the best openings instead of playing like a normal person”, your wish has come true."
  },
  {
    "objectID": "blog/medieval_philosophers/index.html",
    "href": "blog/medieval_philosophers/index.html",
    "title": "Designing a Poster to Visualize the Timeline of Philosophers in the Islamic World",
    "section": "",
    "text": "One day when I was in 5th grade, I walked into my classroom to find a new poster on the wall. It was a visualization of the entirety of world history - I was transfixed. This is that poster below. You can buy it here. It’s greatest innovation is squishing geography (which is generally two-dimensional) onto the y-axis. A lot of detail is lost, but the gained ability to visualize history all at once on a wall poster makes it worth it.\n\nI quickly asked my parents if I could get one for myself. When I finally did, I set it up next to my bed - as I lay there every night, I would look at all the little details. I even found a few mistakes.\nFast forward a decade and a half. I’ve been really enjoying this podcast, The History of Philosophy Without Any Gaps. I’m on episode 290 at the moment. Learning about all of these philosophers is great, but they can be hard to keep track of. I need a timeline that keeps track of geography too.\nThis is exactly the kind of problem for which I developed the wormsplot package in R, inspired by that wonderful poster from my childhood. In this post, I’ll walk through the process of designing a wall poster to visualize the major philosophers of the Islamic world in the Middle Ages.\nCode and data for this project can be found here.\n\nStep 1: The Data\nI gathered this data myself from wherever I could find it - mostly Wikipedia and The Stanford Encyclopedia of Philosophy. It includes one row for each new stop along the way of a biography (starting with the birth date - usually an educated guess), plus one extra for the death date. I’m no historian, so don’t rely too heavily on the accuracy of these data. Actually even if I were a historian this would take a lot of guesswork - that’s how medieval history goes. Anyhow, here’s what it looks like: one column for name, one for city, one for date, and one for the philosopher’s religion (Muslim, Jewish, or Christian). For cities that no longer exist or are called something different now, I wrote the closest modern equivalent and made a note of it on the side.\n\n\n# A tibble: 10 × 5\n   name                      city     date religion  notes   \n   &lt;chr&gt;                     &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   \n 1 al-Kindī                  Kufa      801 Muslim    &lt;NA&gt;    \n 2 al-Kindī                  Baghdad   820 Muslim    &lt;NA&gt;    \n 3 al-Kindī                  Baghdad   871 Muslim    &lt;NA&gt;    \n 4 Ḥunayn ibn Isḥāq          Kufa      809 Christian al-Ḥīrah\n 5 Ḥunayn ibn Isḥāq          Baghdad   828 Christian &lt;NA&gt;    \n 6 Ḥunayn ibn Isḥāq          Baghdad   873 Christian &lt;NA&gt;    \n 7 Isḥāq ibn Ḥunayn          Baghdad   830 Christian &lt;NA&gt;    \n 8 Isḥāq ibn Ḥunayn          Baghdad   910 Christian &lt;NA&gt;    \n 9 Abū Bakr al-Rāzī (Rhazes) Rey       864 Muslim    &lt;NA&gt;    \n10 Abū Bakr al-Rāzī (Rhazes) Baghdad   880 Muslim    &lt;NA&gt;    \n\n\n\n\nStep 2: Fitting Geography Onto One Axis\nThe biggest challenge here is that y-axis. The first step: Find the latitude and longitude of each city in the data by calling the Open Street Map API. This worked very smoothly for everything except the city of Alexandria, which it identified at Alexandria, Virginia. After fixing that problem, I could plot a map of all the cities in the data:\n\n\n\n\n\nThere are many ways to reduce two-dimensional data to one dimension, and the best choice depends on the task at hand. If my places were grouped into distinct regions, I might consider t-SNE or UMAP. If they were generally aligned along some diagonal axis, I might use Principle Component Analysis. As it stands though, I know exactly how I want my y-axis to be organized: It should go East to West along the coast of North Africa and then West to East within Western Europe. This makes sense both geographically and historically: Since Andalusia (Muslim Spain) was the main point of contact between the Islamic world and Christian Europe, Southern France should be ‘farther’ from Tunisia than Spain.\nSo I split the cities into Europe and Non-Europe and lined them up by longitude accordingly. After a few manual adjustments (going through all of Turkey before moving South along the Mediterranean coast, and moving Northern France and London to the far end of the axis), I ended up with this ordering:\n\n\n\n\n\nThe final step was to make up for the fact that certain cities that are very close to each other in longitude are actually quite far away on the North-South axis. I achieved this by scaling the distance between each city on the axis by the true Euclidean distance between them. This stretches out certain parts of the axis disproportionately, but it means that adjacent locations are the right relative distances away from each other. With that, here are all the cities arranged along the new y-axis:\n\n\n\n\n\n\n\nStep 3: Layout\nWith the y-axis defined, it’s time for some graphic design. I originally toyed with a gradient background along the y-axis, but settled on dividing it up into larger regions. Here’s the resulting blank plot:\n\n\n\n\n\n\n\nStep 4: Plot!\nNow that we have a suitable background, all that remains to represent the data. This is done using the wormsplot function stat_worm(). Labels are added with the aid of the label_worms() function. The worms are colored by their religion: Muslims in green, Jews in blue, and Christians in red.\n\nI think it looks quite nice! Even without squinting at the individual names, big trends in the history of philosophy are immediately evident, like the Baghdad school starting with al-Kindī and running for about 150 years, or the explosion of philosophical activity in Andalusia in the 12th century.\nAt the moment, this is just a proof of concept. The labeling of the worms especially needs some work. If I were to produce a finished poster, I would also include a timeline of important events along the bottom, and a legend to explain the format. I might also want to simplify the data somewhat - a few of these figures moved around a lot in their lifetimes (Ibn `Arabi, I’m looking at you) and are making it a bit difficult to follow the lines."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "MS in Neurocognitive Experimental Psychology and Data Science, 2022-Present\nBen Gurion University of the Negev\nBA in Psychology and Biology, 2018-2021\nYeshiva University"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "MS in Neurocognitive Experimental Psychology and Data Science, 2022-Present\nBen Gurion University of the Negev\nBA in Psychology and Biology, 2018-2021\nYeshiva University"
  }
]