[
  {
    "objectID": "hire_your_clone/index.html",
    "href": "hire_your_clone/index.html",
    "title": "Do Employees Tend to Have the Same First Name as Their Bosses?",
    "section": "",
    "text": "I was talking to my dad on the phone a few weeks ago, and he brought up a question: Do employers have a bias against hiring employees who have the same first name as they do? Surely, bosses must try to protect their unique name in the same way that fashionable dressers might be annoyed when another party-goer shows up with an identical outfit to their own. And that’s not to mention the opportunity for confusion: How will members of the team distinguish them in conversation? For these reasons, my dad figured there must be some bias against same-name hiring.\nBut how can we check this empirically?\nThe first step was to find some data. The best I could find came from The Official Board, which maintains detailed corporate organizational charts for tens of thousands of companies worldwide. I downloaded data on 11,702 employee/boss pairs across 779 US companies. Here’s an example of the data after some cleaning:\n\n\n# A tibble: 6 × 53\n  ID     first_name boss_ID boss_first_name company_ID company             level\n  <chr>  <chr>      <chr>   <chr>           <chr>      <chr>               <chr>\n1 ryyj5m Monica     jp7nxq  Jeffrey         joy01      NBCUniversal Local  N-2  \n2 r27e5m Melayne    5vlnro  Jeffrey         lwr96      National Geographi… N-2  \n3 rwmv07 Alan       p4md4r  Jeffrey         xw4d0      JLL Hotels & Hospi… N-2  \n4 561pvo Kym        ez6744  Jeffrey         9lqn4      Sony Pictures Anim… N-2  \n5 z0452d Alba       6qv6r2  Jeffrey         4e0ww      Sony Pictures Dist… N-2  \n6 7ej1vn Nicola     09rz0w  Jeffrey         9lox0      Universal Music Gr… N-2  \n# ℹ 46 more variables: same_first_name <int>, gender <int>, boss_gender <int>,\n#   dyad_gender <chr>, last_name <chr>, EastAsian <dbl>, Japanese <dbl>,\n#   Indian <dbl>, African <dbl>, Muslim <dbl>, EastEuropean <dbl>,\n#   Jewish <dbl>, Hispanic <dbl>, Italian <dbl>, WestEuropean <dbl>,\n#   ethnicity <chr>, boss_last_name <chr>, boss_EastAsian <dbl>,\n#   boss_Japanese <dbl>, boss_Indian <dbl>, boss_African <dbl>,\n#   boss_Muslim <dbl>, boss_EastEuropean <dbl>, boss_Jewish <dbl>, …\n\n\nAt first glance, this seems easy: we’ll find names of bosses and their employees, check how often they are identical, and check that against the expected rate assuming no bias. If the real rate is much lower than could be expected with random pairings, we’ll conclude that the bias is real.\nThere are three problems with this approach: 1. Gender and ethnicity will confound the results: Male bosses might be more likely to hire male employees. Since names tend to be gendered, this will make them that much more likely to have the same same, even if there is no bias at all for the names themselves. The same goes for ethnicity: White bosses might be more likely to hire white employees, making them more likely to both have common white names. We need to account for these other biases before we can know anything about the one we’re interested in. 2."
  },
  {
    "objectID": "projects/wormsplot.html",
    "href": "projects/wormsplot.html",
    "title": "wormsplot",
    "section": "",
    "text": "A worms plot is an elegant way to visualize historical (or fictional!) characters as they move from place to place. Expanding on ggplot2, this package provides a new geom, geom_worm, which allows for an arbitrary number of moves within each worm, and takes intuitively structured data input. The package also includes a convenience function, wormsplot, for generating aesthetic plots with minimal effort."
  },
  {
    "objectID": "projects/wormsplot.html#installation",
    "href": "projects/wormsplot.html#installation",
    "title": "wormsplot",
    "section": "Installation",
    "text": "Installation\nYou can install the development version of wormsplot from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"rimonim/wormsplot\")"
  },
  {
    "objectID": "projects/wormsplot.html#example",
    "href": "projects/wormsplot.html#example",
    "title": "wormsplot",
    "section": "Example",
    "text": "Example\nHere is a plot of the lives of scientists who won the Nobel Prize for physics between 1901 and 1907.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\nlibrary(wormsplot)\n\ndata(nobel_physicists)\n\nnobel_physicists %>%\n  filter(name %in% head(unique(name), 10)) %>%\n  mutate(country = forcats::fct_drop(country)) %>%\n  wormsplot('year', 'country', 'name', worm.color = 'initial',\n            worm.args = list(linewidth = 5.1, lineend = 'round'), region.label.width = 22, label.args = list(size = 3.6)) +\n  labs(title = \"The Lives of Winners of the Nobel Prize in Physics 1901-1907\")\n\n\n\n\nPlots can also be built from scratch using geom_worm() or stat_worm():\n\nlibrary(colorspace)\nlibrary(ggborderline)\n\ndata <- data.frame(\n  x = c(5, 10, 25, 30, 15, 20, 25),\n  y = c(1, 2, 1.5, 1.5, 2.2, 1.2, 1.2),\n  person = c('Me', 'Me', 'Me', 'Me', 'You', 'You', 'You')\n  )\n\nggplot(data, aes(x, y, group = person, color = person, bordercolor = after_scale(darken(colour, .2)))) +\n  stat_worm(linewidth = 10, shorten_lines = 10, geom = 'borderline', lineend = 'round') +\n  theme_minimal() +\n  scale_y_continuous(limits = c(0, 3))"
  },
  {
    "objectID": "projects/paper_provenance.html",
    "href": "projects/paper_provenance.html",
    "title": "paper_provenance",
    "section": "",
    "text": "An interactive web app for visualizing the history of a field leading up to a paper.\nThe app is now deployed at https://rimonim-paper-provenance-paper-provenance-qo5yhn.streamlit.app"
  },
  {
    "objectID": "design.html",
    "href": "design.html",
    "title": "Louis Teitelbaum",
    "section": "",
    "text": "Art\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFliers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Louis Teitelbaum",
    "section": "",
    "text": "wormsplot\n\n\n\nR\n\n\nggplot2\n\n\nDataViz\n\n\n\n\n\n\n\nLouis Teitelbaum\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\npaper_provenance\n\n\n\nPython\n\n\nMeta-Science\n\n\nDataViz\n\n\n\n\n\n\n\nLouis Teitelbaum\n\n\nMar 28, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/police_exploratory/index.html",
    "href": "blog/police_exploratory/index.html",
    "title": "תכלית מוקד 100 להציל חיים",
    "section": "",
    "text": "פוסט זה הוא ניטוח אקספלורטיבי-ראשוני. באו איתי להרפתקה ללמוד מה שאפשר ללמוד על מוקד 100 ועמידתו ביעוד המוצהר שלו, להציל חיים!\nלפני שנתחיל לנתח נתונים ישראליים, באו נלמד כמה נתונים בסיסיים מהמערכת האמריקאית. הנתונים האלה ישמשו לנו כהשערות אפריוריות בניתוח הנתונים הישראליים. חשוב שתהיה לנו הבנה אפריורית מבוססת מכיוון שהנתונים הישראלים שנמצאים ברשותנו כרגע הם לא רק דלים מאוד אלא גם לעתים גם לא ברורים מצד משמעות המשתנים שבם. לעומת זאת, יש לנו מערך נתונים נקי וברור עם מידע מארבעה ערים אמריקאיות (ניו אורלינס, דלס, דטרויט, וצ’רלסטון) על סיווג האירועים, ייזום השיחות (מהמשטרה או מהציבור), תוצאת האירוע, והרבה יותר.\n\n# Data manipulation\n  library(tidyverse)\n# Graphics that can deal with Hebrew text\n  library(ragg)\n  locale(\"he\")\n\n## For the sake of reasonable priors, let's look at some patterns in American 911 data\n\n# American data from Charleston, Detroit, New Orleans, and Dallas \n# (sampled for faster computation - the dataset is 6,347,478 rows long):   \namerican <- read_csv(\"/Users/louisteitelbaum/Documents/all_calls.csv\")[sample(1:6347478, 20000), -(1:26)]\n\nכמה שאלות ראשוניות: - כמה אירועים נפתחו בעקבות קריאה למוקד מהציבור (לעומת זהויים עצמאיים של שוטרים)? - כמה שיחות הגיעו למוקד אך לא נפתח אירוע תגובה בעקבותם? בכמה מהם האירוע הסתיים בעקבות השיחה/לא היה צורך בתגובה/הטרדה וכו’? - כמה פניות למוקד התגלו כמקרי חירום? ממקרי החירום, כמה קיבלו מענה משטרטי? - האם זה נפוץ שיש שינוי בין זיהוי הראשון של סוג האירוע ודיווח הסופי? יש סוגי אירוע שבהם הסיכוי של סיווג מוטעה גבוה יותר?\nהגעת לחלק החופר, כאשר עדיין לא התמקמתי בנתונים. אם אתה לא רוצה ללכת איתי בהרפתקאה הזאת, אתה מוזמן לדלג עד הכותרת “סיכום הנתונים האמריקאים”.\nנתחיל בשאלת הייזום של השיחות.\n\n# What percentage of 911 calls are citizen-initiated?\namerican %>% \n  filter(city %in% c(\"Detroit\", \"NewOrleans\")) %>%\n  group_by(self_initiated) %>% \n  summarise(freq = n()/nrow(.))\n      #> No (Citizen-initiated):  0.443\n      #> Yes (Police-initiated):  0.328\n      #> Other:                   0.228\n\n44.3 אחוז של פניות הגיעו מהציבור בלי הצטרפות המשטרה. חשוב להזכיר שהנתונים האלה מגיעים משתי ערים גדולות (רק דטרויט וניו אורלינס מעניקים את המשתנה הזה). הייתי מנכש שהאחוז יהיה גבוה יותר במקומות פחות עירניים, איפה שיש פחות פעילות משטרתית באופן כללי. גם ראוי לציין שאני לא יודע מה המשמעות של “Other” בהקשר הזה. נראה לי שהכוונה למערכות אזעקות וכדומה, שיכולים ליזום אירוע משרתי בלי קשר גם לשוטרים וגם לאזרחים. רק לניו אורלינס יש את הקטגוריה הזאת. לדטרויט יש רק כן או לא ייזום פנימי.\nמה עם תוצאות השיחות?\n\namerican %>%\n  filter(is.na(disposition) == F, city != \"Detroit\") %>%\n  group_by(disposition) %>%\n  summarise(percent = 100*n()/nrow(.)) %>%\n  arrange(desc(percent)) %>%\n  ungroup() %>%\n  mutate(disposition = factor(disposition, levels = disposition)) %>%\n  ggplot(aes(disposition, percent, fill = disposition)) +\n    geom_bar(stat = \"identity\") +\n    guides(x = guide_axis(angle = 45)) +\n    scale_fill_brewer(palette = \"Spectral\") +\n    theme_minimal() +\n    labs(title = \"Outcome of 911 calls in New Orleans, Charleston, and Dallas\", x = \"\", y = \"Percent of Total Cases\") +\n    theme(plot.title = element_text(hjust = .5), legend.position = \"none\")\n\n\nאחלה תרשים! מה זה אומר? שאלה טובה - יש הרבה קטגוריות פה, ולא ברור מה הם בדיוק. נעבור אחד אחד ונתרגם קצת: - פעילות אכיפה (Enforcement Activity) 39.9 % : שליחת שוטרים לאתר בלא מעצר - יצירת דו”ח (Report Generated) 16.8 % : אני חושב שזה אומר שלא שלחו ניידת, אבל דיווחו על פשיעה או על פעילות חשודה - לא מבוסס (Unfounded) 15.7 % - פעילות ללא מעצר (Non-Arrest Activity) 10.8 % : אני לא בטוח מה ההבדל בין זה לבין “Enforcement Activity” - מבוטל/נסוג (Cancellation/Withdrawn) 9.67 % - חוזר (Repeat) 2.1 % : כלומר, משהו כבר התקשר בנוגע לאורוע הזה? אותו ב”א כבר התקשר? - מעצר (Arrest Issued) 1.9 % - נתינת אזהרה (Warning Issued) 1.3 % - נתינת דוח (Citation Issued) 0.9 %\n- תלונת שוא (False Complaint) 0.7 % - אחר (other) 0.2 %\nאוקיי, עדיין לא ברור לגמרי. למרבה המזל, מייצרי מערך הנתונים הסבירו את החשיבה שלהם פה. הם כותבים, “דלס השתשמה ב12 קטגוריות להתייחס לתוצאת השיחה. 80% סווגו כ”מבוטל”… עוד 18% הותאמו ל”מעצר”. אנחנו מאמינים שזה מהווה מוגבלות של הנתונים ולא תכונה ייחודית של הקהילה בדלס, מכיוון שפניות שזוהו כגרמים לתגובה מינורית כולם סווגו כ”מבוטל. בסגנון דומה בצד שני של הספקטרום, 60% של הערכים מניו אורלינס סווגו כ”פעילות דרושה ננקטה,” שהתאמנו ל”פעילות אכיפה”. אבל באמת, קטגוריה זאת יכולה להתאים לכל תגובה חמורה או קלה.”\n…ועוד הרבה יותר. לפי דבריהם, הבעיה היא ש”בתוך עיר מסויימת, הקטגוריות בהן נעשה שימוש לסוג פנייה מסוים לעתים קרובות משתנות משנה לשנה, וגם הגדרות משתנות ככל שעובר הזמן… לא מפתיע, כי הנתונים האלה מגיעים ממערכת שמתוכננת לנווט שוטרים לאירועים ולספק תמיכה כאשר הם חוקרים פנייה. בתוך המערכת הזאת, ערכים אינם מתוכננות להקל על ניתוח הנתונים אלא להבטיח נגישות למידע בשביל השוטרים המגיבים לפנייה”.\nהנה החלוקה לפי ערים:\n\nאז העבודה שלנו מסובכת יותר ממה שציפינו. אבל עדיין יש הרבה ללמוד. צריך לזכור שכרגע אנחנו מחפשים רק רקע ככלי כדי להבין את הנתונים הישראליים. אני אציג פה עוד כמה גרפים, ואז אשתדל לסכם את מה שיכלתי ללמוד מהם.\nאם לוקחים רק את הפניות שבוודאי לא מגיעות מהמשטרה, התמונת התוצאות נהיה פשוטה יותר (אשמה של צ’רלסטון, שמסבכת את כל עבודת הסיווג אבל אינה מדווחת על ייזום השיחות).\n\nרוב הפניות מסתיימות בשליחת שוטרים לאתר ללא מעצר. ביותר מ20% של מקרים, המוקדן מדווח על האירוע אבל אין פעילות משטרתית. בעוד 20%, הפנייה חסרה ביסוס בכלל.\nראינו את תוצאות הפניות למוקד. מה עם סוגי הפניות לכתחילה?\n\nהנה סיאטל, להשוואה:\n\nהקבוצה ההכי גדולה (כ35%) היא דברים מגוונים שלא ידעו איך למיין אותם. אחרי זה, “תלונות/תנאים סביבתיים” תופס יותר מ15%. רכוש (גניבה, ונדליזם) ועבירות תנועה שניהם מתקרבים ל10%. פשע אלים מהווה פחות מ5% אבל עדיין אינה זנוחה. בריאות הנפש מגיעה ל1 אחוז. שריפות ומקרי חירום רפואים הם זנוחים לגמרי. סדר הנתונים בסיאטל הוא דומה, רק בלי ההסתפקות בחלק הגדול ביותר.\nעכשיו נחזור לתוצאות האירועים - אולי אפשר להבין יותר על הגדרות הקטגוריות על בסיס סוגי האירועים שמתגלגלים אליהם. התרשים הבא מייצג רק את ניו אורלינס - דלס, דטרויט, וצ’רלסטון כולם חסרות או ייזום השיחות או תוצאות.\n\n# Maybe we can learn more from the overlap between call type and outcome\namerican %>%\n  mutate(call_type = str_wrap(call_type, width = 18)) %>%\n  filter(is.na(disposition) == F, \n         is.na(call_type) == F,\n         disposition != \"Unknown\", \n         self_initiated %in% c(\"No\", \"other\")) %>%\n  ggplot(aes(x = \"\", fill = disposition)) +\n    facet_wrap(~call_type, nrow = 4) +\n    geom_bar(position = \"fill\") +\n    scale_fill_brewer(name = \"Outcome\", palette = \"Paired\") +\n    coord_polar(\"y\") +\n    theme_void()\n\n\nפניות הקשורות לעבירות רכוש, עבירות תנועה, אלימות במשפחה, ופשע אלים מסתיימים הרבה פעמים בפעילות אכיפה, אבל יותר נפוץ שהם מתגלים כחסרי ביסוס, והרבה יותר נפוץ שהם מסתיימים (מבחינת המשטרה) ביצירת דו”ח. אני מניח שבהקשר הזה “יצירת דו”ח” אומר שהאירוע הסתיים כך שהמצב כבר לא היה דחוף, ולכן דיווחו על האירוע לצורך טיפול עתידי. מקרי חירום רפואי נראים דומים, רק בלי הסיכוי שהם לא מבוססים. כאן אבל, הייתי רוצה להאמין שב”יצירת דו”ח” הכוונה להתרעה בפני הגופים הרלוונטיים - אמבולנס וכו’. כנ”ל לגבי יצירת דו”חות בשריפות, אלא ששוטרים נוטים כן להגיע לשריפות. נעדרים, עבירות מין, חשד, ואזעקות כולם מחולקים בין פעילות אכיפה לחסר ביסוס. קריאות הקשורות לבריאות הנפש נוטים להענות עם “פעילות אכיפה”, אבל לפעמים רק ביצירת דו”ח.\nלוודא שמה שאנחנו רואים ניתן להכללה, נעיין רגע בנתונים המקבילים מסיאטל, שלא היה מוצג לעיל, ושהקטגוריות שלה הן קצת יותר מפורטות:\n\nseattle <- read_csv(\"/Users/louisteitelbaum/Downloads/Seattle.csv.zip\")[sample(1:4206691, 20000),]\nseattle %>%\n  mutate(call_type = str_wrap(call_type, width = 18)) %>%\n  filter(is.na(disposition) == F, \n         is.na(call_type) == F,\n         disposition != \"Unknown\", \n         self_initiated %in% c(\"No\", \"other\")) %>%\n  ggplot(aes(x = \"\", fill = disposition)) +\n  facet_wrap(~call_type, nrow = 4) +\n  geom_bar(position = \"fill\") +\n  coord_polar(\"y\") +\n  theme_void() +\n  labs(title = \"Incident Outcomes by Type in Seattle\") +\n  theme(plot.title = element_text(hjust = .5) , legend.title = element_blank())\n\n\nלעיניי, התמונה פה דומה למה שראינו בניו אורלינס. חידוש אחד שאפשר לזהות: במקום החלק הגדול של “לא מבוסס” שראינו מקודם (21.2% בניו אורלינס), פה יש מבוטל (6.6% סה”כ), תלונת שוא (3.6%), לא מבוסס (1.5%), והכי מפתיע - שגיאת מיקום (10.1%). ע”פ זה נראה שאחוז משמעותי של אירועים - ובמיוחד מקרי חירום רפואי - מסתיימים במצב בו המשטרה לא מצליח למקם את האירוע. אני לא יודע מה לעשות עם זה.\nשאלה אחרונה: רציתי לדעת אם זה נפוץ שזיהוי הראשוני של סוג האירוע מתגלה כשגוי. לענות על זה, נצטרך למצוא את הנתונים הגולמיים מסיאטל, שמדווחת על שני הזיהויים.\n\nseattle_raw <- read_csv(\"/Users/louisteitelbaum/Downloads/Call_Data_Seattle.csv\")[sample(1:4842110, 20000),]\n\nדי מהר אחרי שפתחתי את המערך הזה, ראיתי את הבעיה:\n\nseattle_raw %>%\n  group_by(`Initial Call Type`, `Final Call Type`) %>%\n  summarise() %>% \n  group_by(`Initial Call Type`) %>%\n  summarise(ntypes = n()) %>%\n  ggplot(aes(ntypes)) +\n    geom_histogram(binwidth = 1) + \n    labs(title = \"Number of Unique Final Call Types Per Initial Call Type\", \n         subtitle = \"Seattle\",\n         x = \"Number of Unique Final Call Types\",\n         y = \"Count of Initial Call Types\") +\n    theme_minimal()\n\nseattle_raw %>%\n  group_by(`Initial Call Type`, `Final Call Type`) %>%\n  summarise() %>% \n  group_by(`Final Call Type`) %>%\n  summarise(ntypes = n()) %>%\n  ggplot(aes(ntypes)) +\n  geom_histogram(binwidth = 1) + \n  labs(title = \"Number of Unique Initial Call Types Per Final Call Type\", \n       subtitle = \"Seattle\",\n       x = \"Number of Unique Initial Call Types\",\n       y = \"Count of Final Call Types\") +\n  theme_minimal()\n\n \nבמילים אחרות, פרדיגמות הסיווג בזיהוי ראשוני וזיהוי סופי הם שונים לגמרי. יש הרבה סוגים ראשוניים שמתחלקים לעשר ואפילו עשרים סוגים סופיים, ויש הרבה סוגים סופיים שכוללים אפילו שלושים סוגים ראשוניים. ניתן עוד לעקוב אחרי השאלה הזאת (למשל, עם cosine similarity למדוד את הדמיון הלשוני בין ראשוני לסופי), אבל אין לי את הזמן או את הרצון כרגע.\nאני מתחיל להרגיש יותר בבית בנתונים האלה. אפשר לסכם את מה למדנו.\n\nסיכום הנתונים האמריקאים\n\nאחוז משמעותי של אירועים במוקד מגיעים מזיהויים עצמאיים של שוטרים, אבל לא רוב המוחלט (ניו אורלינס - 20%, סיאטל - 41%, דטרויט - 55%)\nקרוב לחצי מהשיחות שמגיעות למוקד לא מובילות לפעילות משטרתית אקטיבית (סיאטל - 52%, ניו אורלינס - 45%). עם זאת, רוב מוחלט מהמקרים האלה נסגרים או בדיווח משטרתי (לפעמים לאמבולנס או לכבאות) או באי-יכולת לאתר את האירוע. אולי 5% מתגלים כתלונת שוא כלשהי. כ7% בוטלות תוך כדי השיחה.\nקשה לדעת כמה פניות אפשר להחשב כמקרי חירום, כאשר הרבה מהאירועים מסוג הזה כבר הסתיימו בזמן השיחה ולכן לא קיבלו מענה משטרתי. אפשר להגיד, אבל, שאירועים הקשורים לבריאות הנפש מייצגים כ3% של שיחות (רובם ב”א אבדני, חולה נפש מסתובב ברחוב באופן לא מסוכן, או מנת יתר). חירום רפואי מייצג פחוז מ1%.\nקשה לדעת כמה זהויים שגויים יש, אבל כן אפשר להגיד שיש זיהוי ראשוני ע”י המוקדן, וזיהוי סופי אחרי סיום האירוע.\nבכללי, כל הנתונים מהמוקד קיימים בראש ובראשונה לעזור לפעילות המוקד. כתוצאה מזאת, אין שום סטנדרטיזציה, וקשה לקבל שום תמונה רחבה שהיא על העניינים.\n\n\n\nהנתונים הישראליים\nנתונים על מוקד 100 מפה, פה, ופה. על אוכלוסייה מפה.\n\n# Incoming calls by region, city, day of week (Jan 1 - March 21, 2020)\nday.place.1 <- read_csv(\"/Users/louisteitelbaum/Documents/moked_janfebmar.csv\",\n                        locale = locale(date_names = \"he\", encoding = \"UTF-8\"))\n\n# Incoming calls by region, city, day of week (March 21 - April 30, 2020)\nday.place.2 <- read_csv(\"/Users/louisteitelbaum/Documents/moked_marapr.csv\",\n                        locale = locale(date_names = \"he\", encoding = \"UTF-8\"))\n\n# General Population Demographics\ndemographics <- read_csv(\"/Users/louisteitelbaum/Documents/demographics/demographics.csv\")\ndemographics <- demographics[-(1:11), c(2, 8, 11)]\nnames(demographics) <- c(\"ir\", \"pop\", \"arabs\")\ndemographics$pop[demographics$pop == \"-\"] <- 0\ndemographics$arabs[demographics$arabs == \"-\"] <- 0\ndemographics$pop <- as.numeric(gsub(\",\", \"\", demographics$pop))\ndemographics$arabs <- as.numeric(gsub(\",\", \"\", demographics$arabs))\n\n# Incidents (unclear what this means exactly) by city, type, subtype, year, and quarter (2015-2017)\ntype.place.year <- read_csv(\"/Users/louisteitelbaum/Documents/moked100.csv\")\ntype.place.year <- type.place.year[-(1:6), c(1:3, 5:6, 11, 16)]\nnames(type.place.year) <- c(\"ir\", \"type\", \"subtype\", \"total\", \"2015\", \"2016\", \"2017Q1-2\")\ntype.place.year <- type.place.year %>%\n  mutate(across(4:7, ~replace(., . == \"$\", \"1\"))) %>%\n  mutate(across(4:7, ~replace(., . == \"-\", \"0\"))) %>%\n  type_convert()\n\nאם רוצים לדעת על סכנת מוות, ובמיוחד במישור ההלכתי, יש לנו הזדמנות בישראל שלא היתה לנו בארה”ב: התנהגות הדתיים. אם בדרך כלל יש אחוז משעותי של פניות למוקד שהם אינם מהווים סכנת מוות (ולכן אסורים בשבת על פי הלכה כפי שהיא נהוגה), נוכל לצפות שמספר הפנות ירד באופן משמעותי בשבת, ובמיוחד במקומות אם אוכלוסייה דתית יותר.\nלצערנו, השנתון הסטטיסטי של משטרת ישראל לא מדווח על חלוקה של פניות לימות השבוע. הוא כן מחלק ככה את אירועי התגובה (“מענה לאירוע עקב קריאה שנתקבלה מהציבור או זיהוי עצמי של שוטר אשר פתח אירוע במוקד”). הנה הנתונים משנת 2020:\n\nקשה לי לראות מה קורה בטבלה כזאת. הנה אותו דבר בתרשים:\n\nweekdays <- tibble(type = c(\"תנועה\", \"איכות חיים\", \"אלימות\", \"חירום וסכנות לציבור\", \"בטחון\", \"פעילות משטרתית\", \"רכוש\", \"סדר ציבורי\", \"סמים ואלכוהול\", \"מין\", \"עבירות ברשת\",   \"היסטורי\", \"פח'ע\"),\n                   א = c(106065, 38568, 43801, 57382, 20330, 83383, 44352, 5345, 6002, 1577, 127, 18, 6276),\n                   ב = c(106998, 34000, 43729, 56949, 20217, 84631, 42905, 5681, 5903, 1478, 79, 13, 5245),\n                   ג = c(111376, 36151, 44429, 57119, 20393, 83880, 44268, 5730, 5821, 1449, 113, 17, 5096),\n                   ד = c(113958, 36685, 44582, 59439, 19669, 86445, 44976, 5272, 5994, 1467, 101, 22, 2031),\n                   ה = c(122195, 49062, 46911, 60205, 19652, 85625, 46262, 6653, 5740, 1506, 110, 23, 5132),\n                   ו = c(100986, 58084, 44784, 56144, 14522, 66167, 40251, 5955, 5099, 1265, 77, 12, 3158),\n                   שבת = c(61737, 52357, 38406, 47621, 12583, 58115, 28104, 7552, 4471, 1075, 59, 10, 2439))\nweekdays <- weekdays %>%\n  pivot_longer(2:8, names_to = \"day\", values_to = \"n\") %>%\n  mutate(day = factor(day, levels = c(\"שבת\", \"ו\", \"ה\", \"ד\", \"ג\", \"ב\", \"א\")))\noptions(scipen=10000)\n\nggplot(weekdays, aes(day, n, fill = type)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  theme(legend.title = element_blank()) +\n  labs(x = \"יום\",\n       y = \"מספר אירועים\")\n\n\nלא צריך כלים סטטיסטים לראות שמספר האירועים יורד בשבת. עדיין קצת קשה לראות את הפרופורציות. הנה השוואה בין פרופוציות הממוצעות של ימי השבוע לבין הפרופורציות של שבת:\n\nweekdays %>%\n  mutate(shabbat = recode(if_else(day == \"שבת\", \"Y\", \"N\"), \"Y\" = \"שבת\", \"N\" = \"ימות השבוע\")) %>%\n  group_by(shabbat, type) %>%\n  summarise(n = sum(n)) %>%\n  group_by(shabbat) %>%\n  summarise(type = type, prop = n/sum(n)) %>%\n  ggplot(aes(x = \"\", prop, fill = type)) +\n    geom_bar(stat = \"identity\", color = \"white\") +\n    facet_wrap(~shabbat) +\n    theme_minimal() +\n    theme(axis.text.y = element_blank(), axis.title = element_blank(), legend.title = element_blank())\n\n\nבשבת יש פרופורציה קטנה יחסית של אירועי תנועה. זה הגיוני. יש גם פרופורציה גדולה יחסית של אירועי איכות חיים. אפשר לנחש שזה מייצג את הבלגן והרעש של סופ”ש, מצורף עם כמות נמוכה של כל הסוגים האחרים. אף אחד מאלה לא מוכיחים שמספר האירועים הכללי בשבת הוא תוצאה של התנהגות הדתיים במדינה. סביר להניח שגם חילינום נוסעים פחות בשבת - אין עבודה - ולכן יש פחות אירועים במשטרה. גם יכול להיות שהמשטרה היא קצת יותר עצלנית בסופ”שים ולכן מטפלת בקצת פחות אירועים בכל הקטגוריות (תזכר שאנחנו מסתכלים רק על אירועי תגובה כרגע).\nלמרבה המזל, יש לנו נתונים על פניות למוקד מחולקים גם לימי השבוע וגם לערים. האם אפשר לראות שלערים עם אוכלוסייה דתית יותר יש הפרש גדול יותר בין הפניות בשבת לבין ימי החול? נסתכל על כמה ערים.\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# WRANGLE DATA (Incoming calls by region, city, day of week)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Variable names into English\nnames(day.place.1) <- c(\"mahoz\", \"hodesh\", \"ir\", \"yom\", \"n\") -> names(day.place.2)\n\n# Only cities for which we have info on all weekdays for each month\nday.place.1 <- day.place.1 %>%\n  group_by(mahoz, hodesh, ir) %>%\n  filter(n() == 7) %>%\n  ungroup()\n\nday.place.2 <- day.place.2 %>%\n  group_by(mahoz, hodesh, ir) %>%\n  filter(n() == 7) %>%\n  ungroup()\n\n# Combine day.place.1 and day.place.2\nday.place <- bind_rows(day.place.1, day.place.2)\nrm(list = c(\"day.place.1\", \"day.place.2\"))\n\n# Remove summing rows\nday.place <- day.place[day.place$mahoz != 'סה\"כ' & day.place$mahoz != \"Total\", ]\nday.place <- day.place[day.place$hodesh != \"Total\" & day.place$ir != \"Total\" & day.place$yom != \"Total\", ]\n\n# Rename + Factorize days of the week\nday.place <- day.place %>%\n  mutate(yom = factor(yom, \n                       levels = c(unique(day.place$yom)[1], \n                                  unique(day.place$yom)[6], \n                                  unique(day.place$yom)[7], \n                                  unique(day.place$yom)[2],\n                                  unique(day.place$yom)[5], \n                                  unique(day.place$yom)[4],\n                                  unique(day.place$yom)[3]), \n                       labels = c(\"שבת\", \"ו\", \"ה\", \"ד\", \"ג\", \"ב\", \"א\")))\n\n# One case per weekday per city\nday.place <- day.place %>%\n  group_by(mahoz, ir, yom) %>%\n  summarise(n = sum(n)) %>%\n  ungroup()\n\nday.place <- day.place[day.place$ir != \"לא ידוע\", ]\n\n# Add variables to see proportion of calls coming in on each day\nday.place <- day.place %>%\n  left_join(demographics) %>%\n  group_by(ir) %>%\n  mutate(daypercent = 100*n/sum(n),\n         callspercap = n/pop) %>%\n  ungroup()\n  \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# VISUALIZE \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nday.place[day.place$ir %in% unique(day.place$ir)[sample(1:56, 25)], ] %>%\n  ggplot(aes(yom, daypercent, fill = (yom == \"שבת\") )) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ir) +\n  theme(legend.position = \"none\", \n        plot.title = element_text(hjust = 0.5)) +\n  labs(x = \"יום\",\n       y = \"אחוז פניות\",\n       title = \"2020 מינואר לאפריל 100 פניות למוקד \")\n\n\nאני לא רואה שום נטייה פה. אולי אני פשות לא מכיר כמה דתיים יש בכל עיר. אם נדע את גודל האוכלוסייה הדתית בכל עיר (בייחס לאוכלוססיה הככלית כמובן) נוכל לבנות מודל סטטיסטי ולראות אם אוכלוסייה דתית יותר גורמת להנמחת הפניות למוקד בשבת. בליתי הרבה זמן בחיפוש הנתונים האלה. מפקד האוכלוסין סופר כמה יהודים יש בכל עיר, אבל לא כמה דתיים. מרכז המחקר פיו מונה את הדתיים למיניהם אבל מחלק רק למחוזות, לא ערים. נצטרך למצוא proxy - משתנה שמתואם מספיק ליעד. באתר ‘כיפה’ יש רשימות של כל בתי הכנסת וכל המקוואות בארץ, לפי ערים. אלך על זה.\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# WEB SCRAPING\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Number of synagogues by city in Israel\nshuls_link <- url(\"https://www.kipa.co.il/%D7%91%D7%AA%D7%99-%D7%9B%D7%A0%D7%A1%D7%AA/%D7%91%D7%AA%D7%99-%D7%9B%D7%A0%D7%A1%D7%AA-%D7%9C%D7%A4%D7%99-%D7%A2%D7%99%D7%A8/\", \"rb\")\nshuls_page <- read_html(shuls_link)\nshul_cities <- shuls_page %>% html_elements(\".mikve-list a\") %>% html_text()\nshul_city_links <- shuls_page %>% html_elements(\".mikve-list a\") %>% html_attr(\"href\")\n\nget_shuls <- function(city_link){\n  city_page <- read_html(city_link)\n  nshuls <- city_page %>% html_elements(\"thead+ tbody .clickable-row td:nth-child(1)\") %>% length()\n  npages <- city_page %>% html_elements(\".pager a\") %>% html_text()\n  npages <- max(na.omit(as.numeric(head(npages, -1))))\n  if(npages > 0){\n    last_page <- read_html(paste(city_link, \"page/\", npages, \"/\", sep = \"\"))\n    nshuls_last <- last_page %>% html_elements(\"thead+ tbody .clickable-row td:nth-child(1)\") %>% length()\n    nshuls <- nshuls + 20*(npages-2) + nshuls_last\n  }\n  nshuls\n}\n    # I tried this with sapply and got an error 410 somewhere along the line. This loop is my way of sidestepping that error.\nshuls <- tibble(ir = shul_cities,\n                shuls = rep(NA, length(shul_city_links)))\nfor (city in 1:length(shul_cities)) {\n  try(\n    shuls$shuls[city] <- get_shuls(shul_city_links[city])\n  )\n}\n\n# Number of mikvaot by city in Israel\nmikvaot_link <- \"https://www.kipa.co.il/%D7%9E%D7%A7%D7%95%D7%95%D7%90%D7%95%D7%AA/%D7%9E%D7%A7%D7%95%D7%95%D7%90%D7%95%D7%AA-%D7%9C%D7%A4%D7%99-%D7%A2%D7%99%D7%A8/\"\nmikvaot_page <- read_html(mikvaot_link)\nmikva_cities <- mikvaot_page %>% html_elements(\".synagog a\") %>% html_text()\nmikva_city_links <- mikvaot_page %>% html_elements(\".synagog a\") %>% html_attr(\"href\")\n\nget_mikvaot <- function(city_link){\n  city_page <- read_html(city_link)\n  nmikvaot <- city_page %>% html_elements(\"td:nth-child(1)\") %>% length()\n  npages <- city_page %>% html_elements(\".pager a\") %>% html_text()\n  npages <- max(na.omit(as.numeric(head(npages, -1))))\n  if(npages > 0){\n    last_page <- read_html(paste(city_link, \"page/\", npages, \"/\", sep = \"\"))\n    nmikvaot_last <- last_page %>% html_elements(\"thead+ tbody td:nth-child(1)\") %>% length()\n    nmikvaot <- nmikvaot + 20*(npages-2) + nmikvaot_last\n  }\n  nmikvaot\n}\nmikvaot <- tibble(ir = mikva_cities,\n                  mikvaot = as.vector(sapply(mikva_city_links, FUN = get_mikvaot)))\n\nלפני שאסמוך על המקוואות ובתי הכנסת, צריך לוודא שהנתונים נראים איך שהיינו מצפים. הם מתואמים אחד אל השני לפחות?\n\nshuls %>%\n  left_join(mikvaot) %>%\n  left_join(demographics) %>%\n  mutate(\n    shulspercap = shuls/pop,\n    mikvaotpercap = mikvaot/pop\n  ) %>% \n  summarise(cor = cor(shulspercap, mikvaotpercap, use = \"pairwise.complete.obs\"))\n\nכן. מספר בתי הכנסת בעיר מתואם עם מספר המקוואות (שניהם מחולקים לאוכלוסייה הכללית) r=.64. זה הגיוני - יותר בתי כנסת לאדם, יותר מקוואות לאדם. אפשר גם לראות את מספר בתי הכנסת מול אוכלוסייה הכללית של העיר:\n\nshulspopmod <- lm(shuls~pop, data = (shuls%>%left_join(demographics)%>%filter(ir != \"ירושלים\")))\n\nshuls %>%\n  left_join(demographics) %>%\n  filter(ir != \"ירושלים\", is.na(pop) == F, is.na(shuls) == F) %>%\n  mutate(shulspopresid = rstandard(shulspopmod)) %>%\n  ggplot(aes(pop, shuls)) +\n    geom_point(aes(color = (shulspopresid > 0))) +\n    geom_smooth(method = \"lm\", color = \"black\", alpha = 0, size = 1) +\n    theme_minimal() + \n    theme(legend.position = \"none\") +\n    scale_x_continuous(breaks = c(100000, 200000), labels = c(\"100,000\", \"200,000\")) + \n    labs(x = \"אוכלוסייה\",\n         y = \"מספר בתי כנסת בעיר\")\n\n\nאפשר להניח שערים שיש להם יותר בתי כנסת ביחס לאוכלוסייה שלהם (צבועים בתכלת) הם אלה שיש להם קהילה דתית מובהקת יותר. אפשר לראות כמה ערים בחלק העליון של הגרף - הם כדוגמת צפת, שיש לה הרבה יותר בתי כנסת מאשר מגיע לה. הערים האדומות בחלק התחתון הם היותר חילוניים או ערביים.\nעכשיו אפשר לשאול את השאלה: האם אוכלוסייה דתית יותר גורמת להנמחת הפניות למוקד בשבת?\n\n# Does more religious Jewish population predict a lower proportion of 100 calls on shabbat?\nrequire(MASS)\n\nmikvaotpopmod <- lm(mikvaot~pop, data = (mikvaot%>%left_join(demographics)%>%filter(ir != \"ירושלים\")))\nmikvaot <- mikvaot %>% left_join(demographics) %>% filter(ir != \"ירושלים\", is.na(pop)==F, is.na(mikvaot)==F) %>% mutate(mikvaotpopresid = rstandard(mikvaotpopmod))\n\nshab.place <- shuls %>% \n  left_join(demographics) %>%\n  filter(ir != \"ירושלים\", is.na(pop) == F, is.na(shuls) == F) %>%\n  mutate(\n    shulspopresid = rstandard(shulspopmod)\n  ) %>%\n  left_join(mikvaot) %>%\n  right_join(day.place) %>%\n  filter(pop > 1000) %>%\n  filter(yom == \"שבת\")\n\n\nmikvamod <- lm(daypercent~mikvaotpopresid, data = shab.place)\nvisualize(mikvamod)\nsummary(mikvamod)\nrobustmikvamod <- rlm(daypercent~mikvaotpopresid, data = shab.place)\nsummary(robustmikvamod)\ncompare.fits(daypercent~mikvaotpopresid, data = shab.place, mikvamod, robustmikvamod)\n\nshulmod <- lm(daypercent~shulspopresid, data = shab.place)\nvisualize(shulmod)\nsummary(shulmod)\nrobustshulmod <- rlm(daypercent~shulspopresid, data = shab.place)\nsummary(robustshulmod)\ncompare.fits(daypercent~shulspopresid, data = shab.place, shulmod, robustshulmod)\n\nבלי להכנס לכל הטכניקה… התשובה היא לא. לא רואים שום קשר לינארי בין אוכלוסייה דתית לפניות למוקד בשבת. אני עוד לא יודע מה לעשות עם זה. אכן, היו נתונים לכל ימי השבוע רק ב42 ערים, וגם אז בהרבה מהם לא היו נתונים לכל אורך הזמן בין ינואר לאפריל 2020. אולי זאת הבעיה? מה שגם מוסיף קושי זה שהנתונים אינם מחולקים לסוגי האירוע - רק לימים. לו היה לי מערך נתונים מחולק גם לימים וגם לערים, אולי הייתי מוצא משהו.\nמתוך ייאוש מוחלט, אני אנסה עם דיסקרטיזציה לשני חלקים - התכלת והאדום שאינו לעיל על גרף האוכלוסייה ומספר בתי הכנסת."
  },
  {
    "objectID": "blog/wordle/index.html",
    "href": "blog/wordle/index.html",
    "title": "Statistically Optimal Wordle",
    "section": "",
    "text": "DISCLAIMER: A few days after I posted this, my favorite math YouTuber, 3Blue1Brown, came out with his own treatment of optimal Wordle. His treatment is much better informed and more elegantly presented than mine.\nWordle is all the rage these days. If you’re reading this, you probably already know the rules of the game. You get six guesses at a hidden five-letter word randomly selected each day. Each guess must itself be a legitimate five-letter word, and as feedback, you learn whether each letter in your guess is 1. in the target word and in the correct location (green), 2. in the target word but not in the correct location (yellow), or 3. not in the target word at all (black).\nThis is my Wordle game from yesterday:\n\nAround the Internet, strategy articles abound with titles like “THE 20 BEST WORDLE STARTING WORDS ACCORDING TO SCIENCE”, “How to crack Wordle: 5-letter words to use first”, and “Best Wordle start words to help you win at Wordle”.\nPretty much all of these articles are thinking about letter frequency; the best starting word is the one that has all the highest frequency letters in the dictionary. This is a pretty tempting way to go. In fact, if Wordle only told you whether or not each letter was in the target word, it might be pretty close to optimal. The thing is, Wordle also tells you about placement. If a letter is green, you know it’s in that place. If it’s yellow, you know it’s not in that place. This is really valuable information and it would be a shame to throw it out. Unfortunately, it also makes the whole thing a lot more complicated. For example, I bet words with ts in the fifth place are better guesses than words with ts in the third place. Since more words in the dictionary have t in the fifth place, you’ll rule out a lot more if it’s not there, plus you’re more likely to hit the jackpot with a green tile.\nSo let’s start by looking at some letter frequencies in the Wordle dictionary of possible answers (as scraped from their website and represented below as “answer_dictionary_vec”). This time though, let’s pay attention to where the letters are in the word.\n\nlibrary(tidyverse)\nlibrary(tidytext)\n\n\nplaces <- data.frame(letter = answer_dictionary_vec) %>%\n  separate(letter, into = c(\"blank\", \"1\", \"2\", \"3\", \"4\", \"5\"), sep = \"\") %>%\n  select(2:6) %>%\n  pivot_longer(cols = 1:5, names_to = \"place\", values_to = \"letter\") %>%\n  count(place, letter)\n\nplaces %>%\n  group_by(place) %>%\n  arrange(desc(n), .by_group = T) %>%\n  top_n(10, n) %>%\n  ungroup() %>%\n  mutate(letter = factor(paste(letter, place, sep = \"__\"), levels = rev(paste(letter, place, sep = \"__\")))) %>%\n  ggplot(aes(letter, n)) +\n    geom_bar(stat = \"identity\", show.legend = FALSE) +\n    facet_wrap(~place, scales = \"free\") +\n    coord_flip() +\n    scale_x_discrete(labels = function(x) gsub(\"__.+$\", \"\", x)) +\n    theme_minimal() +\n    labs(title = \"Letter Frequencies by Place\", x = NULL, y = \"Occurences\")\n\nThe top ten most common letters in each letter-place: \nThis is interesting. e is the most common (and therefore most useful in a guess) for the last two letters of the word. For the first letter though, we should prefer a consonant like s. s is almost never at the end because Wordle answers are never plurals (even though plurals are allowed in guesses). We could start developing a scoring system to find the words with the most common letters in their rightful places, but I have a better idea. As long as we’re allowing computers to help us with this guessing game, let’s try to get straight to the probabilities involved instead of stopping at frequencies and saying “good enough”.\nI went on a lot of long roadtrips as a kid, and the most popular game in our family car was “20 Questions”. One person thinks of a specific thing–a species of animal, a place, a household appliance–and everyone else has to ask yes-or-no questions to try to guess what that person is thinking of. If they can’t get it after 20 questions, the thinker wins. Anyone who has spent any time as a guesser in 20 Questions knows that you shouldn’t actually start thinking about what the specific thing is until there are only two possible options of what it could be. Before that, your goal is to narrow down the possibilities as much as possible.\nSince Wordle doesn’t allow you to guess categories–only individual five-letter words–the narrowing-down game is much trickier than in 20 Questions. On the other hand, Wordle gives you a whole lot more feedback than a yes-or-no answer. Wheras in 20 Questions the optimal question narrows the possibilities by a half, in Wordle we should be able to do much better than that.\nHow much better?\nHow good a guess is (i.e. how much does it narrow down the remaining possibilities) depends on what the actual answer is. For example, if your first guess is treat and the true answer is tread, you’re only left with one possibility. There is only one word that begins with trea-and does not end in t, and it’s tread. But it your first guess is treat and the true answer is boozy, all you know is that the word doesn’t include a, e, r, or t.\nHere’s a little function that, for a given guess, true answer, and list of currently possible answers, will give you a narrowed-down list of possible answers based on Wordle’s feedback on your guess.\n\n  # input guess (5 item vector), answer (5 item vector), and prior dictionary (list of 5 item vectors)\n\ndictionary_update <- function(guess, answer, dictionary) {\n  for (n in 1:5) {\n    if (guess[n] %in% answer){\n      if (guess[n] == answer[n]) {\n        dictionary <- dictionary[sapply(dictionary, \"[\", n) == guess[n]]\n      }else{\n        dictionary <- dictionary[sapply(dictionary, \"[\", n) != guess[n]\n                                 & sapply(dictionary, function(x) any(guess[n] %in% x))]\n      }\n    }else{\n      dictionary <- dictionary[sapply(dictionary, function(x) !any(guess[n] %in% x))]\n    }\n  }\n  dictionary\n}\n\nSo now we can give exact numbers. How many possibilities are left if your first guess is treat and the true answer is boozy?\n\ndictionary_update(guess = unlist(strsplit(\"treat\", \"\")), answer = unlist(strsplit(\"boozy\", \"\")), dictionary = answer_dictionary)\n\nThe output list is 332 words long, beginning with sissy, humph, and blush. That’s actually not too bad, given that Wordle’s list of possible answers has 2,315 words. Going from 2,315 possible answers before you start to 332 possible answers after the first guess is an 85.6% reduction!\nThis is all fine and lovely, but when we’re playing Wordle, we don’t know what the solution is. It could be boozy or tread or any of 2,313 others. So without knowing ahead of time what the solution is, how good of a guess is treat?\nIt’s time to look at some probability density plots.\n\n# Function to create a dataframe with how much the given guess would narrow the possibilities for each possible answer.\nguess_quality <- function(guess) {\n  guess <- unlist(strsplit(guess, \"\"))\n  distribution_table <- data.frame(answer = rep(NA, length(answer_dictionary)),\n                                   posterior_dictionary_length = rep(NA, length(answer_dictionary)))\n  for (n in 1:length(answer_dictionary)) {\n    answer <- answer_dictionary[[n]]\n    posterior_dictionary_length <- length(dictionary_update(guess, answer, answer_dictionary))\n    distribution_table[n, 1] <- paste(answer, collapse = \"\")\n    distribution_table[n, 2] <- posterior_dictionary_length\n  }\n  distribution_table\n}\n\n# Let's graph it!\nlibrary(tidyverse)\n\nguess_quality(\"treat\") %>%\n  mutate(\n    dictionary_reduction = 100-(100*(posterior_dictionary_length/length(answer_dictionary)))\n  ) %>%\n  ggplot(aes(x = dictionary_reduction)) +\n    geom_density(size = 1) +\n    theme_classic() +\n    labs(title = \"Expected Reduction in Possible Answers for First Guess 'treat'\",\n         x = \"Percent Reduction\")\n\nSide note: The fact that all 2,315 possible answers are equally likely to be the true one makes this a whole lot easier. Some might say it means this endeavor is not technically “statistics”. That’s ok with me.\n\nThink of “density” here as the probability that, after your guess, you’ll get a particular percent decrease in remaining possibilities. On the whole, treat is clearly a good first guess, with the highest density of possible outcomes up toward 98% reduction, and the worst possible outcome around 85%. Either way, you’re ruling out the vast majority of possibilities. Presumably that’s because about 85% of words in the Wordle dictionary include the letters a, e, r, or t and are therefore ruled out if the correct answer turns out not to include any of them. In between the extremes though, it looks like you’re very unlikely to get an 87% reduction, much more likely to get a 92% reduction, and then less likely again to get a 95% reduction.\nThat curve is so irregular that it makes me curious to see some more.\nThis article recommends starting with adieu.\n\nHuh. Two peaks. Looks like you have about equal chances of adieu being either a super super helpful guess or a measly very helpful guess. Your worst outcome is less likely, and is still not bad at 87.5%.\nWhat about a really terrible first guess?\n\nThat about checks out. Your most likely single outcome here is that none of the letters of boozy are in the answer, in which case you’ve ruled out about half. But it’s a coin flip, because there are some possible answers that are super similar to boozy, and if one of those is the answer you’ve made a lot of progress with your risky first guess.\nI think the most interesting thing about these charts is the spread. Some guesses have a worst-case scenario that’s actually pretty good, but above that it’s a bit of a tossup. That’s what we saw with adieu. On the other hand we saw treat, which is on the whole worse than adieu (and has a worse worst-case scenario) but which carries a greater probability of getting really close, with possibility reduction in the 98% range. Ok, maybe these aren’t the best examples ever, but the point is: Are you playing for the best Wordle batting average or are you playing for a few really, spectacularly good games? In a few cases, it’s possible that there’s a high-stakes Wordle player would opt for the guess with the biggest mound all the way to the right of the graph, regardless of how long the tail is in the other direction.\nBefore moving on to real recommendations, I want to play with these graphs a little bit more. I claimed earlier that letter order matters a lot and that it’s a mistake to reduce the best-first-guess problem to a letter frequency contest. Let’s test that claim now.\n   \nrates, stare, resat, and taser all have the same five letters. We can see that their worst-case scenarios are all identical. This makes sense - the worst case scenario is that none of those letters are anywhere in the word. But the curves above that worst-case scenario are different. Which is the best? A conservative Wordle player might wish to know which has the highest average reduction. In other words, how much does each guess narrow our possibilities down, on average? Let’s see.\n\nrates: 96.83%\nstare: 96.92%\nresat: 96.72%\ntaser: 96.92%\n\nWe could even add couple more obscure anagrams:\n\naster: 96.63%\ntares: 96.91%\n\nOk fine, they’re all pretty similar. But there is a difference! I think e in the fifth place is a big bonus–especially combined with t, r, and s– since so many words end in -se, -te, or -re.\n\n\nWhat’s the Best Starting Word?\nNow we have our metric: What percentage of possible answers do we expect to rule out with this guess? All we have to do is apply this to every one of 12,972 legal guesses! (Yes, there are way more legal guesses than there are possible answers).\nThe problem here is that for each one of those 12,972 legal guesses, the computer will have to test out 2,315 possible answers to see how helpful the guess is in each instance. 12,972 * 2,315 = 30,030,180. Combined with my lack of CS skills and my five-year-old computer, this amounts to a more waiting than I’d like. To speed this up a bit, let’s only look at a random sample of 100 possible answers for each of the 12,972 guesses. It’ll introduce a bit of randomness into the results, but we can fix that later.\n\n# Faster guess assessment\nguess_quality_minimal_fast <- function(guess, dictionary = answer_dictionary) {\n  guess <- unlist(strsplit(guess, \"\"))\n  answer_sample <- answer_dictionary[sample(1:length(answer_dictionary), 100)]\n  distribution <- rep(NA, length(answer_sample))\n  for (n in 1:length(answer_sample)) {\n    answer <- answer_sample[[n]]\n    distribution[n] <- length(dictionary_update(guess, answer, dictionary))\n  }\n  mean(distribution)\n}\n\n# Make dataframe of expected answer narrowing\nexpected_reduction_table <- data.frame(guess = rep(NA, length(guess_dictionary)),\n                                       expected_reduction = rep(NA, length(guess_dictionary)))\nfor (n in 1:length(guess_dictionary)) {\n  guess <- paste(guess_dictionary[[n]], collapse = \"\")\n  expected_reduction_table$guess[n] <- guess\n  expected_reduction_table$expected_reduction[n] <- 100-(100*(guess_quality_minimal_fast/length(answer_dictionary)))\n}\n\nHere are the worst first guesses:\n\nexpected_reduction_table <- expected_reduction_table %>%\n  arrange(desc(expected_reduction))\n\ntail(expected_reduction_table)\n\n#>       guess           expected_reduction\n#> 12967 yuppy           61.77192\n#> 12968 immix           60.55162\n#> 12969 kudzu           60.28942\n#> 12970 fuffy           57.36328\n#> 12971 jugum           56.97495\n#> 12972 qajaq           56.40821\n\nHere are the best first guesses:\n\nhead(expected_reduction_table)\n\n#>   guess           expected_reduction\n#> 1 roate           97.78920\n#> 2 irate           97.73002\n#> 3 soare           97.62505\n#> 4 later           97.47257\n#> 5 ariel           97.42851\n#> 6 arise           97.42462\n\nBefore we discuss these results, though, let’s get rid of that randomness. I’m going to take just the top 200 results of our semi-randomized analysis and rerun it with the full set of possible answers. Unless the very best first guess somehow made it out of the top 200, this should give us the true, statistically optimal best guess.\n\nguess_dictionary_top <- strsplit(head(expected_reduction_table$guess, n = 200), \"\")\n\nexpected_reductions_exact <- data.frame(guess = rep(NA, 200),\n                                       expected_reduction = rep(NA, 200))\nfor (n in 1:200) {\n  guess <- paste(guess_dictionary_top[[n]], collapse = \"\")\n  expected_reductions_exact$guess[n] <- guess\n  distribution_table <- guess_quality(guess) %>%\n    mutate(dictionary_reduction = 100-(100*(posterior_dictionary_length/length(answer_dictionary))))\n  expected_reductions_exact$expected_reduction[n] <- mean(distribution_table$dictionary_reduction)\n}\n\nexpected_reductions_exact <- expected_reductions_exact %>%\n  arrange(desc(expected_reduction))\nhead(expected_reductions_exact, 10)\n\n#>     guess           expected_reduction\n#>  1  roate           97.38987\n#>  2  raise           97.36497\n#>  3  raile           97.35072\n#>  4  soare           97.30881\n#>  5  arise           97.24727\n#>  6  irate           97.24496\n#>  7  orate           97.24014\n#>  8  ariel           97.17980\n#>  9  arose           97.14811\n#>  10 raine           97.10341\n\nThere’s been quite a bit of shifting around, but roate is still on top! Roate, the cumulative net earnings after taxes available to common shareholders, adjusted for tax-affected amortization of intangibles, for the calendar quarters in each calendar year in a specified period of time divided by average shareholder’s tangible common equity! Notice also that roate’s expected reduction has gone down since our randomly-sampled round, from 97.79% to 97.39%. This makes sense and is similar to regression toward the mean–the first set had some randomness and the most extreme results of that randomness were selected to be on top. The second set had no randomness and therefore no boost on the top end.\nThere you have it. The statistically optimal Wordle starting word is roate.\n\n\nCmon, I wanna see the optimal Wordle playing Bot!\nOk fine. Here it is:\n\n    # This is the same as dictionary_update, but it prints the outcome of the guess with nice colors\ndictionary_update_printed <- function(guess, answer, dictionary) {\n  for (n in 1:5) {\n    if (guess[n] %in% answer){\n      if (guess[n] == answer[n]) {\n        dictionary <- dictionary[sapply(dictionary, \"[\", n) == guess[n]]\n        cat(paste0(\"\\033[48;5;46m\", guess[n]))\n      }else{\n        dictionary <- dictionary[sapply(dictionary, \"[\", n) != guess[n]\n                                 & sapply(dictionary, function(x) any(guess[n] %in% x))]\n        cat(paste0(\"\\033[48;5;226m\", guess[n]))\n      }\n    }else{\n      dictionary <- dictionary[sapply(dictionary, function(x) !any(guess[n] %in% x))]\n      cat(paste0(\"\\033[48;5;249m\", guess[n]))\n    }\n  }\n  dictionary\n}\n\n    # Returns average posterior dictionary length\nguess_quality_minimal <- function(guess, dictionary = answer_dictionary) {\n  guess <- unlist(strsplit(guess, \"\"))\n  distribution <- rep(NA, length(dictionary))\n  for (n in 1:length(distribution)) {\n    answer <- dictionary[[n]]\n    distribution[n] <- length(dictionary_update(guess, answer, dictionary))\n  }\n  mean(distribution)\n}\n\n  # THE BOT\n\nplay <- function(answer){\n  stopifnot(length(answer) == 5)\n  # Make the first guess (always \"roate\") and update the dictionary\n  dictionary <- dictionary_update_printed(unlist(strsplit(\"roate\", \"\")), answer, answer_dictionary)\n  cat(\"\\n\")\n  # Loop for the other 5 guesses\n  for (try in 1:5) {\n    # If there's only one option left, guess it and end the game.\n    if (length(dictionary) == 1) {\n      cat(paste0(\"\\033[48;5;46m\", paste(dictionary[[1]], collapse = \"\")))\n      cat(\"\\n\")\n      break\n    }\n    # If there are two options left, guess the first. If that's the answer, end the game. If not, guess the next one and end the game.\n    if (length(dictionary) == 2) {\n      if (paste(dictionary[[1]], collapse = \"\") == paste(answer, collapse = \"\")){\n        dictionary <- dictionary_update_printed(dictionary[[1]], answer, dictionary)\n        cat(\"\\n\")\n        break\n      }else{\n        dictionary <- dictionary_update_printed(dictionary[[1]], answer, dictionary)\n        dictionary <- dictionary_update_printed(dictionary[[1]], answer, dictionary)\n        cat(\"\\n\")\n        break\n      }\n    }\n    # Loop through possible guesses and evaluate how many possible answers we would expected to have after guessing them. Then pick the lowest one and guess it.\n    guess_remainders <- rep(NA, length(guess_dictionary))\n    for (n in 1:length(guess_dictionary)) {\n      guess <- paste(guess_dictionary[[n]], collapse = \"\")\n      guess_remainders[n] <- guess_quality_minimal(guess, dictionary)\n    }\n    dictionary <- dictionary_update_printed(guess_dictionary[[which.min(guess_remainders)]], answer, dictionary)\n    cat(\"\\n\")\n    if (paste(guess_dictionary[[which.min(guess_remainders)]], collapse = \"\") == paste(answer, collapse = \"\")) {\n      break\n    }\n  }\n}\n\nObviously the bot always starts by guessing “roate”. After each new guess, the expected helpfulness of each possible next guess is reassessed based on the shortened list of answer possibilities. Other than that, the whole process is the same one I’ve been working with all along; each guess is the one that is expected to narrow down the remaining possibilities the most.\nLet’s see it play a few games!\n           \nCan we learn any sage advice from this digital Wordle master? Just by looking at the games, it’s hard to say. I’m surprised by its willingness to use rarish letters in the second guess, but that doesn’t amount to a general statement about what kind of words make good second guesses. If we really want to learn some grandmaster-level Wordle skills, we’ll have to return to our probability density plots.\nHere’s one for our champion first guess:\n\nLooks pretty good, but still not clear how to squeeze strategy from this. The problem is that I have no idea how those peaks and valleys correspond to actual situations we might face in the Wordle battlefield. I don’t want to know what by what percentages I’m likely to narrow the search–I want to know what second guesses I’m likely to need after roate! It’s time to reveal the ugly monster lurking behind the density plot: the bar chart.\n\nYup. There are a few discrete outcomes that are more likely than the other ones. Well what are they? And what does the bot say to do in those situations?\nAs I’ve mentioned before, the tallest bar all the way to the left (i.e. the single most likely outcome) is definitely the case in which none of the letters in roate are in the target word - you turn up with all black squares. In that case, the computer says that the optimal next guess is slimy.\nHere are the top 5 most likely outcomes of the first guess roate, along with the optimal second guess:\n\n (8.4% chance). Optimal second guess: slimy\n (6.1% chance). Optimal second guess: bludy\n (5.4% chance). Optimal second guess: shunt\n (4.8% chance). Optimal second guess: lysin\n (4.6% chance). Optimal second guess: silen\n\nSo there you are. If you’ve ever looked longingly at a chess grandmaster and thought, “I wish I could just memorize all the best openings instead of playing like a normal person”, your wish has come true."
  },
  {
    "objectID": "blog/medieval_philosophers/index.html",
    "href": "blog/medieval_philosophers/index.html",
    "title": "Designing a Poster to Visualize the Timeline of Philosophers in the Islamic World",
    "section": "",
    "text": "One day when I was in 5th grade, I walked into my classroom to find a new poster on the wall. It was a visualization of the entirety of world history - I was transfixed. This is that poster below. You can buy it here. It’s greatest innovation is squishing geography (which is generally two-dimensional) onto the y-axis. A lot of detail is lost, but the gained ability to visualize history all at once on a wall poster makes it worth it.\n\nI quickly asked my parents if I could get one for myself. When I finally did, I set it up next to my bed - as I lay there every night, I would look at all the little details. I even found a few mistakes.\nFast forward a decade and a half. I’ve been really enjoying this podcast, The History of Philosophy Without Any Gaps. I’m on episode 290 at the moment. Learning about all of these philosophers is great, but they can be hard to keep track of. I need a timeline that keeps track of geography too.\nThis is exactly the kind of problem for which I developed the wormsplot package in R, inspired by that wonderful poster from my childhood. In this post, I’ll walk through the process of designing a wall poster to visualize the major philosophers of the Islamic world in the Middle Ages.\nCode and data for this project can be found here.\n\nStep 1: The Data\nI gathered this data myself from wherever I could find it - mostly Wikipedia and The Stanford Encyclopedia of Philosophy. It includes one row for each new stop along the way of a biography (starting with the birth date - usually an educated guess), plus one extra for the death date. I’m no historian, so don’t rely too heavily on the accuracy of these data. Actually even if I were a historian this would take a lot of guesswork - that’s how medieval history goes. Anyhow, here’s what it looks like: one column for name, one for city, one for date, and one for the philosopher’s religion (Muslim, Jewish, or Christian). For cities that no longer exist or are called something different now, I wrote the closest modern equivalent and made a note of it on the side.\n\n\n# A tibble: 10 × 5\n   name                      city     date religion  notes   \n   <chr>                     <chr>   <dbl> <chr>     <chr>   \n 1 al-Kindī                  Kufa      801 Muslim    <NA>    \n 2 al-Kindī                  Baghdad   820 Muslim    <NA>    \n 3 al-Kindī                  Baghdad   871 Muslim    <NA>    \n 4 Ḥunayn ibn Isḥāq          Kufa      809 Christian al-Ḥīrah\n 5 Ḥunayn ibn Isḥāq          Baghdad   828 Christian <NA>    \n 6 Ḥunayn ibn Isḥāq          Baghdad   873 Christian <NA>    \n 7 Isḥāq ibn Ḥunayn          Baghdad   830 Christian <NA>    \n 8 Isḥāq ibn Ḥunayn          Baghdad   910 Christian <NA>    \n 9 Abū Bakr al-Rāzī (Rhazes) Rey       864 Muslim    <NA>    \n10 Abū Bakr al-Rāzī (Rhazes) Baghdad   880 Muslim    <NA>    \n\n\n\n\nStep 2: Fitting Geography Onto One Axis\nThe biggest challenge here is that y-axis. The first step: Find the latitude and longitude of each city in the data by calling the Open Street Map API. This worked very smoothly for everything except the city of Alexandria, which it identified at Alexandria, Virginia. After fixing that problem, I could plot a map of all the cities in the data:\n\n\n\n\n\n\n\n\nThere are many ways to reduce two-dimensional data to one dimension, and the best choice depends on the task at hand. If my places were grouped into distinct regions, I might consider t-SNE or UMAP. If they were generally aligned along some diagonal axis, I might use Principle Component Analysis. As it stands though, I know exactly how I want my y-axis to be organized: It should go East to West along the coast of North Africa and then West to East within Western Europe. This makes sense both geographically and historically: Since Andalusia (Muslim Spain) was the main point of contact between the Islamic world and Christian Europe, Southern France should be ‘farther’ from Tunisia than Spain.\nSo I split the cities into Europe and Non-Europe and lined them up by longitude accordingly. After a few manual adjustments (going through all of Turkey before moving South along the Mediterranean coast, and moving Northern France and London to the far end of the axis), I ended up with this ordering:\n\n\n\n\n\nThe final step was to make up for the fact that certain cities that are very close to each other in longitude are actually quite far away on the North-South axis. I achieved this by scaling the distance between each city on the axis by the true Euclidean distance between them. This stretches out certain parts of the axis disproportionately, but it means that adjacent locations are the right relative distances away from each other. With that, here are all the cities arranged along the new y-axis:\n\n\n\n\n\n\n\nStep 3: Layout\nWith the y-axis defined, it’s time for some graphic design. I originally toyed with a gradient background along the y-axis, but settled on dividing it up into larger regions. Here’s the resulting blank plot:\n\n\n\n\n\n\n\nStep 4: Plot!\nNow that we have a suitable background, all that remains to represent the data. This is done using the wormsplot function stat_worm(). Labels are added with the aid of the label_worms() function. The worms are colored by their religion: Muslims in green, Jews in blue, and Christians in red.\n\n\n\n\n\nI think it looks quite nice! Even without squinting at the individual names, big trends in the history of philosophy are immediately evident, like the Baghdad school starting with al-Kindī and running for about 150 years, or the explosion of philosophical activity in Andalusia in the 12th century.\nAt the moment, this is just a proof of concept. The labeling of the worms especially needs some work. If I were to produce a finished poster, I would also include a timeline of important events along the bottom, and a legend to explain the format. I might also want to simplify the data somewhat - a few of these figures moved around a lot in their lifetimes (Ibn `Arabi, I’m looking at you) and are making it a bit difficult to follow the lines."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "MS in Neurocognitive Experimental Psychology and Data Science, 2022-Present\nBen Gurion University of the Negev\nBA in Psychology and Biology, 2018-2021\nYeshiva University"
  },
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "Louis Teitelbaum’s CV",
    "section": "",
    "text": "louist@post.bgu.ac.il\n Home Page |  Blog\n GitHub |  LinkedIn\n\n\n\nEnglish - native\nHebrew - fluent\n\n\n\nProgramming: R, Python, SQL, Git(Hub), SPSS, Excel\nDocumentation: (R)Markdown/Quarto, JupyterLab, Google Docs/Sheets\nDesign: ggplot2, Gimp, Photoshop, Inkscape\nDeployment: Shiny, Streamlit\n\n\n\nLast updated on 2023-06-28."
  },
  {
    "objectID": "CV/index.html#title",
    "href": "CV/index.html#title",
    "title": "Louis Teitelbaum’s CV",
    "section": "Louis Teitelbaum",
    "text": "Louis Teitelbaum"
  },
  {
    "objectID": "CV/index.html#professional-experience",
    "href": "CV/index.html#professional-experience",
    "title": "Louis Teitelbaum’s CV",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nNeurocognitive Psychology Researcher\nN/A\nN/A\nSince 2020\nWorking Memory and Predictive Processing: - Thesis work under supervision of Prof. Yoav Kessler, Ben Gurion University\nLanguage and Communication: - Thesis work under supervision of Prof. Bruno Galantucci, Yeshiva University Psychology Department - Developed novel Natural Language Processing (NLP) based metrics for diadic conversations using Python and R. - Wrote peer review - Designed research agenda and experimental paradigm for the continuation of research post-COVID-19 - Read over 100 scholarly articles in the field of Language and Communication\nPower Dynamics in Long-Term Romantic Relationships: - Research Assistant for Dr. Jenny Isaacs, Yeshiva University Psychology Department - Researched and designed survey for data collection. - Analyzed data in SPSS and R; finalized analysis for publication. - Logged over 200 hours as a primary contributor to project.\n\n\nR Developer\nN/A\nN/A\nSince 2021\n\nDeveloper of wormsplot for visualizing the movements of historical figures; Read the blog post."
  },
  {
    "objectID": "CV/index.html#education",
    "href": "CV/index.html#education",
    "title": "Louis Teitelbaum’s CV",
    "section": "Education",
    "text": "Education\n\nM.A. in Experimental Cognitive Psychology\nBen-Gurion University\nBe’er Sheva, Israel\n2022 - Present\nSupervisor: Prof. Yoav Kessler\n\n\nB.A. in Psychology with Minor in Biology\nYeshiva University Honors\nNew York, NY\n2018 - 2021\nGraduated Magna Cum Laude\nActivities and societies: Neuroscience Club President"
  },
  {
    "objectID": "CV/index.html#awards",
    "href": "CV/index.html#awards",
    "title": "Louis Teitelbaum’s CV",
    "section": "Awards and Scholarships",
    "text": "Awards and Scholarships\n\nChaplain Joseph Hoenig Memorial Award for Excellence in the Study of Psychology\nYeshiva University Psychology Department\nNew York, NY\nSep 2021\n\n\nAmerican Psychology-Law Society Grant for Undergraduate Research\nAmerican Psychological Association Division 41: American Psychology-Law Society\nN/A\nJan 2021\nFor research in marital power dynamics in divorce mediation.\n\n\nThree funding awards from the Yeshiva University Honors Program\nYeshiva University Honors Program\nNew York, NY\n2021"
  },
  {
    "objectID": "CV/index.html#pubs",
    "href": "CV/index.html#pubs",
    "title": "Louis Teitelbaum’s CV",
    "section": "Scientific Publications and Presentations",
    "text": "Scientific Publications and Presentations\n\nTheses\nN/A\nN/A\nN/A\n\nTeitelbaum, L. (2021). When faithful informational exchange is just not worth it: Reformulation ability as a predictor of other-initiated repair [Honors thesis, Yeshiva University]. Yeshiva Academic Institutional Repository.\n\n\n\nPoster Presentations\nN/A\nN/A\nN/A\n\nTeitelbaum, L. & Galantucci, B. (2021). When faithful informational Exchanges are just too much work: Repair avoidance and reformulation cost. Poster presented at the 2021 Association for Psychological Science (APS) Virtual Convention.\nTeitelbaum, L., Isaacs, J., & Pittinsky, N. (2021). Is power all bad? Marital power imbalances and effective role division. Poster presented at the 2021 Association for Psychological Science (APS) Virtual Convention."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Louis Teitelbaum",
    "section": "",
    "text": "Designing a Poster to Visualize the Timeline of Philosophers in the Islamic World\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nDataViz\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Questions in Dialogue\n\n\n\n\n\n\n\nR\n\n\nBayesian Statistics\n\n\nDialogue\n\n\nCognitive Science\n\n\nCognitive Linguistics\n\n\nMultilevel Modeling\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nTracking Cognitive Performance with Online Chess\n\n\n\n\n\n\n\nR\n\n\nCognitive Science\n\n\nChess\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nתכלית מוקד 100 להציל חיים\n\n\n\n\n\n\n\nR\n\n\nData Mining\n\n\nScraping\n\n\nGovernment\n\n\nעברית\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\n  \n\n\n\n\nStatistically Optimal Wordle\n\n\n\n\n\n\n\nR\n\n\nprobability theory\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2022\n\n\nLouis Teitelbaum\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/minecraft_cogsci/index.html",
    "href": "blog/minecraft_cogsci/index.html",
    "title": "Predicting Questions in Dialogue",
    "section": "",
    "text": "The Minecraft Dialogue Corpus (Narayan-Chen et al., 2019) is a collection of 509 text-based conversations between players collaborating to build structures. The “Architect” is shown a target structure and needs to explain to the “Builder” how to build it. The Architect can observe the Builder but cannot place blocks.\nFor now, I am using this corpus as practice to learn Bayesian statistics as I go through Statistical Rethinking, by Richard McElreath with its wonderful brms/tidyverse translation by A Solomon Kurz.\nCode and data for this project can be found here.\nA sample dialogue from the Minecraft Corpus:\nAll three of the Builder’s turns in this dialogue (lines 2, 5, and 8) are questions. They have some qualities in common: they all request something of the interlocutor, and they all signal their status as questions with a question mark at the end. This latter feature makes them easy to identify with one line of code:\nWhat causes people to ask questions in the Minecraft collaborative building task? Let’s start by thinking about the Builder’s questions from the excerpt above.\nThe question on line 2, “Now what?” does not have an obvious antecedent - the Builder seems to have understood and excecuted the previous instruction and is now merely moving the conversation along by asking for another one.\nThe questions on lines 5 and 8, on the other hand, do have clear antecedents in the conversation. Specifically, they each refer to the architect’s immediately preceding instruction and request clarification thereof. These questions can therefore be considered repair initiations, turns in talk that identify trouble (i.e. a need for clarification) in a preceding turn or turns uttered by an interlocutor.\nHence the first theoretical answer to my question: What causes people to ask questions? The need for clarification.\nOf course not all questions are repair initiations, and not all repair initiations are presented as questions. Nevertheless, the structure of the Minecraft collaborative building task makes the correlation very high. In the task, the Architect has access to all of the information that the Builder needs to proceed (the design of the target structure) and information is the only thing that the Builder can get from the Architect. This means that pretty much all of the Builder’s questions are aimed at clarifying information coming from the Architect.\nFor these reasons, and since I have no way of identifying true repair iniitations other than going through the whole corpus myself, I will operationally define repair as any Builder’s utterance that includes a question mark.\nTo convince you that almost all Builder questions in the corpus really are repair initiations, here are 10 randomly selected repairs:\nEven without looking at context, it seems clear at a glance that all but line 1571 here are indeed repairs."
  },
  {
    "objectID": "blog/minecraft_cogsci/index.html#how-to-predict-repair",
    "href": "blog/minecraft_cogsci/index.html#how-to-predict-repair",
    "title": "Predicting Questions in Dialogue",
    "section": "How to Predict Repair",
    "text": "How to Predict Repair\nBefore I start exploring predictor variables, I need to figure out how to model the probability of repair. My first instinct was use Builder turns as cases and model whether of not they are repairs. There’s a fatal flaw with this plan though: the natural alternative to initiating repair is probably not initiating something other than repair. The alternative is more likely to be not saying anything. In other words, we only have data about whether or not texts the Builder sent were repairs. We have no data about texts the Builder didn’t send.\nI’ll draw that more formally as a DAG:\n\nlibrary(ggdag)\nshorten_dag_arrows <- function(tidy_dag, shorten_distance){\n  # Update underlying ggdag object\n  tidy_dag$data <- dplyr::mutate(tidy_dag$data, \n                                 proportion = shorten_distance/sqrt((xend - x)^2 + (yend - y)^2),\n                                 xend = (1-proportion/2)*(xend - x) + x, \n                                 yend = (1-proportion/2)*(yend - y) + y,\n                                 xstart = (1-proportion/2)*(x - xend) + xend,\n                                 ystart = (1-proportion/2)*(y-yend) + yend) %>% select(!proportion)\n  return(tidy_dag)\n}\n\n# Length -> TF-IDF Sum -> Probability of Repair -> Question Mark at End\n# Turns since Last Repair -> Probability of Repair -> Question Mark at End\ndag_coords_0 <-\n  tibble(name = c(\"C\", \"U\", \"R\", \"Q\"),\n         x    = c(1, 2, 3, 4),\n         y    = c(2, 2, 2, 2))\n\ndagify(U ~ C,\n       R ~ U,\n       Q ~ R,\n       coords = dag_coords_0) %>%\n  tidy_dagitty() %>%\n  shorten_dag_arrows(.25) %>%\n  dag_label(labels = c(\"C\" = \"\", \n                       \"U\" = str_wrap(\"Builder Responds\", 10), \n                       \"R\" = \"Repair\", \n                       \"Q\" = str_wrap(\"Question Mark\", 10))) %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"R\"), size = 30, show.legend = F) +\n  geom_dag_text(aes(label = label), size = 4) +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +\n  geom_dag_edges(aes(x = xstart, y = ystart)) +\n  theme_dag()\n\n\n\n\n? -> Builder Responds -> Probability of Repair -> Question Mark at End\n\n\nAs yet undiscussed variables predict whether or not the Builder will respond to an instruction from the Architect, which in turn predicts whether the response will be a repair initiation (if the Builder doesn’t respond there’s zero probability of the response being anything), which in turn influences our proxy variable, the presence of a question mark.\nAll of this means that my inital stategy would probably wash out any effect of predictors on repair, since we would already be stratifying by whether the Builder responds. I think the simplest way to solve this problem is to use Architect turns as cases and predict whether or not the next turn will be a Builder repair. Sometimes the next turn will be another Architect turn (when Builder did not respond), and sometimes it will be a non-repair Builder turn.\nTime to code a few predictor variables.\n\n# Orthographic Length of Previous Turn\n# Mean TF-IDF of previous turn (word-fanciness? information density?)\n# Sum TF-IDF of previous turn (lexical complexity? information?)\n# Turns since last question asked\n\nd1 <- convert(minecraftcorpus_tfidf, to = \"data.frame\") %>% \n  select(!doc_id) %>%\n  transmute(tfidfsum = rowSums(across())) %>%\n  bind_cols(minecraftcorpusdf) %>%\n  select(!c(file, session)) %>%\n  mutate(repair = grepl(\"\\\\?\", text) & role == \"B\",\n         repair_next = NA,\n         length = nchar(text),\n         charssincerepair = 0,\n         partner = if_else(role == \"B\", \n                           str_remove_all(str_sub(conversation, -2, -1), \"A\"),\n                           str_remove_all(str_sub(conversation, 2, 3), \"-\")))\n\ncharssincerepair <- 0L\n\nfor (n in 2:nrow(d1)) {\n  if(d1$conversation[n-1L] != d1$conversation[n]) {\n    charssincerepair <- 0L\n  }\n  if(d1$repair[n] == TRUE & d1$role[n] == \"B\"){\n    charssincerepair <- 0L\n  }\n  d1$charssincerepair[n] <- charssincerepair\n  charssincerepair <- charssincerepair + d1$length[n]\n}\n\nfor (n in 1:(nrow(d1)-1L)) {\n  if(d1$conversation[n+1L] == d1$conversation[n]) {\n    d1$repair_next[n] <- d1$repair[n+1]\n  }\n}\n\n# Repair as Factor, + Case Index\nd1 <- d1 %>%  \n  mutate(repair_next  = factor(repair_next, levels = c(FALSE, TRUE)),\n         case = factor(1:n()))\n\nd1 <- d1 %>%\n  filter(role == \"A\",\n         charssincerepair != 0,\n         tfidfsum != 0) %>%\n  mutate(charssincerepair_log = log(charssincerepair),\n         charssincerepair_log_s = (charssincerepair_log-mean(charssincerepair_log, na.rm = T))/sd(charssincerepair_log, na.rm = T),\n         tfidfsum_log = log(tfidfsum),\n         tfidfsum_log_s = (tfidfsum_log - mean(tfidfsum_log, na.rm = T))/sd(tfidfsum_log, na.rm = T))"
  },
  {
    "objectID": "blog/minecraft_cogsci/index.html#length-of-previous-turn",
    "href": "blog/minecraft_cogsci/index.html#length-of-previous-turn",
    "title": "Predicting Questions in Dialogue",
    "section": "Length of Previous Turn",
    "text": "Length of Previous Turn\nI have already theorized that the likelihood of a given Builder turn being a repair initiation is increased by the need for clarification of previous turns. Are there certain types of instructions that need to be clarified more often? How about long and complicated ones?\nHere’s a quick and dirty graph of repair against length of the previous turn, with repair formatted as numeric and a Loess line running between No and Yes:\n\nlibrary(ggbeeswarm)\n\nd1 %>%\n  ggplot(aes(length, (as.numeric(repair_next)-1))) +\n  geom_quasirandom(method = \"pseudorandom\", \n                   width = .2,\n                   groupOnX = F, \n                   alpha = .1, \n                   varwidth = T) +\n  geom_smooth() +\n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(breaks = c(0, .25, .5, .75, 1),\n                     labels = c(\"No\", .25, .5, .75, \"Yes\")) +\n  labs(x = \"Length of Turn (characters, log scale)\",\n       y = \"Probability of Next-Turn Repair\") +\n  theme_minimal()\n\n\n\n\nRepair X Length of Previous Turn\n\n\nLooks promising! It’s hard to tell just by looking at the data points, but the regression line seems to think that longer previous turns are associated wih more repairs. It is worth noting at this point that there are very few next-turn repairs and very many next-turn non-repairs. This means that when we model this formally we will have to be careful interpreting the regression coefficients.\nWe might do a bit better if, rather than counting the number of characters, we had a measure more closely related to how much information is being conveyed. TF-IDF (Term Frequency * Inverse Document Frequency) fits the bill. The TF-IDF of a word describes how rare it is in the whole corpus vs. how common it is in its own turn. Presumably, rarer words are less predictable and therefore more informative and more confusing. The sum of TF-IDF scores of all words in an turn should tell us something about how much new semantic material is included in each turn.\n\n\n\nRepair X Length of Previous Turn\n\n\nThis looks similar to the first one. Indeed, TF-IDF Sum and Turn length are correlated in the corpus at r = 0.956. Nevertheless, I’m going to stick with TF-IDF Sum because it makes more sense to me as a theoretical predictor.\n\nBayesian Modeling\nI’ll start by simulating reasonable priors. I’ll let the slope be positive or negative. Less than 15% of Builder turns are repairs, so I’ll lower the intercepts. After playing around with the parameters a bit, I settled on this:\n\n# Simulating Reasonable Priors\nd1 %>%\n  group_by(repair_next) %>%\n  summarise(perc = 100*n()/nrow(.))   # 14.3% of Architect turns are immediately followed by a Builder repair initiation\n\npriors <- \n  tibble(n = 1:50,\n         a = rnorm(50, -1.5, 1),\n         b = rnorm(50, 0, 1)) %>% \n  expand(nesting(n, a, b), x = seq(from = -3, to = 3, length.out = 200)) %>% \n  mutate(p = inv_logit_scaled(a+b*x)) %>%\n  arrange(n) %>%\n  mutate(n = factor(n)) \npriors %>%\n  ggplot(aes(x, p, group = n)) +\n  geom_line(alpha = .5)\n\nLet’s set up the model.\n\nlibrary(brms)\nlibrary(tidybayes)\n\ntfidf_mod <- brm(\n  repair_next ~ 1 + tfidfsum_log_s,\n  data = d1,\n  family = bernoulli,\n  prior = c(prior(normal(-1.5, 1), class = Intercept),\n            prior(normal(0, 1), class = b)),\n  sample_prior = \"yes\")\n\nHere’s the model summary:\n\nprint(tfidf_mod))\n\n\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + tfidfsum_log_s \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept         -1.82      0.03    -1.87    -1.77 1.00     3175     2569\n## tfidfsum_log_s     0.26      0.03     0.20     0.31 1.00     2970     2172\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNow I can sample the posteriors and see what the models thinks.\n\nlibrary(ggbeeswarm)\n\nd1$tfidfsum_log_s[d1$tfidfsum == min(d1$tfidfsum, na.rm = T)] # -2.87\nd1$tfidfsum_log_s[d1$tfidfsum == max(d1$tfidfsum, na.rm = T)] # 2.51\n\n# Sample Prior and Posterior\n\ntfidf_mod_priors <- as_draws_df(tfidf_mod, c(\"prior_Intercept\", \"prior_b\"))[1:100,] %>%\n  as_tibble() %>%\n  mutate(n = factor(1:100)) %>%\n  expand(nesting(n, prior_Intercept, prior_b), x_log_s  = seq(from = -2.9, to = 2.51, length.out = 200)) %>%\n  mutate(p = inv_logit_scaled(prior_Intercept+prior_b*x_log_s),\n         x_log = x_log_s * sd(d1$tfidfsum_log) + mean(d1$tfidfsum_log),\n         x = exp(x_log))\n\nn_iter <- 50\ntfidf_mod_fitted <-\n  fitted(tfidf_mod,\n         newdata  = tibble(tfidfsum_log_s = seq(from = -2.9, to = 2.51, length.out = 200)),\n         summary  = F,\n         nsamples = n_iter) %>% \n  as_tibble() %>%\n  mutate(iter = 1:n_iter) %>% \n  pivot_longer(-iter) %>% \n  mutate(tfidfsum_log_s = rep(seq(from = -2.9, to = 2.51, length.out = 200), times = n_iter)) %>% \n  mutate(tfidfsum_log = tfidfsum_log_s * sd(d1$tfidfsum_log) + mean(d1$tfidfsum_log),\n         tfidfsum = exp(tfidfsum_log_s * sd(d1$tfidfsum_log) + mean(d1$tfidfsum_log)))\n\n\ntfidf_mod_postpredict <- tfidf_mod_fitted %>%\n  ggplot(aes(x = tfidfsum)) +\n    geom_hline(yintercept = .5, color = \"red\") +\n    geom_line(aes(y = value, group = iter), color = \"blue\", alpha = .1) +\n    geom_line(data = tfidf_mod_priors,\n              aes(x, p, group = n), color = \"black\", alpha = .08) + \n    geom_quasirandom(data = d1,\n                     aes(x = tfidfsum,\n                         y = as.integer(repair_next)-1),\n                     alpha = 1/10,\n                     groupOnX = F,\n                     width = 1/10,\n                     method = \"pseudorandom\",\n                     varwidth = T) +\n    scale_x_continuous(trans = \"log10\", minor_breaks = seq(10, 100, by = 10)) +\n    scale_y_continuous(breaks = c(0, .25, .5, .75, 1),\n                       labels = c(\"No\", .25, .5, .75, \"Yes\")) +\n    labs(title = \"Data with Prior and Posterior Predictions\",\n         y = \"Probability of Next-Turn Repair\", \n         x = \"Sum TF-IDF (Log Scale)\") +\n    theme_minimal()\n\ntfidf_mod_postpredict\n\nThe faint grey lines are 100 samples from the prior distribution. In blue are 50 samples from the posterior.\n\n\n\nTF-IDF Model Data with Prior and Posterior Predictions\n\n\nThe posterior predictions look almost like a straight line on the logarithmic scale - for very short, simple instructions from the Architect, the Builder’s response is most likely not to be a repair. As total TF-IDF goes up, the probability of repair does too, at first rapidly, then more slowly."
  },
  {
    "objectID": "blog/minecraft_cogsci/index.html#time-since-last-repair",
    "href": "blog/minecraft_cogsci/index.html#time-since-last-repair",
    "title": "Predicting Questions in Dialogue",
    "section": "Time Since Last Repair",
    "text": "Time Since Last Repair\nAs described in Dingemanse et al. (2015), another predictor of repair is the time elapsed since the last repair. People don’t tend to initiate repair twice in a row. In lieu of using actual timestamps, I’ll use the total number of characters typed as a proxy for time elapsed. I’ll use the same priors as before.\n\ncharssincerepair_mod <- brm(\n  repair_next ~ 1 + charssincerepair_log_s,\n  data = d1,\n  family = bernoulli,\n  prior = c(prior(normal(-1.5, 1), class = Intercept),\n            prior(normal(0, 1), class = b)),\n  sample_prior = \"yes\")\n\n\nprint(charssincerepair_mod)\n\n\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + charssincerepair_log_s \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept                 -1.81      0.03    -1.87    -1.76 1.00     3148     2719\n## charssincerepair_log_s    -0.24      0.03    -0.29    -0.19 1.00     3209     2570\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\ncharssincerepair_mod_priors <- as_draws_df(charssincerepair_mod, c(\"prior_Intercept\", \"prior_b\"))[1:100,] %>%\n  as_tibble() %>%\n  mutate(n = factor(1:100)) %>%\n  expand(nesting(n, prior_Intercept, prior_b), x_log_s  = seq(from = -4, to = 2.63, length.out = 200)) %>%\n  mutate(p = inv_logit_scaled(prior_Intercept+prior_b*x_log_s),\n         x_log = x_log_s * sd(d1$charssincerepair_log) + mean(d1$charssincerepair_log),\n         x = exp(x_log))\n\nn_iter <- 50\ncharssincerepair_mod_fitted <-\n  fitted(charssincerepair_mod,\n         newdata  = tibble(charssincerepair_log_s = seq(from = -4, to = 2.63, length.out = 200)),\n         summary  = F,\n         nsamples = n_iter) %>% \n  as_tibble() %>%\n  mutate(iter = 1:n_iter) %>% \n  pivot_longer(-iter) %>% \n  mutate(charssincerepair_log_s = rep(seq(from = -4, to = 2.63, length.out = 200), times = n_iter)) %>% \n  mutate(charssincerepair_log = charssincerepair_log_s * sd(d1$charssincerepair_log) + mean(d1$charssincerepair_log),\n         charssincerepair = exp(charssincerepair_log_s * sd(d1$charssincerepair_log) + mean(d1$charssincerepair_log)))\n\n\ncharssincerepair_mod_postpredict <- charssincerepair_mod_fitted %>%\n  ggplot(aes(x = charssincerepair)) +\n  geom_hline(yintercept = .5, color = \"red\") +\n  geom_line(aes(y = value, group = iter), color = \"blue\", alpha = .1) +\n  geom_line(data = charssincerepair_mod_priors,\n            aes(x, p, group = n), color = \"black\", alpha = .08) + \n  geom_quasirandom(data = d1,\n                   aes(x = charssincerepair,\n                       y = as.integer(repair_next)-1),\n                   alpha = 1/10,\n                   groupOnX = F,\n                   width = 1/10,\n                   method = \"pseudorandom\",\n                   varwidth = T) +\n  scale_x_continuous(trans = \"log10\", minor_breaks = seq(10, 100, by = 10)) +\n  scale_y_continuous(breaks = c(0, .25, .5, .75, 1),\n                     labels = c(\"No\", .25, .5, .75, \"Yes\")) +\n  labs(title = \"Data with Prior and Posterior Predictions\",\n       y = \"Probability of Next-Turn Repair\", \n       x = \"Total Characters Since Last Repair (Log Scale)\") +\n  theme_minimal()\n\ncharssincerepair_mod_postpredict\n\n\n\n\nCharacters Since Last Repair Model Data with Prior and Posterior Predictions\n\n\nHmmm. The model thinks that repairs get less likely as time goes on after the last repair. I have a hard time believing that. Where did I go wrong?\nAnswer: I didn’t use a multilevel model.\nWhat I think is happening is this: most of the variation in likelihood of next-turn repair is between participants. As it happens, pairs of participants in which the Builder is more likely to initiate next-turn repair are also more likely to have short turns and therefore fewer total characters since the last repair. Or something like that.\nBefore I get into complicated modeling to test this directly, I’ll drive home the main point with a simple, intercept-only multilevel model.\n\nbysubj_mod <- \n  brm(data = d1, \n      family = bernoulli,\n      repair_next ~ 1 + (1 | partner),\n      prior = c(prior(normal(-1.5, 1), class = Intercept), \n                prior(exponential(1), class = sd)),\n      iter = 5000, chains = 4, cores = 2,\n      sample_prior = \"yes\")\n\nplot(bysubj_mod)\nprint(bysubj_mod)\n\nbysubj_mod_post <- as_draws(bysubj_mod)\n\nbysubj_mod_post_mdn <- \n  coef(bysubj_mod, robust = T)$partner[, , ] %>% \n  data.frame() %>% \n  mutate(post_mdn = inv_logit_scaled(Estimate),\n         post_97.5 = inv_logit_scaled(Q97.5),\n         post_2.5 = inv_logit_scaled(Q2.5),\n         partner = unique(d1$partner)) %>%\n  right_join(d1) %>%\n  group_by(Estimate, Est.Error, post_2.5, post_97.5, post_mdn, partner) %>%\n  summarise(prop_repair_next = sum(repair_next == T)/n())\n\nView(bysubj_mod_post)\n\nbysubj_mod_post_mdn %>%\n  ggplot(aes(partner)) +\n    geom_hline(yintercept = inv_logit_scaled(median(bysubj_mod_post$`1`$b_Intercept)), \n               linetype = 2, color = \"orange2\") +\n    geom_point(aes(y = post_mdn, color = \"Model Intercepts\"), size = 3) +\n    geom_errorbar(aes(ymin = post_2.5, ymax = post_97.5, color = \"Model Intercepts\")) +\n    geom_point(aes(y = prop_repair_next, color = \"Empirical Proportions\"), size = 3, shape = 1) +\n    labs(title = \"Individual Differences in Repair Behavior\",\n         subtitle = str_wrap(\"The dashed line represents the model average intercept. \n                             Error bars represent 95% confidence interval.\", 60),\n         x = \"Builder ID\",\n         y = \"Probability of Next-Turn Repair\") +\n    scale_color_manual(name = element_blank(), \n                       values = c(\"black\", \"orange2\")) +\n    theme_minimal()\n\n\n\n\nIndividual Differences in Repair Behavior\n\n\nAs expected, different Builders have vastly different likelihoods of initiating next-turn repair.\nNow for the monster model. I just showed that different Builders have different likelihoods of initiating next-turn repair, but the same is probably true for different Architects. An Architect who likes to type a lot of turns in a row will lower the probability of any given next-turn being a repair, just as a Builder who initiates repair a lot will raise the probability. Clustering by both Architect and Builder gets complicated, and for now I’m only interested in group-level effects, so I’m just going to cluster by conversation and leave the model agnostic about whether it’s the Builder, the Architect, or some interaction between them that’s causing any difference. I’m also allowing both intercept and slope to vary, and telling the model to estimate the correlation between them.\n\ncharssincerepair_bysubj_mod <- \n  brm(data = d1, \n      family = bernoulli,\n      repair_next ~ 1 + charssincerepair_log_s + (1 + charssincerepair_log_s | conversation),\n      prior = c(prior(normal(-1.5, 1), class = Intercept), \n                prior(normal(0, 1), class = b),\n                prior(exponential(1), class = sd),\n                prior(lkj(2), class = cor)),\n      iter = 5000, chains = 4, cores = 2)\n\nprint(charssincerepair_bysubj_mod)\n\n\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + charssincerepair_log_s + (1 + charssincerepair_log_s | conversation) \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 5000; warmup = 2500; thin = 1;\n##          total post-warmup draws = 10000\n## \n## Group-Level Effects: \n## ~conversation (Number of levels: 64) \n##                                       Estimate Est.Error l-95% CI u-95% CI Rhat\n## sd(Intercept)                             0.76      0.08     0.62     0.94 1.00\n## sd(charssincerepair_log_s)                0.13      0.06     0.01     0.25 1.00\n## cor(Intercept,charssincerepair_log_s)     0.29      0.28    -0.33     0.79 1.00\n##                                       Bulk_ESS Tail_ESS\n## sd(Intercept)                             2003     3424\n## sd(charssincerepair_log_s)                1927     2051\n## cor(Intercept,charssincerepair_log_s)     7747     4847\n## \n## Population-Level Effects: \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept                 -1.87      0.10    -2.08    -1.68 1.00     1365     2679\n## charssincerepair_log_s     0.02      0.04    -0.05     0.09 1.00     7615     7229\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nBingo. The population-level effect of characters since repair is now slightly positive! Let’s visualize that:\n\n\n\nData with Multilevel Model Estimates\n\n\nThe blue line is the model’s estimate of the average effect of characters since last repair on repair probability within groups, with its 95% confidence interval. The grey lines are estimates for each conversation. This time I’m convinced - the effect of content elapsed since the last repair is small if it exists at all. The graph I linked to above from Dingemanse et al. (2015) shows the probability that repair will have occured, which of course rapidly approaches 1 as time goes on. The probability that any given turn will be a repair initiation doesn’t seem to change much, at least in the Minecraft corpus.\nAll this makes me want to re-do my analysis of TF-IDF Sum as a multilevel model. Here’s what that looks like:\n\ntfidfsum_bysubj_mod <- \n  brm(data = d1, \n      family = bernoulli,\n      repair_next ~ 1 + tfidfsum_log_s + (1 + tfidfsum_log_s | conversation),\n      prior = c(prior(normal(-1.5, 1), class = Intercept), \n                prior(normal(0, 1), class = b),\n                prior(exponential(1), class = sd),\n                prior(lkj(2), class = cor)),\n      iter = 5000, chains = 4, cores = 2)\n      \nprint(tfidfsum_bysubj_mod)\n\n\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + tfidfsum_log_s + (1 + tfidfsum_log_s | conversation) \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 5000; warmup = 2500; thin = 1;\n##          total post-warmup draws = 10000\n## \n## Group-Level Effects: \n## ~conversation (Number of levels: 64) \n##                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)                     0.76      0.08     0.62     0.93 1.00     1843\n## sd(tfidfsum_log_s)                0.12      0.06     0.01     0.23 1.00     2173\n## cor(Intercept,tfidfsum_log_s)    -0.15      0.31    -0.72     0.50 1.00     9390\n##                               Tail_ESS\n## sd(Intercept)                     3034\n## sd(tfidfsum_log_s)                3267\n## cor(Intercept,tfidfsum_log_s)     5410\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept         -1.92      0.10    -2.11    -1.72 1.00     1431     2332\n## tfidfsum_log_s     0.27      0.04     0.20     0.34 1.00     8006     6499\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nDidn’t change much here, but it does look like the population-level effect is slightly higher than with the fixed-effects model. Interestingly enough, the model estimates that the intercept and beta are slightly negatively correlated. That means that pairs who initiate repair more in general also show less effect of utterance length (or TF-IDF sum, to be exact) on their rates. It seems possible to me that, in highly informational contexts like the Minecraft collaborative building task, some people have a top-down push to just initiate repair as much as possible, regardless of whether or not there’s an obvious need for it. All of this is pretty speculative though, since the estimated error on the correlation parameter is enormous.\nHere’s what the new model looks like as a graph:\n\n\n\nData with Multilevel Model Estimates"
  },
  {
    "objectID": "blog/minecraft_cogsci/predicting_questions_README.html",
    "href": "blog/minecraft_cogsci/predicting_questions_README.html",
    "title": "Louis Teitelbaum",
    "section": "",
    "text": "A sample dialogue from the Minecraft Corpus:\n\n(1) Architect: start with a stack of 5 purple blocks in the middle\n(2) Builder: Now what?\n(3) Architect: cool! extend the top purple block to a row of 5 purple blocks\n_(4) Architect: so like an upside down “L”__\n(5) Builder: So they should extend to one side, correct?\n(6) Architect: yep!\n(7) Architect: nice! now, put a block above and below the second block from the right\n(8) Builder: What color should those blocks be?\n\nAll three of the Builder’s turns in this dialogue (lines 2, 5, and 8) are questions. They have some qualities in common: they all request something of the interlocutor, and they all signal their status as questions with a question mark at the end. This latter feature makes them easy to identify with one line of code:\nminecraftcorpusdf %>% mutate(questionmark = grepl(\"\\\\?\", text))\nWhat causes people to ask questions in the Minecraft collaborative building task? Let’s start by thinking about the Builder’s questions from the excerpt above.\nThe question on line 2, “Now what?” does not have an obvious antecedent - the Builder seems to have understood and excecuted the previous instruction and is now merely moving the conversation along by asking for another one.\nThe questions on lines 5 and 8, on the other hand, do have clear antecedents in the conversation. Specifically, they each refer to the architect’s immediately preceding instruction and request clarification thereof. These questions can therefore be considered repair initiations, turns in talk that identify trouble (i.e. a need for clarification) in a preceding turn or turns uttered by an interlocutor.\nHence the first theoretical answer to my question: What causes people to ask questions? The need for clarification.\nOf course not all questions are repair initiations, and not all repair initiations are presented as questions. Nevertheless, the structure of the Minecraft collaborative building task makes the correlation very high. In the task, the Architect has access to all of the information that the Builder needs to proceed (the design of the target structure) and information is the only thing that the Builder can get from the Architect. This means that pretty much all of the Builder’s questions are aimed at clarifying information coming from the Architect.\nFor these reasons, and since I have no way of identifying true repair iniitations other than going through the whole corpus myself, I will operationally define repair as any Builder’s utterance that includes a question mark.\nminecraftcorpusdf %>% mutate(repair = questionmark & role == \"B\")\nTo convince you that almost all Builder questions in the corpus really are repair initiations, here are 10 randomly selected repairs:\nminecraftcorpusdf %>%\n  filter(repair == TRUE) %>%\n  select(text) %>%\n  sample_n(10)\n  \n#> 1785    like that?\n#> 4312    like so? or like that?\n#> 14821   Are they on the ground?\n#> 15121   here?\n#> 628     like that?\n#> 3642    ooooh like this?\n#> 1571    it might be easier to describe all one color? then build from there?\n#> 182     is the purple supposed to be on the third level? i think i could make it float..\n#> 12521   Facing towards the middle?\n#> 2951    is this correct?\nEven without looking at context, it seems clear at a glance that all but line 1571 here are indeed repairs.\n\n\nBefore I start exploring predictor variables, I need to figure out how to model the probability of repair. My first instinct was use Builder turns as cases and model whether of not they are repairs. There’s a fatal flaw with this plan though: the natural alternative to initiating repair is probably not initiating something other than repair. The alternative is more likely to be not saying anything. In other words, we only have data about whether or not texts the Builder sent were repairs. We have no data about texts the Builder didn’t send.\nI’ll draw that more formally as a DAG:\nlibrary(ggdag)\nshorten_dag_arrows <- function(tidy_dag, shorten_distance){\n  # Update underlying ggdag object\n  tidy_dag$data <- dplyr::mutate(tidy_dag$data, \n                                 proportion = shorten_distance/sqrt((xend - x)^2 + (yend - y)^2),\n                                 xend = (1-proportion/2)*(xend - x) + x, \n                                 yend = (1-proportion/2)*(yend - y) + y,\n                                 xstart = (1-proportion/2)*(x - xend) + xend,\n                                 ystart = (1-proportion/2)*(y-yend) + yend) %>% select(!proportion)\n  return(tidy_dag)\n}\n\n# Length -> TF-IDF Sum -> Probability of Repair -> Question Mark at End\n# Turns since Last Repair -> Probability of Repair -> Question Mark at End\ndag_coords_0 <-\n  tibble(name = c(\"C\", \"U\", \"R\", \"Q\"),\n         x    = c(1, 2, 3, 4),\n         y    = c(2, 2, 2, 2))\n\ndagify(U ~ C,\n       R ~ U,\n       Q ~ R,\n       coords = dag_coords_0) %>%\n  tidy_dagitty() %>%\n  shorten_dag_arrows(.25) %>%\n  dag_label(labels = c(\"C\" = \"\", \n                       \"U\" = str_wrap(\"Builder Responds\", 10), \n                       \"R\" = \"Repair\", \n                       \"Q\" = str_wrap(\"Question Mark\", 10))) %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"R\"), size = 30, show.legend = F) +\n  geom_dag_text(aes(label = label), size = 4) +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +\n  geom_dag_edges(aes(x = xstart, y = ystart)) +\n  theme_dag()\n\n\n\n? -> Builder Responds -> Probability of Repair -> Question Mark at End\n\n\nAs yet undiscussed variables predict whether or not the Builder will respond to an instruction from the Architect, which in turn predicts whether the response will be a repair initiation (if the Builder doesn’t respond there’s zero probability of the response being anything), which in turn influences our proxy variable, the presence of a question mark.\nAll of this means that my inital stategy would probably wash out any effect of predictors on repair, since we would already be stratifying by whether the Builder responds. I think the simplest way to solve this problem is to use Architect turns as cases and predict whether or not the next turn will be a Builder repair. Sometimes the next turn will be another Architect turn (when Builder did not respond), and sometimes it will be a non-repair Builder turn.\nTime to code a few predictor variables.\n# Orthographic Length of Previous Turn\n# Mean TF-IDF of previous turn (word-fanciness? information density?)\n# Sum TF-IDF of previous turn (lexical complexity? information?)\n# Turns since last question asked\n\nd1 <- convert(minecraftcorpus_tfidf, to = \"data.frame\") %>% \n  select(!doc_id) %>%\n  transmute(tfidfsum = rowSums(across())) %>%\n  bind_cols(minecraftcorpusdf) %>%\n  select(!c(file, session)) %>%\n  mutate(repair = grepl(\"\\\\?\", text) & role == \"B\",\n         repair_next = NA,\n         length = nchar(text),\n         charssincerepair = 0,\n         partner = if_else(role == \"B\", \n                           str_remove_all(str_sub(conversation, -2, -1), \"A\"),\n                           str_remove_all(str_sub(conversation, 2, 3), \"-\")))\n\ncharssincerepair <- 0L\n\nfor (n in 2:nrow(d1)) {\n  if(d1$conversation[n-1L] != d1$conversation[n]) {\n    charssincerepair <- 0L\n  }\n  if(d1$repair[n] == TRUE & d1$role[n] == \"B\"){\n    charssincerepair <- 0L\n  }\n  d1$charssincerepair[n] <- charssincerepair\n  charssincerepair <- charssincerepair + d1$length[n]\n}\n\nfor (n in 1:(nrow(d1)-1L)) {\n  if(d1$conversation[n+1L] == d1$conversation[n]) {\n    d1$repair_next[n] <- d1$repair[n+1]\n  }\n}\n\n# Repair as Factor, + Case Index\nd1 <- d1 %>%  \n  mutate(repair_next  = factor(repair_next, levels = c(FALSE, TRUE)),\n         case = factor(1:n()))\n\nd1 <- d1 %>%\n  filter(role == \"A\",\n         charssincerepair != 0,\n         tfidfsum != 0) %>%\n  mutate(charssincerepair_log = log(charssincerepair),\n         charssincerepair_log_s = (charssincerepair_log-mean(charssincerepair_log, na.rm = T))/sd(charssincerepair_log, na.rm = T),\n         tfidfsum_log = log(tfidfsum),\n         tfidfsum_log_s = (tfidfsum_log - mean(tfidfsum_log, na.rm = T))/sd(tfidfsum_log, na.rm = T))\n\n\n\nI have already theorized that the likelihood of a given Builder turn being a repair initiation is increased by the need for clarification of previous turns. Are there certain types of instructions that need to be clarified more often? How about long and complicated ones?\nHere’s a quick and dirty graph of repair against length of the previous turn, with repair formatted as numeric and a Loess line running between No and Yes:\nlibrary(ggbeeswarm)\n\nd1 %>%\n  ggplot(aes(length, (as.numeric(repair_next)-1))) +\n  geom_quasirandom(method = \"pseudorandom\", \n                   width = .2,\n                   groupOnX = F, \n                   alpha = .1, \n                   varwidth = T) +\n  geom_smooth() +\n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(breaks = c(0, .25, .5, .75, 1),\n                     labels = c(\"No\", .25, .5, .75, \"Yes\")) +\n  labs(x = \"Length of Turn (characters, log scale)\",\n       y = \"Probability of Next-Turn Repair\") +\n  theme_minimal()\n\n\n\nRepair X Length of Previous Turn\n\n\nLooks promising! It’s hard to tell just by looking at the data points, but the regression line seems to think that longer previous turns are associated wih more repairs. It is worth noting at this point that there are very few next-turn repairs and very many next-turn non-repairs. This means that when we model this formally we will have to be careful interpreting the regression coefficients.\nWe might do a bit better if, rather than counting the number of characters, we had a measure more closely related to how much information is being conveyed. TF-IDF (Term Frequency * Inverse Document Frequency) fits the bill. The TF-IDF of a word describes how rare it is in the whole corpus vs. how common it is in its own turn. Presumably, rarer words are less predictable and therefore more informative and more confusing. The sum of TF-IDF scores of all words in an turn should tell us something about how much new semantic material is included in each turn.\n\n\n\nRepair X Length of Previous Turn\n\n\nThis looks similar to the first one. Indeed, TF-IDF Sum and Turn length are correlated in the corpus at r = 0.956. Nevertheless, I’m going to stick with TF-IDF Sum because it makes more sense to me as a theoretical predictor.\n\n\nI’ll start by simulating reasonable priors. I’ll let the slope be positive or negative. Less than 15% of Builder turns are repairs, so I’ll lower the intercepts. After playing around with the parameters a bit, I settled on this:\n# Simulating Reasonable Priors\nd1 %>%\n  group_by(repair_next) %>%\n  summarise(perc = 100*n()/nrow(.))   # 14.3% of Architect turns are immediately followed by a Builder repair initiation\n\npriors <- \n  tibble(n = 1:50,\n         a = rnorm(50, -1.5, 1),\n         b = rnorm(50, 0, 1)) %>% \n  expand(nesting(n, a, b), x = seq(from = -3, to = 3, length.out = 200)) %>% \n  mutate(p = inv_logit_scaled(a+b*x)) %>%\n  arrange(n) %>%\n  mutate(n = factor(n)) \npriors %>%\n  ggplot(aes(x, p, group = n)) +\n  geom_line(alpha = .5)\nLet’s set up the model.\nlibrary(brms)\nlibrary(tidybayes)\n\ntfidf_mod <- brm(\n  repair_next ~ 1 + tfidfsum_log_s,\n  data = d1,\n  family = bernoulli,\n  prior = c(prior(normal(-1.5, 1), class = Intercept),\n            prior(normal(0, 1), class = b)),\n  sample_prior = \"yes\")\nHere’s the model summary:\nprint(tfidf_mod))\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + tfidfsum_log_s \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept         -1.82      0.03    -1.87    -1.77 1.00     3175     2569\n## tfidfsum_log_s     0.26      0.03     0.20     0.31 1.00     2970     2172\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nNow I can sample the posteriors and see what the models thinks.\nlibrary(ggbeeswarm)\n\nd1$tfidfsum_log_s[d1$tfidfsum == min(d1$tfidfsum, na.rm = T)] # -2.87\nd1$tfidfsum_log_s[d1$tfidfsum == max(d1$tfidfsum, na.rm = T)] # 2.51\n\n# Sample Prior and Posterior\n\ntfidf_mod_priors <- as_draws_df(tfidf_mod, c(\"prior_Intercept\", \"prior_b\"))[1:100,] %>%\n  as_tibble() %>%\n  mutate(n = factor(1:100)) %>%\n  expand(nesting(n, prior_Intercept, prior_b), x_log_s  = seq(from = -2.9, to = 2.51, length.out = 200)) %>%\n  mutate(p = inv_logit_scaled(prior_Intercept+prior_b*x_log_s),\n         x_log = x_log_s * sd(d1$tfidfsum_log) + mean(d1$tfidfsum_log),\n         x = exp(x_log))\n\nn_iter <- 50\ntfidf_mod_fitted <-\n  fitted(tfidf_mod,\n         newdata  = tibble(tfidfsum_log_s = seq(from = -2.9, to = 2.51, length.out = 200)),\n         summary  = F,\n         nsamples = n_iter) %>% \n  as_tibble() %>%\n  mutate(iter = 1:n_iter) %>% \n  pivot_longer(-iter) %>% \n  mutate(tfidfsum_log_s = rep(seq(from = -2.9, to = 2.51, length.out = 200), times = n_iter)) %>% \n  mutate(tfidfsum_log = tfidfsum_log_s * sd(d1$tfidfsum_log) + mean(d1$tfidfsum_log),\n         tfidfsum = exp(tfidfsum_log_s * sd(d1$tfidfsum_log) + mean(d1$tfidfsum_log)))\n\n\ntfidf_mod_postpredict <- tfidf_mod_fitted %>%\n  ggplot(aes(x = tfidfsum)) +\n    geom_hline(yintercept = .5, color = \"red\") +\n    geom_line(aes(y = value, group = iter), color = \"blue\", alpha = .1) +\n    geom_line(data = tfidf_mod_priors,\n              aes(x, p, group = n), color = \"black\", alpha = .08) + \n    geom_quasirandom(data = d1,\n                     aes(x = tfidfsum,\n                         y = as.integer(repair_next)-1),\n                     alpha = 1/10,\n                     groupOnX = F,\n                     width = 1/10,\n                     method = \"pseudorandom\",\n                     varwidth = T) +\n    scale_x_continuous(trans = \"log10\", minor_breaks = seq(10, 100, by = 10)) +\n    scale_y_continuous(breaks = c(0, .25, .5, .75, 1),\n                       labels = c(\"No\", .25, .5, .75, \"Yes\")) +\n    labs(title = \"Data with Prior and Posterior Predictions\",\n         y = \"Probability of Next-Turn Repair\", \n         x = \"Sum TF-IDF (Log Scale)\") +\n    theme_minimal()\n\ntfidf_mod_postpredict\nThe faint grey lines are 100 samples from the prior distribution. In blue are 50 samples from the posterior.\n\n\n\nTF-IDF Model Data with Prior and Posterior Predictions\n\n\nThe posterior predictions look almost like a straight line on the logarithmic scale - for very short, simple instructions from the Architect, the Builder’s response is most likely not to be a repair. As total TF-IDF goes up, the probability of repair does too, at first rapidly, then more slowly.\n\n\n\n\nAs described in Dingemanse et al. (2015), another predictor of repair is the time elapsed since the last repair. People don’t tend to initiate repair twice in a row. In lieu of using actual timestamps, I’ll use the total number of characters typed as a proxy for time elapsed. I’ll use the same priors as before.\ncharssincerepair_mod <- brm(\n  repair_next ~ 1 + charssincerepair_log_s,\n  data = d1,\n  family = bernoulli,\n  prior = c(prior(normal(-1.5, 1), class = Intercept),\n            prior(normal(0, 1), class = b)),\n  sample_prior = \"yes\")\nprint(charssincerepair_mod)\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + charssincerepair_log_s \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept                 -1.81      0.03    -1.87    -1.76 1.00     3148     2719\n## charssincerepair_log_s    -0.24      0.03    -0.29    -0.19 1.00     3209     2570\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\ncharssincerepair_mod_priors <- as_draws_df(charssincerepair_mod, c(\"prior_Intercept\", \"prior_b\"))[1:100,] %>%\n  as_tibble() %>%\n  mutate(n = factor(1:100)) %>%\n  expand(nesting(n, prior_Intercept, prior_b), x_log_s  = seq(from = -4, to = 2.63, length.out = 200)) %>%\n  mutate(p = inv_logit_scaled(prior_Intercept+prior_b*x_log_s),\n         x_log = x_log_s * sd(d1$charssincerepair_log) + mean(d1$charssincerepair_log),\n         x = exp(x_log))\n\nn_iter <- 50\ncharssincerepair_mod_fitted <-\n  fitted(charssincerepair_mod,\n         newdata  = tibble(charssincerepair_log_s = seq(from = -4, to = 2.63, length.out = 200)),\n         summary  = F,\n         nsamples = n_iter) %>% \n  as_tibble() %>%\n  mutate(iter = 1:n_iter) %>% \n  pivot_longer(-iter) %>% \n  mutate(charssincerepair_log_s = rep(seq(from = -4, to = 2.63, length.out = 200), times = n_iter)) %>% \n  mutate(charssincerepair_log = charssincerepair_log_s * sd(d1$charssincerepair_log) + mean(d1$charssincerepair_log),\n         charssincerepair = exp(charssincerepair_log_s * sd(d1$charssincerepair_log) + mean(d1$charssincerepair_log)))\n\n\ncharssincerepair_mod_postpredict <- charssincerepair_mod_fitted %>%\n  ggplot(aes(x = charssincerepair)) +\n  geom_hline(yintercept = .5, color = \"red\") +\n  geom_line(aes(y = value, group = iter), color = \"blue\", alpha = .1) +\n  geom_line(data = charssincerepair_mod_priors,\n            aes(x, p, group = n), color = \"black\", alpha = .08) + \n  geom_quasirandom(data = d1,\n                   aes(x = charssincerepair,\n                       y = as.integer(repair_next)-1),\n                   alpha = 1/10,\n                   groupOnX = F,\n                   width = 1/10,\n                   method = \"pseudorandom\",\n                   varwidth = T) +\n  scale_x_continuous(trans = \"log10\", minor_breaks = seq(10, 100, by = 10)) +\n  scale_y_continuous(breaks = c(0, .25, .5, .75, 1),\n                     labels = c(\"No\", .25, .5, .75, \"Yes\")) +\n  labs(title = \"Data with Prior and Posterior Predictions\",\n       y = \"Probability of Next-Turn Repair\", \n       x = \"Total Characters Since Last Repair (Log Scale)\") +\n  theme_minimal()\n\ncharssincerepair_mod_postpredict\n\n\n\nCharacters Since Last Repair Model Data with Prior and Posterior Predictions\n\n\nHmmm. The model thinks that repairs get less likely as time goes on after the last repair. I have a hard time believing that. Where did I go wrong?\nAnswer: I didn’t use a multilevel model.\nWhat I think is happening is this: most of the variation in likelihood of next-turn repair is between participants. As it happens, pairs of participants in which the Builder is more likely to initiate next-turn repair are also more likely to have short turns and therefore fewer total characters since the last repair. Or something like that.\nBefore I get into complicated modeling to test this directly, I’ll drive home the main point with a simple, intercept-only multilevel model.\nbysubj_mod <- \n  brm(data = d1, \n      family = bernoulli,\n      repair_next ~ 1 + (1 | partner),\n      prior = c(prior(normal(-1.5, 1), class = Intercept), \n                prior(exponential(1), class = sd)),\n      iter = 5000, chains = 4, cores = 2,\n      sample_prior = \"yes\")\n\nplot(bysubj_mod)\nprint(bysubj_mod)\n\nbysubj_mod_post <- as_draws(bysubj_mod)\n\nbysubj_mod_post_mdn <- \n  coef(bysubj_mod, robust = T)$partner[, , ] %>% \n  data.frame() %>% \n  mutate(post_mdn = inv_logit_scaled(Estimate),\n         post_97.5 = inv_logit_scaled(Q97.5),\n         post_2.5 = inv_logit_scaled(Q2.5),\n         partner = unique(d1$partner)) %>%\n  right_join(d1) %>%\n  group_by(Estimate, Est.Error, post_2.5, post_97.5, post_mdn, partner) %>%\n  summarise(prop_repair_next = sum(repair_next == T)/n())\n\nView(bysubj_mod_post)\n\nbysubj_mod_post_mdn %>%\n  ggplot(aes(partner)) +\n    geom_hline(yintercept = inv_logit_scaled(median(bysubj_mod_post$`1`$b_Intercept)), \n               linetype = 2, color = \"orange2\") +\n    geom_point(aes(y = post_mdn, color = \"Model Intercepts\"), size = 3) +\n    geom_errorbar(aes(ymin = post_2.5, ymax = post_97.5, color = \"Model Intercepts\")) +\n    geom_point(aes(y = prop_repair_next, color = \"Empirical Proportions\"), size = 3, shape = 1) +\n    labs(title = \"Individual Differences in Repair Behavior\",\n         subtitle = str_wrap(\"The dashed line represents the model average intercept. \n                             Error bars represent 95% confidence interval.\", 60),\n         x = \"Builder ID\",\n         y = \"Probability of Next-Turn Repair\") +\n    scale_color_manual(name = element_blank(), \n                       values = c(\"black\", \"orange2\")) +\n    theme_minimal()\n\n\n\nIndividual Differences in Repair Behavior\n\n\nAs expected, different Builders have vastly different likelihoods of initiating next-turn repair.\nNow for the monster model. I just showed that different Builders have different likelihoods of initiating next-turn repair, but the same is probably true for different Architects. An Architect who likes to type a lot of turns in a row will lower the probability of any given next-turn being a repair, just as a Builder who initiates repair a lot will raise the probability. Clustering by both Architect and Builder gets complicated, and for now I’m only interested in group-level effects, so I’m just going to cluster by conversation and leave the model agnostic about whether it’s the Builder, the Architect, or some interaction between them that’s causing any difference. I’m also allowing both intercept and slope to vary, and telling the model to estimate the correlation between them.\ncharssincerepair_bysubj_mod <- \n  brm(data = d1, \n      family = bernoulli,\n      repair_next ~ 1 + charssincerepair_log_s + (1 + charssincerepair_log_s | conversation),\n      prior = c(prior(normal(-1.5, 1), class = Intercept), \n                prior(normal(0, 1), class = b),\n                prior(exponential(1), class = sd),\n                prior(lkj(2), class = cor)),\n      iter = 5000, chains = 4, cores = 2)\n\nprint(charssincerepair_bysubj_mod)\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + charssincerepair_log_s + (1 + charssincerepair_log_s | conversation) \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 5000; warmup = 2500; thin = 1;\n##          total post-warmup draws = 10000\n## \n## Group-Level Effects: \n## ~conversation (Number of levels: 64) \n##                                       Estimate Est.Error l-95% CI u-95% CI Rhat\n## sd(Intercept)                             0.76      0.08     0.62     0.94 1.00\n## sd(charssincerepair_log_s)                0.13      0.06     0.01     0.25 1.00\n## cor(Intercept,charssincerepair_log_s)     0.29      0.28    -0.33     0.79 1.00\n##                                       Bulk_ESS Tail_ESS\n## sd(Intercept)                             2003     3424\n## sd(charssincerepair_log_s)                1927     2051\n## cor(Intercept,charssincerepair_log_s)     7747     4847\n## \n## Population-Level Effects: \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept                 -1.87      0.10    -2.08    -1.68 1.00     1365     2679\n## charssincerepair_log_s     0.02      0.04    -0.05     0.09 1.00     7615     7229\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nBingo. The population-level effect of characters since repair is now slightly positive! Let’s visualize that:\n\n\n\nData with Multilevel Model Estimates\n\n\nThe blue line is the model’s estimate of the average effect of characters since last repair on repair probability within groups, with its 95% confidence interval. The grey lines are estimates for each conversation. This time I’m convinced - the effect of content elapsed since the last repair is small if it exists at all. The graph I linked to above from Dingemanse et al. (2015) shows the probability that repair will have occured, which of course rapidly approaches 1 as time goes on. The probability that any given turn will be a repair initiation doesn’t seem to change much, at least in the Minecraft corpus.\nAll this makes me want to re-do my analysis of TF-IDF Sum as a multilevel model. Here’s what that looks like:\ntfidfsum_bysubj_mod <- \n  brm(data = d1, \n      family = bernoulli,\n      repair_next ~ 1 + tfidfsum_log_s + (1 + tfidfsum_log_s | conversation),\n      prior = c(prior(normal(-1.5, 1), class = Intercept), \n                prior(normal(0, 1), class = b),\n                prior(exponential(1), class = sd),\n                prior(lkj(2), class = cor)),\n      iter = 5000, chains = 4, cores = 2)\n      \nprint(tfidfsum_bysubj_mod)\n##  Family: bernoulli \n##   Links: mu = logit \n## Formula: repair_next ~ 1 + tfidfsum_log_s + (1 + tfidfsum_log_s | conversation) \n##    Data: d1 (Number of observations: 11754) \n##   Draws: 4 chains, each with iter = 5000; warmup = 2500; thin = 1;\n##          total post-warmup draws = 10000\n## \n## Group-Level Effects: \n## ~conversation (Number of levels: 64) \n##                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)                     0.76      0.08     0.62     0.93 1.00     1843\n## sd(tfidfsum_log_s)                0.12      0.06     0.01     0.23 1.00     2173\n## cor(Intercept,tfidfsum_log_s)    -0.15      0.31    -0.72     0.50 1.00     9390\n##                               Tail_ESS\n## sd(Intercept)                     3034\n## sd(tfidfsum_log_s)                3267\n## cor(Intercept,tfidfsum_log_s)     5410\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept         -1.92      0.10    -2.11    -1.72 1.00     1431     2332\n## tfidfsum_log_s     0.27      0.04     0.20     0.34 1.00     8006     6499\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nDidn’t change much here, but it does look like the population-level effect is slightly higher than with the fixed-effects model. Interestingly enough, the model estimates that the intercept and beta are slightly negatively correlated. That means that pairs who initiate repair more in general also show less effect of utterance length (or TF-IDF sum, to be exact) on their rates. It seems possible to me that, in highly informational contexts like the Minecraft collaborative building task, some people have a top-down push to just initiate repair as much as possible, regardless of whether or not there’s an obvious need for it. All of this is pretty speculative though, since the estimated error on the correlation parameter is enormous.\nHere’s what the new model looks like as a graph:\n\n\n\nData with Multilevel Model Estimates"
  },
  {
    "objectID": "blog/chess_brain/index.html",
    "href": "blog/chess_brain/index.html",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "",
    "text": "The tragedy of an undergraduate psychology degree: I sit in lecture after lecture, learning about human mind/brain/behavior. It’s all fascinating of course, but I didn’t come here to learn about human mind/brain/behavior. I came here to learn about my mind/brain/behavior. Or at least those of people I interact with. Unfortunately, interesting psychology research tends to be conducted in laboratories with trained researchers. Even after the research stage, good psychometric evaluations are preformed by good clinicians in controlled environments. I don’t have access to those things at this stage of my life.\nLike, for example, I’ve read lots of interesting papers about sleep and circadian rhythms, but have no more than anecdotal evidence about my own cognitive performance throughout the day, or the amount of sleep I really need per night.\nIt felt silly to do so much learning about quantified psychology without even trying to quantify my own psychology, so I made myself a daily survey. I kept it short–no room for multi-item measures–but it has questions for measuring how stressed I am, how happy I am, and how much sleep I got the previous night, among other things. At this point I’ve been filling it out for a few months.\nThis is, of course, terrible science. The samples are non-independent, I’m biased in my answers by whatever hypotheses I might be thinking about, and I’m not even consistent about the time of exact time of day at which I take the survey. My stress and mood obviously fluctuate a lot throughout the day. I try to think through the whole day when I fill out the survey, but the measures are rough at best.\nOne thing my survey doesn’t measure is cognitive performance. I don’t even ask myself how tired I’m feeling. I don’t ask about this because I think I have a better way to measure it: Chess."
  },
  {
    "objectID": "blog/chess_brain/index.html#is-online-chess-a-good-psychometric-tool",
    "href": "blog/chess_brain/index.html#is-online-chess-a-good-psychometric-tool",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "Is Online Chess a Good Psychometric Tool?",
    "text": "Is Online Chess a Good Psychometric Tool?\nI play online chess almost every day. I also like to play super-fast timed games, so I tend to play a lot of games per day. The site I play on, lichess.org, automatically records game and rating statistics.\nChess performance is closely tied to general cognitive capabilities, as is evident from this paper, this paper, and common sense. Can I use my chess performance to measure how cognitively capable I am day to day? How about hour to hour?\nLet’s start with the basics: How do we measure chess performance? I could get a really good AI chess algorithm to evaluate every individual move I make, but the easier option is just to look at rating. Lichess uses the Glicko 2 rating sytem, which uses all sorts of statistical tricks to estimate how good a player is at chess. At the conclusion of each game, both players’ ratings are updated. The winner gains some points and the loser loses some. The amount that they are updated depends on both their previous ratings and how certain it is that those ratings are accurate. So if I gain a lot of points after a game, that means that I did exceptionally well (by my own standards) by beating my opponent. Since I often play many games per day, the average change in my rating per game I play in a day should be a decent measure of how good my chess playing is that day.\nBefore we get to days, let’s start with hours in the day. Circadian rhythms–the body’s clock–have been found to modulate many basic cognitive processes. Since my current occupation allows me to play chess at various hours of the day, we can see if I tend to do better or worse at certain times.\nFor context, here are results from this study showing one person’s performance on a variety of cognitive tasks at various times throughout the day.\n\n\n\nDiurnal Variation in Cognitive Performance\n\n\nHere is my chess performance over the past few months.\n\n\n\nMy Chess Performance by Hour of the Day\n\n\nLooks about right! I bet if I stayed up later than usual and played chess into the night, there would be a gradual dropoff in performance, just like in subject H. K.’s multiplication speed above.\nThe results so far lend some initial validation to my measure. Let’s bring in the data from my survey and ask some more questions."
  },
  {
    "objectID": "blog/chess_brain/index.html#how-does-the-amount-of-sleep-i-get-at-night-affect-my-chess-performance",
    "href": "blog/chess_brain/index.html#how-does-the-amount-of-sleep-i-get-at-night-affect-my-chess-performance",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "How does the amount of sleep I get at night affect my chess performance?",
    "text": "How does the amount of sleep I get at night affect my chess performance?\n\n\n\nMy Chess Performance by Hours of Sleep the Previous Night\n\n\nRemember, this represents my rough recolection in late afternoon/evening of when I went to sleep the last night and woke up in the morning.\nThe effect looks to be mildy positive, with a peak around 8 hours. Does this mean I’m at my best when I get 8 hours of sleep? Unclear. I’m surprised at how weak this relationship seems to be. For some reason, the curve looks much more like what I expected when I limit it to data from my updated survey version, which I’ve been using only since December.\n\n\n\nMy Chess Performance by Hours of Sleep the Previous Night\n\n\nThis difference could be due to chance, or some pitfall of my measurement system, or something about my schedule since December as opposed to before."
  },
  {
    "objectID": "blog/chess_brain/index.html#how-do-my-affect-and-anxiety-affect-the-amount-of-chess-i-play",
    "href": "blog/chess_brain/index.html#how-do-my-affect-and-anxiety-affect-the-amount-of-chess-i-play",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "How do my affect and anxiety affect the amount of chess I play?",
    "text": "How do my affect and anxiety affect the amount of chess I play?\n\n\n\nMy Chess Performance by Affect\n\n\n\n\n\nMy Chess Performance by Anxiety"
  },
  {
    "objectID": "blog/chess_brain/index.html#how-do-all-these-things-affect-the-way-in-which-i-losewin-my-games",
    "href": "blog/chess_brain/index.html#how-do-all-these-things-affect-the-way-in-which-i-losewin-my-games",
    "title": "Tracking Cognitive Performance with Online Chess",
    "section": "How do all these things affect the way in which I lose/win my games?",
    "text": "How do all these things affect the way in which I lose/win my games?"
  }
]